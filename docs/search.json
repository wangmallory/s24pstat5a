[
  {
    "objectID": "Pages/calendar.html",
    "href": "Pages/calendar.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "TA\n\n\nEmail\n\n\nSection Time\n\n\nLocation\n\n\n\n\n\n\nDaniel Silva\n\n\ndcsilva@ucsb.edu\n\n\n12:30 - 1:20pm\n\n\nPhelps 1513\n\n\n\n\nHezhong Zhang\n\n\nhzhang586@umail.ucsb.edu\n\n\n2 - 2:50pm\n\n\nPhelps 1513\n\n\n\n3 - 3:50pm\n\n\nPhelps 1513",
    "crumbs": [
      "Course Info",
      "Sections and Calendar"
    ]
  },
  {
    "objectID": "Pages/calendar.html#schedule-of-sections",
    "href": "Pages/calendar.html#schedule-of-sections",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "TA\n\n\nEmail\n\n\nSection Time\n\n\nLocation\n\n\n\n\n\n\nDaniel Silva\n\n\ndcsilva@ucsb.edu\n\n\n12:30 - 1:20pm\n\n\nPhelps 1513\n\n\n\n\nHezhong Zhang\n\n\nhzhang586@umail.ucsb.edu\n\n\n2 - 2:50pm\n\n\nPhelps 1513\n\n\n\n3 - 3:50pm\n\n\nPhelps 1513",
    "crumbs": [
      "Course Info",
      "Sections and Calendar"
    ]
  },
  {
    "objectID": "Pages/calendar.html#visual-weekly-schedule",
    "href": "Pages/calendar.html#visual-weekly-schedule",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Visual Weekly Schedule",
    "text": "Visual Weekly Schedule",
    "crumbs": [
      "Course Info",
      "Sections and Calendar"
    ]
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#recap",
    "href": "Pages/Lectures/Lecture22/Lec22.html#recap",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Recap",
    "text": "Recap\n\nGiven observations \\(\\{(x_i, \\ y_i)\\}_{i=1}^{n}\\) on a response variable y and an explanatory variable x, we adopt the model \\[ \\texttt{y} = f(\\texttt{x}) + \\texttt{noise} \\]\nAssuming a linear signal function, we obtain a model \\[ \\texttt{y} = \\beta_0 + \\beta_1 \\cdot \\texttt{x} + \\texttt{noise} \\] and we have previously seen how to use our data to obtain good estimates \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) of \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\nA natural question arises- how can we assess how good our fit is?\nTo answer this question, it turns out we need to be a little more specific in our model."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#modelling-noise",
    "href": "Pages/Lectures/Lecture22/Lec22.html#modelling-noise",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Modelling Noise",
    "text": "Modelling Noise\n\nLet’s start off by rewriting our model as \\[ \\texttt{y} = f(\\texttt{x}) + \\varepsilon \\] where we use \\(\\varepsilon\\) to now denote our noise.\nThe reason we do this is we can now rewrite the model to be in terms of the individual observations \\(x_i\\) and \\(y_i\\) (as opposed to the variables x and y): \\[ y_i = f(x_i) + \\varepsilon_i \\] where \\(\\varepsilon_i\\) represents the noise associated with a single observation.\nIn a wide array of situations, it turns out to be a good idea to assume normally-distributed noise; that is, we often assume \\(\\varepsilon_i \\sim \\mathcal{N}(0, \\ \\sigma^2)\\) for all indices i."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#modelling-noise-1",
    "href": "Pages/Lectures/Lecture22/Lec22.html#modelling-noise-1",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Modelling Noise",
    "text": "Modelling Noise\n\nAgain, crucially, we never get to see exactly what the observed value of \\(\\varepsilon_i\\) is for any particular i.\nHowever, if we rewrite our model as \\[ \\varepsilon_i = y_i - f(x_i) \\] then we see that we actually already have a pretty good estimator for \\(\\varepsilon_i\\)- the ith residual!\n\nThat is, \\[ e_i = y_i - \\widehat{y}_i \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#residual-plots",
    "href": "Pages/Lectures/Lecture22/Lec22.html#residual-plots",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Residual Plots",
    "text": "Residual Plots\n\nThis leads us to one of the most important regression diagnostic tools: residual plots.\nPoints in a residual plot have x-coordinate given by the value of the residual and y-coordinate given by the corresponding fitted value.\nLet’s do an example quickly to illustrate how to construct one by hand, using our toy example from last lecture: \\[\\begin{align*}\n\\boldsymbol{x}    & = \\{3, \\ 7, \\ 8\\}   \\\\\n\\boldsymbol{y}    & = \\{20, \\ 14, \\ 17\\}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-1",
    "href": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-1",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Residual Plots",
    "text": "Residual Plots\n\n\nThe fitted value of the blue point above is \\((155 - 6 \\cdot 3) / 7 = 19.57143\\) and its corresponding residual is \\(20 - 19.57143 = 0.42857\\), meaning one of the points on our residual plot will be \\[ (0.42857, \\ 19.57143) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-2",
    "href": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-2",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Residual Plots",
    "text": "Residual Plots\n\n\nThe fitted value of the blue point above is \\((155 - 6 \\cdot 7) / 7 = 16.14286\\) and its corresponding residual is \\(14 - 16.14286 = -2.14286\\), meaning one of the points on our residual plot will be \\[ (-2.14286, \\ 16.14286) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-3",
    "href": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-3",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Residual Plots",
    "text": "Residual Plots\n\n\nThe fitted value of the blue point above is \\((155 - 6 \\cdot 7) / 7 = 15.28571\\) and its corresponding residual is \\(17 - 15.28571 = 1.71429\\), meaning one of the points on our residual plot will be \\[ (1.71429, \\ 15.28571) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-4",
    "href": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-4",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Residual Plots",
    "text": "Residual Plots\n\nHence, the residual plot for the above regression would be"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-5",
    "href": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-5",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Residual Plots",
    "text": "Residual Plots\n\nOne of the main purposes of examining residual plots is to see whether or not there are any remaining patterns in the data after fitting a model.\nRemember- residuals are meant to mimic the noise apparent in our observations.\nHence, if our model fit truly capture all of the relationship present in the data, we would expect the residuals to not display any noticeable pattern.\nThis leads us to how we use residual plots:\n\n\n\n\n\n\n\n\nTip\n\n\nIf the residual plot displays a pattern, then the fitted model is not capturing all of the relationship present in the data."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-6",
    "href": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-6",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Residual Plots",
    "text": "Residual Plots"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-7",
    "href": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-7",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Residual Plots",
    "text": "Residual Plots"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-8",
    "href": "Pages/Lectures/Lecture22/Lec22.html#residual-plots-8",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Residual Plots",
    "text": "Residual Plots"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#other-diagnostics",
    "href": "Pages/Lectures/Lecture22/Lec22.html#other-diagnostics",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Other Diagnostics",
    "text": "Other Diagnostics\n\nThere are, of course, many other diagnostics one can perform after performing regression.\n\nEven within the residual plot, there are other things to look for (things like homoskedasticity and heterodskedasticity)\nThere are also numerical tools we can use to diagnose our model performance.\n\nI save these for your future courses to cover."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#where-does-data-come-from-1",
    "href": "Pages/Lectures/Lecture22/Lec22.html#where-does-data-come-from-1",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Where Does Data Come From?",
    "text": "Where Does Data Come From?\n\nThroughout this course, we have been using various pieces of data.\nOne thing we should discuss, as good Data Scientists, is where this data actually came from?\n\nWho collected it? How was the data collected? Who were the subjects included in the data?\n\nWe will now begin to discuss some possible answers to these questions, as well as some practical strategies for collecting data of our own!"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#the-research-process",
    "href": "Pages/Lectures/Lecture22/Lec22.html#the-research-process",
    "title": "PSTAT 5A: Lecture 22",
    "section": "The Research Process",
    "text": "The Research Process\n\nIndeed, most experiments and studies begin with some sort of question.\n\nFor example: “does this new drug truly reduce blood pressure?”\nOr: “is smoking really linked with higher rates of lung cancer?”\nOr: “what is the average mercury content in swordfish in the Atlantic Ocean?”\nOr: “over the past 3 years, what is the average number of people that have been admitted into the PSTAT major?”\n\nThe next step is something we’ve actually done several times in this class already: identify the population of interest!\n\nCan anyone tell me what the populations associated with the above research questions are?"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#sampling-procedures",
    "href": "Pages/Lectures/Lecture22/Lec22.html#sampling-procedures",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\n\nNow, we need the most crucial piece of all: data!\n\nSpecifically, we need to collect our data.\n\nThis will entail taking a sample (or possibly many samples) from our population.\n\nThis is why it’s important to know what our population is- so we know where to start taking samples from.\n\nThere are many ways to take a sample!\nOne way is to take what is known as a simple random sample (or SRS, for short).\nA simple random sample is akin to assigning a unique number to each person in the population, and then picking some subset of these numbers uniformly at random.\n\nCrucially, in this way, each member of the population has an equal chance of being included in the sample."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#sampling-procedures-1",
    "href": "Pages/Lectures/Lecture22/Lec22.html#sampling-procedures-1",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\n\nNow, Simple Random Samples do have some potential downsides.\nAs an example, let’s consider the following situation: suppose our population is the workplace at some company (which we will call Company X).\nSay we are interested in determining whether or not there is systemic racism present in Company X.\nTo determine whether or not this is the case, we might take a sample and administer a survey (we’ll talk more about surveys in a bit).\nCan anyone tell me a potential problem with this setup if we were to take a Simple Random Sample?\n\nWhat if I tell you, for example, that 80% of people in this company are Caucasian and only 20% are not?"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#sampling-procedures-2",
    "href": "Pages/Lectures/Lecture22/Lec22.html#sampling-procedures-2",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\n\nThat’s right- we run the risk of obtaining a biased sample.\nSaid differently: if we take an SRS of employees at this company, there is a high probability that this sample will contain a disproportionately larger number of Caucasians than non-Caucasians.\n\nThis would almost certainly affect the results of our survey!\n\nOne way to remedy this would be to perform what is known as in which we first divide the population into several strata (i.e. groups), and then take an SRS from each stratum.\n\nThis has the benefit of ensuring a roughly equal number of participants from each stratum, but has the downsides of being very dependent on the strata that are created.\nThat is, sometimes it won’t necessarily be obvious how to divide the population!"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#sampling-procedures-3",
    "href": "Pages/Lectures/Lecture22/Lec22.html#sampling-procedures-3",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\n\nAnother type of sampling is called cluster sampling. Similar to stratified sampling, the population is first divided into several groups (now called clusters). But, instead of taking an SRS from every cluster we instead take an SRS of clusters and then take an SRS from each included cluster\n\nNote, then, that we are not including every cluster in our sample in this way of sampling.\nThis has the benefit of being (potentially) cheaper, but has the (obvious) downside of potentially skewing results due to the lack of certain clusters.\n\nBy the way, this is slightly different than the notion of Cluster Sampling outlined in the textbook; the textbook calls the above scheme “multistage sampling”. However, this term is not widely used, and as such we will simply refer to the above as “Cluster Sampling”, as most statisticians do."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#example",
    "href": "Pages/Lectures/Lecture22/Lec22.html#example",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Example",
    "text": "Example\n\nLet’s consider an example from the textbook:\n\n\n\n\n\n\n\n\nExample (credit to OpenIntro Statistics)\n\n\n\n\nSuppose we are interested in estimating the malaria rate in a densely tropical portion of rural Indonesia. We learn that there are 30 villages in that part of the Indonesian jungle, each more or less similar to the next. Our goal is to test 150 individuals for malaria. What sampling method should be employed?\n\n\n\n\n\n\n\nAt surface level, an SRS may seem tempting.\n\nAfter all, the villages are “more or less similar to the next”.\n\nHowever, an SRS will likely contain individuals from all (or certainly most) of the villages.\nThis would require someone to actually visit these villages to collect data, which will be incredibly costly.\nAs such, an SRS is probably not a good idea."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#example-1",
    "href": "Pages/Lectures/Lecture22/Lec22.html#example-1",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Example",
    "text": "Example\n\nIndeed, cluster sampling seems to be the way to go.\nSpecifically, we could assign each village to its own cluster, then select some number of villages (say, maybe 15 or so) randomly, and then select some number of people (say, 10 or so) from each of the selected villages.\nBecause the villages are “more or less similar”, the people included in our sample would likely not have many obvious differences (in the context of the experiment) and we would believe our sample to be relatively representative.\nAdditionally, the experimenter (or a volunteer) would only need to visit 15 villages instead of all 30.\nOf course, we would need to modify our statistical tools slightly to reflect the fact that our sample was actually collected via a clustering scheme- such modifications (though not too challenging) are outside the scope of this course.\n\nRight now, I’m merely trying to get us to think about the practicalities of data collection!"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#bias",
    "href": "Pages/Lectures/Lecture22/Lec22.html#bias",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Bias",
    "text": "Bias\n\nEven if we end up with a fairly representative sample, there are ways bias can creep in.\nSuppose we administer a survey to a representative sample of people.\nIt is not guaranteed that everyone in our sample will actually fill out the survey; even if a particular individual does attempt the survey, there is no guarantee they will finish the entirety of the survey.\nThis can lead to gaps in our data with respect to certain demographics, which is itself a form of bias.\nWe call this non-response bias."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#bias-1",
    "href": "Pages/Lectures/Lecture22/Lec22.html#bias-1",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Bias",
    "text": "Bias\n\nAnother sampling strategy that is prone to bias is known as convenience sampling.\nWe can think of convenience sampling as any sort of sampling scheme where individuals who are easily accessed have a higher chance of being included in the sample.\nFor example, suppose we poll students about their views on the housing crisis in Santa Barbara.\n\nIf we conduct this poll at UCSB, we are more likely to get UCSB students and much less likely to get, say, UCLA students.\nSimilarly, if we conducted our study at UCLA, we would be more likely to include UCLA students than, say, NYU students."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#bias-2",
    "href": "Pages/Lectures/Lecture22/Lec22.html#bias-2",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Bias",
    "text": "Bias\n\nSo, why would we ever use a convenience sample?\nWell, as the name suggests- it is convenient!\nConvenience samples are often both the easiest to obtain, as well as the cheapest.\nHaving said that, most statisticans agree thate convenience samples are bad. This is due to what is known as the garbage in garbage out philosophy.\nLoosely speaking, the “garbage in garbage out” philosophy states that our results are only as good as our data- even the most sophisticated statistical models will output nonsensical or skewed results if we are feeding them nonsensical or skewed data!"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#another-distinction",
    "href": "Pages/Lectures/Lecture22/Lec22.html#another-distinction",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Another Distinction",
    "text": "Another Distinction\n\nThere is another distinction we should be aware of: the difference between an observational study and an experiment.\nIn an observational study, no treatment is ever explicitly applied (or withheld).\nThis is in contrast to an experiment, in which researchers assign treatments to cases.\nFor example, suppose a researcher is interested in determining the relationship between cancer rates and tanning beds.\nIn an experiment, the researcher would take some sample of volunteers, split them into groups, and assign one group to tan regularly and another to not use tanning beds at all for the duration of the study.\n\nAt the end of the study, the researcher would collect data and analyze."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#another-distinction-1",
    "href": "Pages/Lectures/Lecture22/Lec22.html#another-distinction-1",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Another Distinction",
    "text": "Another Distinction\n\nIf instead the researcher were to conduct an observational study, they might take some sample of people who already use tanning beds, analyze cancer rates, and repeat for a sample of people who do not use tanning beds.\n\nNote that in this case, the experimenter has not explicitly assigned a treatment (i.e. using a tanning bed) to either group. In a sense, the groups were formed around the treatment.\n\nNaturally, experiments come with all sorts of ethical considerations.\n\nFor instance, in trying to determine the relationship between cancer rates and smoking, nobody would actually force a group of participants to smoke, just for the purposes of an experiment.\nThis is one “pro” in favor of observational studies- the experimenter is not responsible for forcing a group of people to do (or not do something) they wouldn’t want to."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#another-distinction-2",
    "href": "Pages/Lectures/Lecture22/Lec22.html#another-distinction-2",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Another Distinction",
    "text": "Another Distinction\n\nOf course, observational studies are not perfect either.\nSpecifically, observational studies cannot be used to identify causal relationships; they can only ever identify associations (or a lack thereof).\nRemember that assocations are not the same things as causation!\n\nSo, just because an observational study indicates a link between smoking and increased lung cancer rates, that is not sufficient justification to say that smoking causes increased lung cancer rates. To make that causal claim, an experiment would need to be conducted."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#example-2",
    "href": "Pages/Lectures/Lecture22/Lec22.html#example-2",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\nExample (OpenIntro Statistics, 1.19)\n\n\n\n\nA large college class has 160 students. All 160 students attend the lectures together, but the students are divided into 4 groups, each of 40 students, for lab sections administered by different teaching assistants. The professor wants to conduct a survey about how satisfied the students are with the course, and he believes that the lab section a student is in might affect the student’s overall satisfaction with the course.\n\nWhat type of study is this?\nSuggest a sampling strategy for carrying out this study."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#solutions",
    "href": "Pages/Lectures/Lecture22/Lec22.html#solutions",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Solutions",
    "text": "Solutions\n\nBecause treatment has neither been administered nor withheld by the researcher, this is an example of an observational study.\nIn this case, stratified sampling would likely be a good idea, with each lab section assigned to a stratum. This ensures each stratum (lab section) is represented in the sample."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#longitudinal-vs.-cross-sectional-studies",
    "href": "Pages/Lectures/Lecture22/Lec22.html#longitudinal-vs.-cross-sectional-studies",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Longitudinal vs. Cross-Sectional Studies",
    "text": "Longitudinal vs. Cross-Sectional Studies\n\nSuppose a particular drug claims to significantly reduce blood sugar levels. Consider the following two scenarios:\n\nAn SRS of 100 people is taken from a certain demographic, and divided randomly into two groups: group A and group B. Group A is administered the drug and Group B is not administered the drug. After a week, the researcher collects data on the blood sugar levels of the two groups.\nAn SRS of 50 people is taken from a certain demographic. These 50 people have their blood sugar levels recorded, and are then all administered the drug. A week later, the participants have their blood sugar levels recorded."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#longitudinal-vs.-cross-sectional-studies-1",
    "href": "Pages/Lectures/Lecture22/Lec22.html#longitudinal-vs.-cross-sectional-studies-1",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Longitudinal vs. Cross-Sectional Studies",
    "text": "Longitudinal vs. Cross-Sectional Studies\n\nNote that both of these situations seemingly end up with the same data: 50 measurements of pre-treatment blood sugar levels, and 50 measurements of post-treatment blood sugar levels.\nHowever, we can see that these two studies are fundamentally different.\nIn the first study, people were divided into two groups (called the treatment and control groups, respectively).\nIn the second, the same 50 individuals were tracked over time.\nThis is an example of the distinction between a cross-sectional study and a longitudinal study."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#longitudinal-vs.-cross-sectional-studies-2",
    "href": "Pages/Lectures/Lecture22/Lec22.html#longitudinal-vs.-cross-sectional-studies-2",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Longitudinal vs. Cross-Sectional Studies",
    "text": "Longitudinal vs. Cross-Sectional Studies\n\nIn a longitudinal study, the same set of individuals is tracked over time.\nIn a cross-sectional study, individuals are divided into several groups.\nNotice that data in longitudinal studies necessarily possess serial correlation: that is, measurements are correlated!\n\nPre-treatment measurements for, say, John, are very likely correlated with John’s post-treatment measurements since these measurements were still collected from John!"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#principles-of-experimental-design",
    "href": "Pages/Lectures/Lecture22/Lec22.html#principles-of-experimental-design",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Principles of Experimental Design",
    "text": "Principles of Experimental Design\n\nLet’s quickly return to our distinction between observational studies and experiments.\nSuppose it is decided that we want to conduct an experiment.\nWe then need to think very carefully about how we want to design our experiment.\nThis leads us into our final discussion for this course: Experimental Design."
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#treatment-vs.-control-groups",
    "href": "Pages/Lectures/Lecture22/Lec22.html#treatment-vs.-control-groups",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Treatment vs. Control Groups",
    "text": "Treatment vs. Control Groups\n\nFirst, let me make clear the distinction between treatment and control groups.\nTreatment groups are groups to which one or more treatments is/are administered.\n\nThere can potentially be multiple treatment groups; for example, we could have 4 groups each testing a different medicine\n\nTypically, we always leave at least one group “alone”; i.e. one group to which no treatment is administered. This group is called the control group.\n\nIt is also possible to have multiple control groups!"
  },
  {
    "objectID": "Pages/Lectures/Lecture22/Lec22.html#experimental-design",
    "href": "Pages/Lectures/Lecture22/Lec22.html#experimental-design",
    "title": "PSTAT 5A: Lecture 22",
    "section": "Experimental Design",
    "text": "Experimental Design\n\nExperimental Design loosely refers to the principles of and procedures related to setting up an experiment.\nI won’t go into this in too much detail.\nI do highly encourage you to read Section 1.4 of your textbook, as it provides a very nice summary of some of the main tenants of experimental design.\nRemember - as Data Scientists, it is very important we understand our data as much as possible before performing analyses.\n\nPart of knowing our data is knowing how it was collected, and, indeed, how we might go about collecting our own!"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#cats",
    "href": "Pages/Lectures/Lecture14/Lec14.html#cats",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Cats!",
    "text": "Cats!\n\nAs we have previously seen…\n… Ethan loves cats!\nSo, let’s consider another cat example!\nIt is often stated that only 1 in 5 orange tabby cats is female; i.e. that only 20% of orange tabby cats are female.\nLet’s say we take a representative sample of 100 orange tabby cats and find that 19 of these cats are female.\nSince we observed a proportion of only 19% female cats in our sample, does that mean the claim of 20% of orange tabby cats being female is wrong?\nWell, no! We know that this 19% is actually an observed instance of \\(\\widehat{P}\\), which itself is random.\n\nFurthermore, 19% is pretty close to 20% so there’s nothing obviously letting us know that the claim is false."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#cats-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#cats-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Cats!",
    "text": "Cats!\n\nHowever, if instead our sample of 100 orange tabby cats contained only 1 female in this sample, we might start to question the claim that 20% of orange tabby cats are female.\nOkay, what if in our sample of 100 orange tabby cats we actually only observed 15 female cats?\nThings are perhaps a bit less clear now… we know that there will be some variability in our point estimator, but how much variability would we really expect? Enough to plausibly observe a sample proportion of 15%?\n\n\n\n\nA little more abstractly: we started with a hypothesis (in this example, “20% of orange tabby cats are female”). We then wish to use our data to test how plausible this hypothesis is."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing",
    "href": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nThis is exactly the framework of hypothesis testing.\nIn hypothesis testing, we start with a pair of competing claims which we call the null hypothesis and alternative hypothesis, respectively.\n\nWe use \\(H_0\\), read as “h-naught”, to denote the null hypothesis and \\(H_A\\) to denote the alternative hypothesis.\n\nFor instance, in our cat example above the null hypothesis would be “\\(H_0\\): the true proportion of orange cats that are female is 20%”.\nOftentimes we will want to phrase our hypotheses in more mathematical terms. This is where the notation we’ve used over the past few lectures comes into play: letting \\(p\\) denote the true proportion of orange tabby cats that are female, we can write our null hypothesis as \\[ H_0: \\ p = 0.2 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nWhat about the alternative hypothesis?\nAs the name suggests, the alternative hypothesis provides some sort of alternative to the null.\nLet’s look at our cat example again. Here are some potential alternatives to the null:\n\n\\(p \\neq 0.2\\) (i.e. the true proportion of orange tabby cats that are female is not 20%)\n\\(p &gt; 0.2\\) (i.e. the true proportion of orange tabby cats that are female is larger than 20%)\n\\(p &lt; 0.2\\) (i.e. the true proportion of orange tabby cats that are female is less than 20%)\n\\(p = 0.10\\) (i.e. the true proportion of orange tabby cats that are female is 10%)"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing-2",
    "href": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nEach of these alternative hypotheses has a name.\nBefore we talk about these names, let’s establish a slightly more general framework for conducting hypothesis testing on a proportion.\nOur null hypothesis will often take the form \\[ H_0 : \\ p = p_0 \\] for some prespecified value \\(p_0\\) (e.g. 20%, like in our cat example above).\nThis leads to four possible alternative hypotheses:\n\n\\(H_A: \\ p \\neq p_0\\)\n\\(H_A: \\ p &gt; p_0\\)\n\\(H_A: \\ p &lt; p_0\\)\n\\(H_A: \\ p = p_1\\) (where \\(p_1 \\neq p_0\\))\n\nI’d like to stress: in a specific hypothesis testing problem, we need to pick one of these alternative hypotheses"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#terminology",
    "href": "Pages/Lectures/Lecture14/Lec14.html#terminology",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Terminology",
    "text": "Terminology\n\nWhen our alternative hypothesis is of the form \\(H_A: \\ p \\neq p_0\\), we refer to the situation as a two-sided hypothesis test.\nWhen our alternative hypothesis is of the form \\(H_A: \\ p &gt; p_0\\) or \\(H_A: \\ p &lt; p_0\\), we refer to the situation as a one-sided hypothesis test. Specifically:\n\n\\(H_A: \\ p &lt; p_0\\) leads to a lower-tailed test\n\\(H_A: \\ p &gt; p_0\\) leads to an upper-tailed test\n\nWhen our alternative hypothesis is of the form \\(H_A: \\ p = p_1\\) (for some value \\(p_1\\) different than our null value \\(p_0\\)), we refer to the situation as a simple-vs-simple hypothesis test.\nAgain, our test will be only one of the above!\nAdditionally, it is usually up to the tester (i.e. the statistician or datascientist in charge of conducting the hypothesis test) to pick which set of hypotheses to use."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#setting-the-hypotheses",
    "href": "Pages/Lectures/Lecture14/Lec14.html#setting-the-hypotheses",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Setting the Hypotheses",
    "text": "Setting the Hypotheses\n\nOkay, so which test to we use when?\nIn practice, there isn’t a one-size-fits-all approach to knowing which set of hypotheses to adapt in a given situation.\nUsually, in the absence of any additional information, we adopt a two-sided test as it tends to be the most general.\nHowever, sometimes additional information may be available to us that may influence us to select a different type of test.\n\nFor example, if previous studies have resulted in numerous observed sample proportions much less than the null value \\(p_0\\), we may want to adopt a lower-tailed test.\n\nHow do we set the null hypothesis? Well, typically the null hypothesis is easier to set: I like to think of it as the “status quo”.\n\nIn other words, the null hypothesis is often taken to be whatever the existing claims state; e.g. that 20% of orange tabby cats are female."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example",
    "href": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle.\n\nWhat is the population?\nWhat is the sample?\nDefine the parameter of interest.\nDefine the random variable of interest.\nWrite down the null hypothesis for this test.\nWrite down the two-sided alternative hypothesis for this test.\nWrite down the lower-tailed alternative hypothesis for this test.\nWrite down the upper-tailed alternative hypothesis for this test."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nThe population is the set of all US households.\nThe sample is the representative sample of 500 US households we took.\nThe parameter of interest is \\(p =\\) the true proportion of US households that own a vehicle.\nThe random variable of interest is \\(\\widehat{P} =\\) the proportion of households in a sample of 500 that own a vehicle.\nRecall that the null hypothesis can be thought of as the “status quo”.\n\nIn other words, we take the null to be whatever claim we want to test: \\[ H_0 : \\ p = 0.917 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions-contd",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions-contd",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions (cont’d)",
    "text": "Solutions (cont’d)\n\nThe two-sided alternative hypothesis would be that the proportion of households that own a vehicle is not equal to 91.7%: \\[ H_A: \\ p \\neq 0.917 \\]\nThe lower-tailed alternative hypothesis would be that the proportion of households that own a vehicle is less than 91.7%: \\[ H_A: \\ p &lt; 0.917 \\]\nThe lower-tailed alternative hypothesis would be that the proportion of households that own a vehicle is greater than 91.7%: \\[ H_A: \\ p &gt; 0.917 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing-3",
    "href": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing-3",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nAlright, here is what we have so far in terms of hypotheses:\n\nThe null hypothesis represents a sort of “status quo” statement.\nThe alternative hypothesis represents an alternative to the status quo.\n\nSo, what is a hypothesis test?\nA hypothesis test is a framework/procedure that allows us to determine whether or not the null should be rejected in favor of the alternative.\nNaturally, a hypothesis test will depend on data! As such, we can think of a hypothesis test as a function that takes in data and outputs either reject H0 or fail to reject H0. \\[ \\texttt{decision}(\\texttt{data}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\texttt{...} \\\\  \\texttt{fail to reject } H_0 & \\text{if } \\texttt{...} \\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#fail-to-reject",
    "href": "Pages/Lectures/Lecture14/Lec14.html#fail-to-reject",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Fail To Reject?",
    "text": "Fail To Reject?\n\nBy the way, the results of a hypothesis test are always framed in terms of the null hypothesis; e.g. “reject \\(H_0\\)” or “fail to reject \\(H_0\\)”.\nWait, why are we saying “fail to reject \\(H_0\\)”? Isn’t that just equivalent to “accept \\(H_0\\)”?\nWell, not quite…\nThink of it this way: just because we are saying the particular alternative hypothesis we picked is less plausible than the null, doesn’t mean there isn’t a different alternative hypothesis that is more plausible than the null.\nAll we are saying when we fail to reject the null is exactly that- we didn’t have enough information to reject \\(H_0\\) outright. We are not saying that \\(H_0\\) must be true.\nAdmittedly, some statisticians have gotten a little lax with this distinction and you may encounter textbooks and/or professors that use terms like “accept the null”.\n\nFor better or for worse, I am a bit of a traditionalist in this respect and will adhere to the terminology “fail to reject” in favor of “accept”."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#four-states-of-the-world",
    "href": "Pages/Lectures/Lecture14/Lec14.html#four-states-of-the-world",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Four States of the World",
    "text": "Four States of the World\n\nOkay, so we’ve talked a bit more about what a hypothesis test actually is: it is a procedure that takes in data and outputs a decision on whether or not to reject the null.\nBehind the scenes, however, the null will either be true or not.\nThis leads to the following four situations:\n\n\n\n\n\n\n\n\nResult of Test\n\n\n\n\n\n\n\nReject\n\n\nFail to Reject\n\n\n\n\nH0\n\n\nTrue\n\n\n\n\n\n\n\n\nFalse\n\n\n\n\n\n\n\n\n\n\n\nSome of these situations are good, and some of these are bad! Which are which?"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#four-states-of-the-world-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#four-states-of-the-world-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Four States of the World",
    "text": "Four States of the World\n\n\n\n\n\n\n\n\nResult of Test\n\n\n\n\n\n\n\nReject\n\n\nFail to Reject\n\n\n\n\nH0\n\n\nTrue\n\n\nBAD\n\n\nGOOD\n\n\n\n\nFalse\n\n\nGOOD\n\n\nBAD\n\n\n\n\n\n\nWe give names to the two “bad” situations: Type I and Type II errors.\n\n\n\n\n\n\n\n\nResult of Test\n\n\n\n\n\n\n\nReject\n\n\nFail to Reject\n\n\n\n\nH0\n\n\nTrue\n\n\nType I Error\n\n\nGOOD\n\n\n\n\nFalse\n\n\nGOOD\n\n\nType II Error"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#type-i-and-type-ii-errors",
    "href": "Pages/Lectures/Lecture14/Lec14.html#type-i-and-type-ii-errors",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\n\n\n\n\n\n\nDefinition: Type I and Type II errors\n\n\n\n\n\nA Type I Error occurs when we reject \\(H_0\\), when \\(H_0\\) was actually true.\nA Type II Error occurs when we fail to reject \\(H_0\\), when \\(H_0\\) was actually false.\n\n\n\n\n\n\n\n\nA common way of interpreting Type I and Type II errors are in the context of the judicial system.\nThe US judicial system is built upon a motto of “innocent until proven guilty.” As such, the null hypothesis is that a given person is innocent.\nA Type I error represents convicting an innocent person.\nA Type II error represents letting a guilty person go free."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#type-i-and-type-ii-errors-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#type-i-and-type-ii-errors-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\nViewing the two errors in the context of the judicial system also highlights a tradeoff.\nIf we want to reduce the number of times we wrongfully convict an innocent person, we may want to make the conditions for convicting someone even stronger.\nBut, this would have the consequence of having fewer people overall convicted, thereby (and inadvertently) increasing the chance we let a guilty person go free.\nAs such, controlling for one type of error increses the likelihood of committing the other type."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle.\nAssuming we are conducting a two-sided test, what would a Type I error be in the context of this experiment? What about a Type II error?"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nA Type I error would be concluding that the true proportion of US households that own a vehicle is not 91.7%, when in fact 91.7% of US households own a vehicle.\nA Type II error would be concluding that the true proportion of US households that own a vehicle is 91.7%, when in fact the true proportion is not 91.8%."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#leadup",
    "href": "Pages/Lectures/Lecture14/Lec14.html#leadup",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Leadup",
    "text": "Leadup\n\nAlright, now we know about the basics and background surrounding hypothesis tests.\nHow do we actually construct one?\nLet’s focus on hypothesis testing for population proportions for now; we’ll deal with sample means later.\nRecall our setup: our hypothesis test should be some sort of decision-making process of the form \\[ \\texttt{decision}(\\texttt{data}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\texttt{...} \\\\  \\texttt{fail to reject } H_0 & \\text{if } \\texttt{...} \\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#leadup-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#leadup-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Leadup",
    "text": "Leadup\n\nFor the moment, let’s return to the cat example from the beginning of the lecture.\nLetting \\(p\\) denote the true proportion of orange tabby cats that are female, our null hypothesis takes the form \\(H_0: \\ p = 0.2\\).\nSuppose we take a two-sided alternative: \\(H_A: \\ p \\neq 0.2\\).\nNow, we have a good summary statistic for proportions: \\(\\widehat{P}\\).\nAs such, our decision process should probably be of the form \\[ \\texttt{decision}(\\widehat{p}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\texttt{...} \\\\  \\texttt{fail to reject } H_0 & \\text{if } \\texttt{...} \\\\ \\end{cases}  \\]\nSaid differently: if we observe a value of \\(\\widehat{p} = 0.82\\), or a value of \\(\\widehat{p} = 0.001\\), we would likely be inclined to reject the null."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#test-statistic",
    "href": "Pages/Lectures/Lecture14/Lec14.html#test-statistic",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nSo, it makes sense to reject \\(H_0\\) when \\(\\widehat{p}\\) is very far away from \\(p_0\\) (which, in the cat example, is \\(0.2\\)). \\[ \\texttt{decision}(\\widehat{p}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if $\\widehat{p}$ is far from $p_0$} \\\\  \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]\nFor reasons that will become clear in a few slides, we typically avoid using \\(\\widehat{p}\\) and instead use a standardized version of \\(\\widehat{p}\\): \\[ \\mathrm{TS} = \\frac{\\widehat{P} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} \\] where \\(\\mathrm{TS}\\) stands for test statistic."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#test-statistic-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#test-statistic-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nNote that, by definition, the test statistic \\(\\mathrm{TS}\\) is a random variable!\n\nThis is because it is simply a centered and scaled version of \\(\\widehat{P}\\), which we know is a random variable.\n\nFor a given sample, however, we will have a given observed value of \\(\\widehat{P}\\), namely \\(\\widehat{p}\\), which will lead to an observed instance of our test statistic \\[ \\mathrm{ts} = \\frac{\\widehat{p} - p_0}{\\sqrt{ \\frac{p_0 (1 - p_0)}{n}}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#test-statistic-2",
    "href": "Pages/Lectures/Lecture14/Lec14.html#test-statistic-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nLet’s try and convert our decision-making process to be in terms of the test statistic.\nFirst, note that saying \\(\\widehat{p}\\) is “far away” from \\(p_0\\) could mean one of two things:\n\n\\(\\widehat{p}\\) was much larger than \\(p_0\\)\n\\(\\widehat{p}\\) was much smaller than \\(p_0\\)\n\nThese two cases can be combined into a single case if we think in terms of the magnitude of the distance bewteen \\(\\widehat{p}\\) and \\(p_0\\), which is equivalent to considering \\(|\\mathrm{ts}|\\).\nWhat I’m getting at is this: if \\(\\widehat{p}\\) was far away from \\(p_0\\), then \\(|\\mathrm{ts}|\\) must be large.\nHence, we can rephrase our decision process as \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if $|\\mathrm{TS}|$ is large} \\\\  \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#constructing-the-test",
    "href": "Pages/Lectures/Lecture14/Lec14.html#constructing-the-test",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Constructing the Test",
    "text": "Constructing the Test\n\nOkay, but how large is “large”?\nThat is, for what values of \\(|\\mathrm{TS}|\\) will we reject the null?\n\nBy the way, the set of values that correspond to a decision of reject is called the rejection region of a test.\n\nIn other words, if our test takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| &gt; c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] what value should we take \\(c\\) to be?"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-errors",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-errors",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Errors",
    "text": "The Errors\n\nWell, to answer this question, we need to return to our considerations of Type II and Type II errors.\nRecall that a Type I error occurs when we reject \\(H_0\\) when \\(H_0\\) was actually true, and a Type II error occurs when we fail to reject \\(H_0\\) when \\(H_0\\) was false.\nChanging the value of \\(c\\) changes the probability of committing the two types of errors!\nSpecifically, setting a larger value of \\(c\\) corresponds to rejecting \\(H_0\\) for fewer values, thereby decreasing the probability of committing a Type I errror but increasing the probability of committing a Type II error.\nConversely, setting a smaller value of \\(c\\) corresponds to rejecting \\(H_0\\) for more values, thereby increasing the probability of committing a Type I error but decreasing the probability of committing a Type II error."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-compromise",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-compromise",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Compromise",
    "text": "The Compromise\n\nWe need to compromise!\nIn practice, we go into the test knowing how much leeway we are going to allow ourselves to commit a Type I error. That is, we prespecify our tolerance for committing a Type I error.\nThe probability of committing a Type I error is called the level of significance (or just significance level), and is often denoted \\(\\alpha\\).\nStatisticians therefore construct a hypothesis test around a specific value of \\(\\alpha\\).\nA common level of significance is \\(\\alpha = 0.05\\), though \\(\\alpha = 0.01\\) and \\(\\alpha = 0.1\\) are sometimes used as well.\nOkay, so what does this mean for our test?"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-compromise-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-compromise-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Compromise",
    "text": "The Compromise\n\nWe now know that \\(\\alpha\\) denotes the probability of rejecting the null when the null is true; i.e. \\[ \\mathbb{P}_{H_0}(|\\mathrm{TS}| &gt; c) = \\alpha \\] where the symbol \\(\\mathbb{P}_{H_0}\\) just means “assuming the null, the probability of….”"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-compromise-2",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-compromise-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Compromise",
    "text": "The Compromise\n\nAgain, remember that \\(\\alpha\\) is fixed (e.g. \\(0.05\\)); it is the value of \\(c\\) we are after!\nSo, a natural question arises: what is the distribution of \\(\\mathrm{TS}\\) under the null?\nRecall that \\[ \\mathrm{TS} = \\frac{\\widehat{P} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} \\]\nNow, assuming the null is true (i.e. that \\(p = p_0\\)), the Central Limit Theorem for Proportions tells us \\[ \\widehat{P} \\stackrel{H_0}{\\sim} \\mathcal{N}\\left(p_0, \\ \\sqrt{\\frac{p_0(1 - p_0)}{n}} \\right) \\] where the symbol \\(\\stackrel{H_0}{\\sim}\\) is just a shorthand for “distributed as, under the null”"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-distribution-of-the-test-statistic",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-distribution-of-the-test-statistic",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Distribution of the Test Statistic",
    "text": "The Distribution of the Test Statistic\n\nTherefore, assuming the null is correct, we have \\[ \\mathrm{TS} \\sim \\mathcal{N}(0, \\ 1)\\]\nSo, our condition \\[ \\mathbb{P}_{H_0}(|\\mathrm{TS}| &gt; c) = \\alpha \\] which, by the symmetry of the standard normal distribution, is equivalent to \\[ \\mathbb{P}_{H_0}(\\mathrm{TS} &lt; -c) = \\frac{\\alpha}{2} \\]\nHence, \\(-c\\) is just the \\((\\alpha / 2) \\times 100\\) percentile of the standard normal distribution!!!"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-test",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-test",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Test",
    "text": "The Test\n\n\n\n\n\n\n\nTwo-Sided Test for a Proportion:\n\n\n\n\nWhen testing \\(H_0: \\ p = p_0\\) vs \\(H_A: \\ p \\neq p_0\\) at an \\(\\alpha\\) level of significance, where \\(p\\) denotes a population proportion, the test takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| &gt; z_{1 - \\alpha/2} \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where:\n\n\\(\\displaystyle \\mathrm{TS} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\\)\n\\(z_{1 - \\alpha/2}\\) denotes the \\((\\alpha/2) \\times 100\\)th percentile of the standard normal distribution, scaled by negative 1 (which is equivalent to the \\((1 - \\alpha/2) \\times 100\\)th percentile)\n\nprovided that: \\(n p_0 \\geq 10\\) and \\(n (1 - p_0) \\geq 10\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-test-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-test-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Test",
    "text": "The Test\n\nFor example, if \\(\\alpha = 0.05\\) then \\(c\\) is negative 1 times the 2.5th percentile of the standard normal distribution; i.e. 1.96 and our test becomes \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| &gt; 1.96 \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]\n\n\nCAUTION!!!\n\n\nAll of this is predicated on our invocation of the Central Limit Theorem for Proportions!\nIn other words, the test above was derived assuming \\[ \\widehat{P} \\stackrel{H_0}{\\sim} \\mathcal{N}\\left(p_0, \\ \\sqrt{\\frac{p_0(1 - p)}{n}} \\right) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-assumptions",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-assumptions",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Assumptions",
    "text": "The Assumptions\n\n\nThis is not always true!\n\n\n\nWhen is this true? In other words, what conditions do we need in order for the above distributional statement to be true?\n\nThat’s right, the success-failure conditions.\n\nBut now, since we only need the above to be true for \\(p = p_0\\), we only need to verify that:\n\n\\(n p_0 \\geq 10\\)\n\\(n (1 - p_0) \\geq 10\\)\n\nIt is very important we check these conditions before conducting our test!"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-test-2",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-test-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Test",
    "text": "The Test\n\n\n\n\n\n\n\nTwo-Sided Test for a Proportion:\n\n\n\nWhen testing \\(H_0: \\ p = p_0\\) vs \\(H_A: \\ p \\neq p_0\\) at an \\(\\alpha\\) level of significance, where \\(p\\) denotes a population proportion:\n\nCheck that the success-failure conditions hold. Namely, check that:\n\n\\(n p_0 \\geq 10\\)\n\\(n (1 - p_0) \\geq 10\\)\n\nCompute the observed value of the test statistic \\[\\displaystyle \\mathrm{ts} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\\]\nCompute the critical value \\(z_{1 - \\alpha/2}\\), which is the the \\((\\alpha/2) \\times 100\\)th percentile of the standard normal distribution, scaled by negative 1 (or simply the \\((1 - \\alpha/2) \\times 100\\)th percentile)\nReject \\(H_0\\) if \\(|\\mathrm{TS}| &gt; z_{1 - \\alpha/2}\\), and fail to reject \\(H_0\\) otherwise."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle. To test that claim, we take a representative sample of 500 US households and observe that 89.4% of these households own a vehicle.\nConduct a two-sided hypothesis test at a \\(5\\%\\) level of significance on Forbes’s claim that 91.7% of US households own a vehicle. Be sure you phrase your conclusion clearly, and in the context of the problem."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions-2",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nAll we really need to do is follow the steps outlined in the previous slide.\n\n\nCheck Conditions\n\n\\(n p_0 = 500 \\cdot (0.917) = 458.5 \\geq 10 \\ \\checkmark\\)\n\\(n (1 - p_0) = 500 \\cdot (1 - 0.917) = 41.5 \\geq 10 \\ \\checkmark\\)\nSince both conditions are met, we can proceed.\n\nCompute the Observed Value of the Test Statistic \\[ \\mathrm{ts} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} = \\frac{0.894 - 0.917}{\\sqrt{\\frac{0.917 (1 - 0.917)}{500}}} = -1.86 \\]\nCompute the critical value Because \\(\\alpha = 0.05\\), the critical value is \\(1.96\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions-3",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions-3",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nConduct the test: We see that \\(|\\mathrm{TS}| = |-1.86| = 1.86\\) which is not greater than \\(1.96\\). As such, we fail to reject the null.\n\n\nNow, the problem told us to phrase our conclusions carefully and in the context of the problem.\nIt is VERY important to include the level of significance in your final conclusions.\nSo, here is how we would phrase the final conclusion of our test:\n\n\n\nAt an \\(\\alpha = 0.05\\) level of significance, there is insufficient evidence to reject Forbes’s claim that 91.7% of US households own a vehicle in favor of the alternative that the true proportion of US households that own a vehicle is not 91.7%.\n\n\n\nIt is very important to include the level of significance in our conclusion! This is because the final outcome of our test may change depending on which level of significance we use."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example-4",
    "href": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example-4",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle. To test that claim, we take a representative sample of 500 US households and observe that 89.4% of these households own a vehicle.\nConduct a two-sided hypothesis test at a \\(10\\%\\) level of significance on Forbes’s claim that 91.7% of US households own a vehicle. Be sure you phrase your conclusion clearly, and in the context of the problem."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions-4",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions-4",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nThe only thing that will change from before is our critical value.\nSince we are using an \\(\\alpha = 0.1\\) level of significance, we find the \\([1 - (0.1 / 2)] \\times 100\\)th percentile (which is equivalent to the \\((0.1 / 2) \\times 100\\)th percentile, scaled by negative 1).\nThere are several ways we could find this critical value.\nThe first is to use our normal table: \\(1.645\\).\nThe second is to use our \\(t-\\)table: \\(1.645\\)\nThe third is to use Python:\n\n\n\nimport scipy.stats as sps\n-sps.norm.ppf(0.1 / 2)\n\n1.6448536269514729"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions-contd-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions-contd-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions (cont’d)",
    "text": "Solutions (cont’d)\n\nHence, we adopt a critical value of \\(1.645\\).\nSince the observed value of our test statistic \\(|\\mathrm{ts}| = 1.86\\) is now larger than the critical value, we reject the null.\n\n\n\nAt an \\(\\alpha = 0.1\\) level of significance, there is evidence to reject Forbes’s claim that 91.7% of US households own a vehicle in favor of the alternative that the true proportion of households that own a vehicle is not 91.7%.\n\n\n\nIntuitively, it makes sense why we might reject for this new level of significance.\nWe know that the level of significance represents the probability of committing a Type I Error.\nIf we increase this value (which we did, in adopting \\(\\alpha = 0.1\\) as opposed to \\(\\alpha = 0.05\\) in the previous example), we are allowing a greater chance of falsely rejecting the null, which comes part-in-parcel with rejecting for more values of the test statistic."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#one-sided-tests",
    "href": "Pages/Lectures/Lecture14/Lec14.html#one-sided-tests",
    "title": "PSTAT 5A: Lecture 14",
    "section": "One-Sided Tests",
    "text": "One-Sided Tests\n\nOn the homework, I ask you to walk through the details of deriving a lower-tailed test.\nYou can use a similar set of arguments to derive the upper-tailed test as well.\nFor posterity’s sake, here are the final results of each:"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#lower-tailed-test",
    "href": "Pages/Lectures/Lecture14/Lec14.html#lower-tailed-test",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Lower-Tailed Test",
    "text": "Lower-Tailed Test\n\n\n\n\n\n\n\nLower-Tailed Test for a Proportion:\n\n\n\n\nWhen testing \\(H_0: \\ p = p_0\\) vs \\(H_A: \\ p &lt; p_0\\) at an \\(\\alpha\\) level of significance, where \\(p\\) denotes a population proportion, the test takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\mathrm{TS} &lt; z_{\\alpha} \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where:\n\n\\(\\displaystyle \\mathrm{TS} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\\)\n\\(z_{\\alpha}\\) denotes the \\((\\alpha) \\times 100\\)th percentile of the standard normal distribution, not scaled by anything\n\nprovided that: \\(n p_0 \\geq 10\\) and \\(n (1 - p_0) \\geq 10\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#upper-tailed-test",
    "href": "Pages/Lectures/Lecture14/Lec14.html#upper-tailed-test",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Upper-Tailed Test",
    "text": "Upper-Tailed Test\n\n\n\n\n\n\n\nUpper-Tailed Test for a Proportion:\n\n\n\n\nWhen testing \\(H_0: \\ p = p_0\\) vs \\(H_A: \\ p &gt; p_0\\) at an \\(\\alpha\\) level of significance, where \\(p\\) denotes a population proportion, the test takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\mathrm{TS} &gt; z_{1 - \\alpha} \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where:\n\n\\(\\displaystyle \\mathrm{TS} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\\)\n\\(z_{1 - \\alpha}\\) denotes the \\((1 - \\alpha) \\times 100\\)th percentile of the standard normal distribution, not scaled by anything\n\nprovided that: \\(n p_0 \\geq 10\\) and \\(n (1 - p_0) \\geq 10\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#note",
    "href": "Pages/Lectures/Lecture14/Lec14.html#note",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Note",
    "text": "Note\n\nI don’t recommend you try and memorize all of the different percentiles and critical values.\nInstead, I recommend you familiarize yourself with the process used to derive these tests, as that will immediately tell you what picture to draw, which will in turn tell you which quantile we are after in a given situation.\nThere are some problems on the practice problems that give you practice with these one-sided alternatives."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#hypothesis-testing-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#hypothesis-testing-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nRecall that in the framework of hypothesis testing, we wish to utilize data to assess the plausibility/validity of a hypothesis, called the null hypothesis.\n\nSpecifically, we wish to determine whether or not the data provides support to reject the null in favor of the alternative hypothesis or not.\n\nIn the case of hypothesis testing for a population proportion \\(p\\), our null takes the form \\(H_0: p = p_0\\) and there are several different alternative hypotheses we could consider:\n\nTwo-tailed alternative/test: \\(H_A: \\ p \\neq p_0\\)\nLower-tailed alternative/test: \\(H_A: \\ p &lt; p_0\\)\nUpper-tailed alternative/test: \\(H_A: \\ p &gt; p_0\\)\nSimple-vs-simple alternative/test: \\(H_A: \\ p = p_1\\) for \\(p_1 \\neq p_0\\)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#hypothesis-testing-for-a-proportion",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#hypothesis-testing-for-a-proportion",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Hypothesis Testing for a Proportion",
    "text": "Hypothesis Testing for a Proportion\n\nThe different alternative hypotheses lead to different forms of our hypothesis test.\nLast lecture, we discussed how to construct a two-sided hypothesis test.\nOn the homework, I ask you to derive the lower-tailed hypothesis test (and I also provide you with the form for an upper-tailed hypothesis test in the last lecture slide deck)\nTo help you with that homework problem, allow me to re-do the derivations we did last lecture, but this time more in the style of the homework problem."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Two-Sided Test for a Proportion",
    "text": "Two-Sided Test for a Proportion\n\nWe start with the test statistic \\[ \\mathrm{TS} = \\frac{\\widehat{P} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} \\]\nIf the null were true, i.e. if the true value of \\(p\\) were \\(p_0\\), then this test statistic would follow the Normal Distribution provided that we are able to invoke the Central Limit Theorem for proportions, which we can do only if\n\n\\(np_0 \\geq 10\\)\n\\(n(1 - p_0) \\geq 10\\)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Two-Sided Test for a Proportion",
    "text": "Two-Sided Test for a Proportion\n\nIf these conditions hold, we then know that (by the Central Limit Theorem for Proportions) \\[ \\widehat{P} \\stackrel{H_0}{\\sim} \\mathcal{N}\\left( p_0, \\ \\sqrt{\\frac{p_0 (1 - p_0)}{n}} \\right) \\] which means, by our Standardization Result, that \\[\\mathrm{TS} \\stackrel{H_0}{\\sim} \\mathcal{N}(0, \\ 1)\\] where the symbol \\(\\stackrel{H_0}{\\sim}\\) just means “distributed under the null”; i.e. assuming the null hypothesis is correct."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-2",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Two-Sided Test for a Proportion",
    "text": "Two-Sided Test for a Proportion\n\nNow, let’s think about when we would reject the null that \\(p = p_0\\) in favor of the alternative that \\(p \\neq p_0\\).\n\nIf we observed a value of \\(\\widehat{p}\\) that was much greater than \\(p_0\\) (i.e. if \\(\\mathrm{ts}\\) was much larger than 0), we would probably reject the null in favor of the alternative.\nBut, if we observed a value of \\(\\widehat{p}\\) that was much smaller than \\(p_0\\) (i.e. if \\(\\mathrm{ts}\\) was much smaller than 0), we would probably also reject the null in favor of the alternative.\nTherefore, we reject when the magnitude of \\(\\mathrm{ts}\\) is large; i.e. with \\(|\\mathrm{ts}| &gt; c\\) for some constant \\(c\\).\n\nThis means our test should take the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| &gt; c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-3",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-3",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Two-Sided Test for a Proportion",
    "text": "Two-Sided Test for a Proportion\n\nFinally, we need to consider how large to make our cutoff (or critical value) \\(c\\).\nThis is where we need to consider the two types of errors we could commit: Type I and Type II Errors\n\nType I error: the null was rejected when it was in fact true (convicting an innocent person)\nType II error: the null was not rejected when it was in fact false (letting a guilty person go free)\n\nWe call the probability of committing a Type I error the level of significance \\(\\alpha\\), which we fix before beginning our testing procedure.\nIn terms of the critical value, this means \\(c\\) should satisfy the equation \\[ \\mathbb{P}_{H_0}(|\\mathrm{TS}| &gt; c) = \\alpha \\] where \\(\\mathbb{P}_{H_0}\\) means “probability of, assuming the null is actually true (i.e. that \\(p = p_0\\))”."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-4",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-4",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Two-Sided Test for a Proportion",
    "text": "Two-Sided Test for a Proportion\n\nSince we know that \\(\\mathrm{TS}\\) follows a standard normal distribution under the null, this means that \\(c\\) should be the \\((\\alpha / 2) \\times 100\\)th percentile of the standard normal distribution.\n\nFor example, if \\(\\alpha = 0.05\\), then the critical value is the \\((0.05) / 2 \\times 100 = 2.5\\)th percentile of the standard normal distribution, which we see to be \\(1.96\\).\n\nAs such, the final form of our test is \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| &gt; z_{1 - \\alpha/2} \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where we use the notation \\(z_{1 - \\alpha/2}\\) to denote the following (which, recall, are equivalent):\n\n\\((\\alpha / 2) \\times 100\\)th percentile of the standard normal distribution, scaled by negative 1.\n\\([1 - (\\alpha / 2)] \\times 100\\)th percentile, not scaled by anything."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#quick-note",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#quick-note",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Quick Note",
    "text": "Quick Note\n\nAgain, it turns out that the \\((\\alpha / 2) \\times 100\\)th percentile of the standard normal distribution scaled by negative 1 is equivalent to the \\((1 - \\alpha / 2)\\)th percentile of the standard normal distribution, not scaled by negative 1.\n\n\n\nimport scipy.stats as sps\n\n-sps.norm.ppf(0.05/2)\n\n1.9599639845400545\n\n\n\n\n\nimport scipy.stats as sps\n\nsps.norm.ppf(1 - 0.05/2)\n\n1.959963984540054\n\n\n\n\nBasically, when doing a two-sided test, ensure that the critical value is positive. (This is not the case for a one-sided test.)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#worked-out-example",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nAdministration within a Statistics department at an unnamed university claims to admit 24% of all applicants. A disgruntled student, dubious of the administration’s claims, takes a representative sample of 120 students who applied to the Statistics major, and found that 20% of these students were actually admitted into the major.\n\nConduct a two-sided hypothesis test at a 5% level of significance on the administrator’s claims that 24% of applicants into the Statistics major are admitted. Be sure you phrase your conclusion clearly, and in the context of the problem."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#solutions",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#solutions",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nWe first phrase the hypotheses.\nLet \\(p\\) denote the true proportion of applicants who get admitted into the major. Since we are performing a two-sided test, our hypotheses take the form \\[ \\left[ \\begin{array}{rr}\nH_0:    p = 0.24    \\\\\nH_A:    p \\neq 0.24\n\\end{array} \\right.\\]\nNow we compute the observed value of the test statistic: \\[ \\mathrm{ts} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}} = \\frac{0.2 - 0.24}{\\sqrt{\\frac{(0.24) \\cdot (1 - 0.24)}{120}}} \\approx -1.026 \\]\nNext, we compute the critical value. Since we are using an \\(\\alpha = 0.05\\) level of significance, we will use the critical value \\(1.96\\)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#solutions-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#solutions-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nFinally, we perform the test: we will reject the null in favor of the alternative if \\(|\\mathrm{ts}|\\) is larger than the critical value.\nIn this case, \\(|\\mathrm{ts}| = |-1.026| = 1.026 &lt; 1.96\\) meaning we fail to reject the null:\n\n\n\nAt an \\(\\alpha = 0.05\\) level of significance, there was insufficient evidence to reject the null hypothesis that 24% of applicants are admitted into the major in favor of the alternative that the true admittance rate was not 24%."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#basics-of-random-variables",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#basics-of-random-variables",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Basics of Random Variables",
    "text": "Basics of Random Variables\n\nRecall long ago we discussed the notion of an experiment: any procedure we can repeat an infinite number of times where each time we repeat the experiment the same fixed set of things (i.e. the outcomes) can occur.\nA random variable, loosely speaking, is some sort of numerical variable that keeps track of certain quantities relating to an experiment.\nFor example, if we toss 7 coins and let \\(X\\) denote the number of heads we observe in these 7 coin tosses, then \\(X\\) would be a random variable.\nThe set of all values a random variable can attain is called the state space, and is denoted \\(S_X\\).\nWe classify random variables based on their state space:\n\nIf \\(S_X\\) has jumps, we say \\(X\\) is a discrete random variable\nIf \\(S_X\\) does not have jumps, we say \\(X\\) is a continuous random variable"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#discrete-random-variables",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#discrete-random-variables",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\nDiscrete random variables are described/summarized by a probability mass function (p.m.f.), which is a specification of the values the random variable can take (i.e. the state space) along with the probabilities with which the random variable attains those values.\n\nP.M.F.’s are often displayed in tabular form: e.g. \\[ \\begin{array}{r|cccc}\n\\boldsymbol{k}              & -1    & 0   & 1   & 2   \\\\\n\\hline\n\\boldsymbol{\\mathbb{P}(X = k)}   & 0.1   & 0.2   & 0.3   & 0.4\n  \\end{array}\\]\nNote that the probability values in a P.M.F. must sum to 1.\n\nQuantities like \\(\\mathbb{P}(X \\leq k)\\) are found by summing up the values of \\(\\mathbb{P}(X = x)\\) for all values of \\(x\\) in the state space that are less than \\(k\\).\n\nFor example, in the example above, \\[\\mathbb{P}(X \\leq 0.5) = \\mathbb{P}(X = -1) + \\mathbb{P}(X = 0)  = 0.1 + 0.2 = 0.3 \\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#expected-value",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#expected-value",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Expected Value",
    "text": "Expected Value\n\nThe expected value of a random variable \\(X\\), denoted \\(\\mathbb{E}[X]\\), represents a sort of “average” of \\(X\\), and is computed as \\[ \\mathbb{E}[X] = \\sum_{\\text{all $k$}} k \\cdot \\mathbb{P}(X = k) \\]\n\nAgain, don’t be scared by the sigma notation! It just represents a sum.\nSo, for example, using our P.M.F. from the previous slide, \\[\\begin{align*}\n\\mathbb{E}[X]   & = (-1) \\cdot \\mathbb{P}(X = -1) + (0) \\cdot \\mathbb{P}(X = 0)    \\\\\n& \\hspace{10mm} + (1) \\cdot \\mathbb{P}(X = 1) + (2) \\cdot \\mathbb{P}(X = 2)    \\\\[3mm]\n  & = (-1) \\cdot (0.1) + (0) \\cdot (0.2) + (1) \\cdot (0.3) + (2) \\cdot (0.4)     \\\\[3mm]\n  & = \\boxed{1}\n  \\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#variance-and-standard-deviation",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#variance-and-standard-deviation",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Variance and Standard Deviation",
    "text": "Variance and Standard Deviation\n\nThere are two formulas we can use for the variance of a random variable \\(X\\): \\[ \\mathrm{Var}(X) = \\sum_{\\text{all $k$}} (k - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = k) \\] or \\[ \\mathrm{Var}(X) = \\left(\\sum_{\\text{all $k$}} k^2 \\cdot \\mathbb{P}(X = k) \\right) - (\\mathbb{E}[X])^2 \\]\nThe standard deviation of a random variable is simply the square root of the variance: \\[ \\mathrm{SD}(X) = \\sqrt{\\mathrm{Var}(X)} \\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#variance-and-standard-deviation-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#variance-and-standard-deviation-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Variance and Standard Deviation",
    "text": "Variance and Standard Deviation\n\nFor example, using the PMF from a few slides ago, the first formula for variance tells us to compute \\[\\begin{align*}\n\\mathrm{Var}(X)     & = \\sum_{\\text{all $k$}} (k - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = k)    \\\\\n  & = (-1 - 1)^2 \\cdot \\mathbb{P}(X = -1) + (0 - 1)^2 \\cdot \\mathbb{P}(X = 0)    \\\\\n    & \\hspace{10mm} + (1 - 1)^2 \\cdot \\mathbb{P}(X = 1) + (2 - 1)^2 \\cdot \\mathbb{P}(X = 2)    \\\\[3mm]\n      & = (-1 - 1)^2 \\cdot (0.1) + (0 - 1)^2 \\cdot (0.2)    \\\\\n      & \\hspace{10mm}+ (1 - 1)^2 \\cdot (0.3) + (2 - 1)^2 \\cdot (0.4)   \\\\[3mm]\n      & = \\boxed{1}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#variance-and-standard-deviation-2",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#variance-and-standard-deviation-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Variance and Standard Deviation",
    "text": "Variance and Standard Deviation\n\n\nUsing the second formula for variance, we first compute \\[\\begin{align*}\n\\sum_{\\text{all $k$}} k^2 \\mathbb{P}(X = k)    &  = (-1)^2 \\cdot \\mathbb{P}(X = -1) + (0)^2 \\cdot \\mathbb{P}(X = 0)    \\\\\n    & \\hspace{10mm} + (1)^2 \\cdot \\mathbb{P}(X = 1) + (2 - 1)^2 \\cdot \\mathbb{P}(X = 2)    \\\\[3mm]\n      & = (-1)^2 \\cdot (0.1) + (0)^2 \\cdot (0.2)    \\\\\n      & \\hspace{10mm}+ (1)^2 \\cdot (0.3) + (2)^2 \\cdot (0.4)   \\\\[3mm]\n      & = 2\n\\end{align*}\\] which means \\[ \\mathrm{Var}(X) = 2 - (1)^2 = \\boxed{1}\\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#binomial-distribution",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#binomial-distribution",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nSuppose we have \\(n\\) independent trials, each resulting in “success” with probability \\(p\\) and “failure” with probability \\(1 - p\\). If \\(X\\) denotes the number of successes in these \\(n\\) trials, we say \\(X\\) follow the Binomial distribution with parameters \\(n\\) and \\(p\\), notated \\[ X \\sim \\mathrm{Bin}(n, \\ p) \\]\nIf \\(X \\sim \\mathrm{Bin}(n, \\ p)\\), then:\n\n\\(S_X = \\{0, 1, 2, \\cdots, n\\}\\)\n\\(\\mathbb{P}(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}\\)\n\\(\\mathbb{E}[X] = np\\)\n\\(\\mathrm{Var}(X) = np(1 - p)\\)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#binomial-distribution-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#binomial-distribution-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nIn order to verify that the Binomial distribution is appropriate to use, we need to check the Binomial Criteria:\n\nIndependence across trials\nFixed number \\(n\\) of trials\nWell-defined notion of “success” and “failure”\nFixed probability \\(p\\) of success across trials.\n\nIf you are going to use the Binomial distribution in a problem, you must check all four of these!"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#case-study-airline-bookings",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#case-study-airline-bookings",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Case Study: Airline Bookings",
    "text": "Case Study: Airline Bookings"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#worked-out-example-2",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nSuppose that GauchoAir has found that each passenger that books a ticket on the GA5A flight from SBA to GCV (GauchoVille) actually shows up with probability 90%.\nIf flight GA5A has only 186 seats, but sells 195 tickets, what is the probability that GauchoAir will need to re-book certain passengers?"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#solution",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#solution",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solution",
    "text": "Solution\n\nAs always, we start by defining quantities.\nLet \\(X\\) denote the number of passengers, out of the 195 booked on the flight, that actually show up for the flight.\nThe video claims that \\(X\\) follows a Binomial Distribution- let’s work with that claim for a moment (and then we can revisit that assumption later).\nSpecifically, \\(X \\sim \\mathrm{Bin}(195, \\ 0.9)\\).\nNow, the airline will only need to re-book passengers when the number of passengers that arive (\\(X\\)) exceeds the capacity of the plane (186).\nSo, the quantity we seek is \\(\\mathbb{P}(X &gt; 186) = \\mathbb{P}(X \\geq 187)\\).\nThough we could do this by hand, let’s use Python.\nWe can also discuss whether we think the Binomial criteria really are satisfied in this case or not."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#continuous-random-variables",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#continuous-random-variables",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nNot all random variables are discrete- many are continuous!\nContinuous random variables are characterized by a probability density function (pdf) \\(f_X(x)\\), which must obey the following two properties:\n\n\\(f_X(x)\\) must be nonnegative everywhere\nThe area underneath the graph of \\(f_X(x)\\) must be 1\n\nThe graph of a pdf is called a density curve\nProbabilities are found as areas underneath the density curve."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#areas-under-the-density-curve",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#areas-under-the-density-curve",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Areas Under the Density Curve",
    "text": "Areas Under the Density Curve\n\n\n\n\nFor example, the area above represents \\(\\mathbb{P}(0.25 \\leq X \\leq 0.75)\\)."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#uniform-distribution",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#uniform-distribution",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\nAn specific example of a continuous distribution is the Uniform distribution with parameters \\(a\\) and \\(b\\): \\(X \\sim \\mathrm{Unif}(a, \\ b)\\).\nThe p.d.f. is given by \\[ f_X(x) = \\begin{cases} \\frac{1}{b - a} & \\text{if } a \\leq x \\leq b \\\\ 0 & \\text{otherwise} \\\\ \\end{cases} \\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#effect-of-changing-a-and-b",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#effect-of-changing-a-and-b",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Effect of Changing \\(a\\) and \\(b\\)",
    "text": "Effect of Changing \\(a\\) and \\(b\\)\n\nviewof a = Inputs.range(\n  [-3, 3], \n  {value: 0, step: 0.1, label: \"a=\"}\n)\n\nviewof b = Inputs.range(\n  [-3, 3], \n  {value: 1, step: 0.1, label: \"b=\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmargin2 = ({top: 20, right: 30, bottom: 30, left: 40})\n\nheight2 = 400\n\nx_values2 = d32.scaleLinear()\n.domain(d32.extent(data2, d =&gt; d.x))\n.range([margin2.left, width - margin2.right])\n\ny_values2 = d32.scaleLinear()\n.domain([Math.min(d32.min(data2, d =&gt; d.y),0), Math.max(1,d32.max(data2, d =&gt; d.y))]).nice()\n.range([height2 - margin2.bottom, margin2.top])\n\nline2 = d32.line()\n.x(d =&gt; x_values2(d.x))\n.y(d =&gt; y_values2(d.y))\n\nxAxis2 = g =&gt; g\n.attr(\"transform\", `translate(0,${height2 - margin2.bottom})`)\n.call(d32.axisBottom(x_values2)\n      .ticks(width / 80)\n      .tickSizeOuter(0))\n\nyAxis2 = g =&gt; g\n.attr(\"transform\", `translate(${margin2.left},0)`)\n.call(d32.axisLeft(y_values2)\n      .tickValues(d32.scaleLinear().domain(y_values2.domain()).ticks()))\n\nfunction unif_pdf (input_value, mu, sigsq) {\nif(input_value &lt; a){\n  return 0\n} else if(input_value &gt; b){\n  return 0\n} else{\n  return 1 / (b - a)\n}\n}\n\nabs_x2=6\n\ndata2 = {\n  let values = [];\n  for (let x = -abs_x2; x &lt; abs_x2; x=x+0.01) values.push({\"x\":x,\"y\":unif_pdf(x, µ, sigsquared)});\n  return values;\n}\n\nd32 = require(\"https://d3js.org/d3.v5.min.js\")\n\nchart2 = {\n  const svg = d32.select(DOM.svg(width, height2));\n  \n  svg.append(\"g\")\n  .call(xAxis2);\n  \n  svg.append(\"g\")\n  .call(yAxis2);\n  \n  svg.append(\"path\")\n  .datum(data2)\n  .attr(\"fill\", \"none\")\n  .attr(\"stroke\", \"steelblue\")\n  .attr(\"stroke-width\", 4)\n  .attr(\"stroke-linejoin\", \"round\")\n  .attr(\"stroke-linecap\", \"round\")\n  .attr(\"d\", line);\n  \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit to https://observablehq.com/@dswalter/normal-distribution for the base of the applet code"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#uniform-distribution-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#uniform-distribution-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\nThe expected value and variance are: \\[ \\mathbb{E}[X] = \\frac{a + b}{2} ; \\quad \\mathrm{Var}(X) = \\frac{(b - a)^2}{12} \\]\nAgain, probabilities are found as areas underneath the density curve:"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#tail-probabilities",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#tail-probabilities",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Tail Probabilities",
    "text": "Tail Probabilities\n\nVisualizing probabilities as areas also enables us to write more complicated probabilistic expressions as differences of tail probabilities:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncan be decomposed as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[ \\huge - \\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[ \\mathbb{P}(a \\leq X \\leq b) = \\underbrace{\\mathbb{P}(X \\leq b)}_{\\text{c.d.f. at $b$}} - \\underbrace{\\mathbb{P}(X \\leq a)}_{\\text{c.d.f. at $a$}} \\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#challenge-problem-try-it-on-your-own",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#challenge-problem-try-it-on-your-own",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Challenge Problem: Try It On Your Own!",
    "text": "Challenge Problem: Try It On Your Own!\n\n\n\n\n\n\n\nTake-Home Exercise 1\n\n\n\n\nThe time (in minutes) spent waiting in line at Starbucks is found to vary uniformly between 5mins and 15mins.\nWhat is the c.d.f. of wait times? (I.e., find the probability that a randomly selected person spends less than \\(x\\) minutes waiting in line, for an arbitrary value \\(x\\). Yes, your final answer will depend on \\(x\\); that’s why the c.d.f. is a function!)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#chalkboard-example",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#chalkboard-example",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Chalkboard Example",
    "text": "Chalkboard Example\n\n\n\n\n\n\n\nChalkboard Example 2\n\n\n\n\nThe time (in minutes) spent waiting in line at Starbucks is found to vary uniformly between 5mins and 15mins.\nA random sample of 10 customers is taken; what is the probability that exactly 4 of these customers will spend between 10 and 13 minutes waiting in line?"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#normal-distribution",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#normal-distribution",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\nWe also learned about the Normal Distribution: \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\)\nThe normal density curve is bell-shaped"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#changing-mu-and-sigma",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#changing-mu-and-sigma",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Changing \\(\\mu\\) and \\(\\sigma\\)",
    "text": "Changing \\(\\mu\\) and \\(\\sigma\\)\n\nviewof µ = Inputs.range(\n  [-3, 3], \n  {value: 0, step: 0.1, label: \"µ:\"}\n)\n\nviewof σ = Inputs.range(\n  [0.2, 3.1], \n  {value: 1, step: 0.01, label: \"σ:\"}\n)\n\nsigsquared = σ**2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmargin = ({top: 20, right: 30, bottom: 30, left: 40})\n\nheight = 400\n\nx_values = d3.scaleLinear()\n    .domain(d3.extent(data, d =&gt; d.x))\n    .range([margin.left, width - margin.right])\n\ny_values = d3.scaleLinear()\n    .domain([Math.min(d3.min(data, d =&gt; d.y),0), Math.max(1,d3.max(data, d =&gt; d.y))]).nice()\n    .range([height - margin.bottom, margin.top])\n    \nline = d3.line()\n    .x(d =&gt; x_values(d.x))\n    .y(d =&gt; y_values(d.y))\n\nxAxis = g =&gt; g\n  .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n  .call(d3.axisBottom(x_values)\n      .ticks(width / 80)\n      .tickSizeOuter(0))\n\nyAxis = g =&gt; g\n  .attr(\"transform\", `translate(${margin.left},0)`)\n  .call(d3.axisLeft(y_values)\n      .tickValues(d3.scaleLinear().domain(y_values.domain()).ticks()))\n    \nfunction normal_pdf (input_value, mu, sigsq) {\n  let left_chunk = 1/(Math.sqrt(2*Math.PI*sigsq))\n  let right_top = -((input_value-mu)**2)\n  let right_bottom = 2*sigsq\n  return left_chunk * Math.exp(right_top/right_bottom)\n}\n\nabs_x=6\n\ndata = {\n  let values = [];\n  for (let x = -abs_x; x &lt; abs_x; x=x+0.01) values.push({\"x\":x,\"y\":normal_pdf(x, µ, sigsquared)});\n  return values;\n}\n\nd3 = require(\"https://d3js.org/d3.v5.min.js\")\n\nchart = {\n  const svg = d3.select(DOM.svg(width, height));\n\n  svg.append(\"g\")\n      .call(xAxis);\n\n  svg.append(\"g\")\n      .call(yAxis);\n  \n  svg.append(\"path\")\n      .datum(data)\n      .attr(\"fill\", \"none\")\n      .attr(\"stroke\", \"steelblue\")\n      .attr(\"stroke-width\", 4)\n      .attr(\"stroke-linejoin\", \"round\")\n      .attr(\"stroke-linecap\", \"round\")\n      .attr(\"d\", line);\n  \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit to https://observablehq.com/@dswalter/normal-distribution for the majority of the applet code"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#standardization",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#standardization",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Standardization",
    "text": "Standardization\n\nIf \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\), \\[ \\left( \\frac{X - \\mu}{\\sigma} \\right) \\sim \\mathcal{N}(0, \\ 1)\\]\n\n\n\n\nAlso, if \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\):\n\n\\(\\mathbb{E}[X] = \\mu\\)\n\\(\\mathrm{Var}(X) = \\sigma^2\\)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#inferential-statistics-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#inferential-statistics-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\n\nThe primary goal of inferential statistics is to take samples from some population, and use summary statistics to try and make inferences about population parameters\nFor example, we could take samples, compute sample proportions \\(\\widehat{P}\\), and try to make inferences about the population proportion \\(p\\).\nWe could also take samples, compute sample means \\(\\overline{X}\\), and try to make inferences about the population mean \\(\\mu\\).\nOur summary statistics will often be point estimators (i.e. quantities that have expected value equal to the corresponding population parameter), which are random variables as they depend on the sample taken.\n\nFor example, different samples of people will have different average heights.\n\nThe distribution of a point estimator is called the sampling distribution of the estimator."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#sampling-distribution-of-widehatp",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#sampling-distribution-of-widehatp",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Sampling Distribution of \\(\\widehat{P}\\)",
    "text": "Sampling Distribution of \\(\\widehat{P}\\)\n\nGiven a population with population proportion \\(p\\), we use \\(\\widehat{P}\\) as a point estimator of \\(p\\).\nAssume the success-failure conditions are met; i.e.\n\n\\(n p \\geq 10\\)\n\\(n (1 - p) \\geq 10\\)\n\nThen, the Central Limit Theorem for Proportions tells us that \\[ \\widehat{P} \\sim \\mathcal{N}\\left(p, \\ \\sqrt{\\frac{p(1 - p)}{n}} \\right) \\]\nIf we don’t have access to \\(p\\) directly (as is often the case), we use the substitution approximation to check whether\n\n\\(n \\widehat{p} \\geq 10\\)\n\\(n (1 - \\widehat{p}) \\geq 10\\)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#sampling-distribution-of-overlinex",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#sampling-distribution-of-overlinex",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Sampling Distribution of \\(\\overline{X}\\)",
    "text": "Sampling Distribution of \\(\\overline{X}\\)\n\nGiven a population with population mean \\(\\mu\\) and population standard deviation \\(\\sigma\\), we use \\(\\overline{X}\\) as a point estimator of \\(\\mu\\).\nIf the population is normally distributed, then \\[ \\overline{X} \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{\\sigma}{\\sqrt{n}} \\right) \\] or, equivalently, \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}\\left(0, \\ 1 \\right) \\]\nIf the population is not normally distributed, but the sample size \\(n\\) is at least 30, then the Central Limit Theorem for the Sample Mean (or just the Central Limit Theorem) tells us \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}\\left(0, \\ 1 \\right) \\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#sampling-distribution-of-overlinex-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#sampling-distribution-of-overlinex-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Sampling Distribution of \\(\\overline{X}\\)",
    "text": "Sampling Distribution of \\(\\overline{X}\\)\n\nIf the population is non-normal, the sample size is large, and we don’t have access to \\(\\sigma\\) (but access to \\(s\\), the sample standard deviation instead), then \\[ \\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} \\sim t_{n - 1}\\]\nRecall that the \\(t-\\)distribution looks like a standard normal distribution, but has wider tails than the standard normal distribution (which accounts for the additional uncertainty injected into the problem by using \\(s\\), a random variable, in place of \\(\\sigma\\), a deterministic constant).\nAlso recall that \\(t_{\\infty}\\) (i.e. the \\(t-\\)distribution with an infinite degrees of freedom) is the same thing as the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#confidence-intervals",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#confidence-intervals",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nInstead of using point estimators (which are random) to estimate population parameters (which are deterministic), it may make more sense to provide an interval that, with some confidence level, contains the true parameter value.\nIn general, when constructing a confidence interval for a parameter \\(\\theta\\), we use \\[ \\widehat{\\theta} \\pm c \\cdot \\mathrm{SD}(\\widehat{\\theta}) \\] where \\(c\\) is some constant that depends on our confidence level.\n\nAgain, think of the fishing analogy from the textbook- if we want to be more certain we’ll catch a fish, we should cast a wider net; i.e. higher confidence levels will lead to wider intervals.\n\nThe coefficient \\(c\\) will also depend on the sampling distribution of \\(\\widehat{\\theta}\\)."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#confidence-intervals-for-a-population-proportion",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#confidence-intervals-for-a-population-proportion",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Confidence Intervals for a Population Proportion",
    "text": "Confidence Intervals for a Population Proportion\n\nTo construct a confidence interval for an unknown population proportion \\(p\\), we use \\[ \\widehat{p} \\pm (c) \\cdot \\sqrt{\\frac{\\widehat{p} \\cdot (1 - \\widehat{p})}{n}} \\] where \\(c\\) denotes the \\((1 - \\alpha) / 2 \\times 100\\)th percentile of the standard normal distribution, scaled by negative 1\n\nAgain, just remember that the coefficient should be positive"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#confidence-intervals-for-a-population-mean",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#confidence-intervals-for-a-population-mean",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Confidence Intervals for a Population Mean",
    "text": "Confidence Intervals for a Population Mean\n\nTo construct a confidence interval for an unknown population mean \\(\\mu\\), we use \\[ \\overline{x} \\pm (z^{\\ast}) \\cdot \\frac{\\sigma}{\\sqrt{n}} \\] or \\[ \\overline{x} \\pm (t^{\\ast}) \\cdot \\frac{s}{\\sqrt{n}} \\] depending on the conditions listed in the previous section of these slides."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#worked-out-example-3",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nSaoirse would like to construct a 95% confidence interval for the true proportion of California Residents that speak Spanish. To that end, she took a representative sample of 120 CA residents and found that 36 of these residents speak Spanish.\n\nIdentify the population\nDefine the parameter of interest.\nDefine the random variable of interest.\nConstruct a 95% confidence interval for the true proportion of CA residents that speak Spanish."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#solutions-2",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#solutions-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nThe population is the set of all California residents.\nThe parameter of interest is \\(p\\), the true proportion of CA residents that speak Spanish.\nThe random variable of interest is \\(\\widehat{P}\\), the proportion of people in a representative sample of 120 CA residents that speak spanish.\nWe check the success-failure conditions, with the substitution approximation:\n\n\\(n \\widehat{p} = (120) \\cdot \\left( \\frac{36}{120} \\right) = 36 \\ \\checkmark\\)\n\\(n (1 - \\widehat{p}) = (120) \\cdot \\left( \\frac{84}{120} \\right) = 84 \\ \\checkmark\\)\n\n\n\nSince these conditions are met, we can proceed in constructing our confidence interval as \\[ 0.3 \\pm 1.96 \\cdot \\sqrt{\\frac{0.3 \\cdot 0.7}{120}} = \\boxed{[0.218 \\ , \\ 0.382]} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#what-is-data",
    "href": "Pages/Lectures/Lecture01/Lec01.html#what-is-data",
    "title": "PSTAT 5A: Lecture 01",
    "section": "What is Data?",
    "text": "What is Data?"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#what-is-data-1",
    "href": "Pages/Lectures/Lecture01/Lec01.html#what-is-data-1",
    "title": "PSTAT 5A: Lecture 01",
    "section": "What is Data?",
    "text": "What is Data?\n\nAccording to Merriam-Webster (source), there are three definitions for data:\n\n\nfactual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation\ninformation in digital form that can be transmitted or processed\ninformation output by a sensing device or organ that includes both useful and irrelevant or redundant information and must be processed to be meaningful"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#example-of-data",
    "href": "Pages/Lectures/Lecture01/Lec01.html#example-of-data",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Example of Data",
    "text": "Example of Data\n\nAs a concrete example of a dataset, let’s explore the palmerpenguins dataset.\nCollected by Dr. Kristen Gorman at the Palmer Station in Antarctica, this dataset contains various measurements of 344 different penguins Dr. Gorman encountered.\nWhen using data, it is critical to know how the data was collected and how each column (or variable is defined)."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#the-data-matrix",
    "href": "Pages/Lectures/Lecture01/Lec01.html#the-data-matrix",
    "title": "PSTAT 5A: Lecture 01",
    "section": "The Data Matrix",
    "text": "The Data Matrix\n\nEach row of the data matrix above corresponds to an individual penguin.\n\nIn general, we refer to a given row of the data matrix as an observational unit, or case.\n\nFor each penguin, we can see that there are observations on several different characteristics; specifically, for each penguin she encountered, Dr. Gorman measured and recorded the penguin’s species, island, bill length (in mm), bill depth (in mm), flipper length (in mm), body mass (in grams), sex, and year of observation.\n\nNotice that these are the column names in our data matrix above. In general, the columns of the data matrix are referred to as variables."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#numerical-vs.-categorical",
    "href": "Pages/Lectures/Lecture01/Lec01.html#numerical-vs.-categorical",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Numerical vs. Categorical",
    "text": "Numerical vs. Categorical\n\nNumerical variables are variables whose observations consist of numbers.\n\nExamples: heights, temperatures, number of free throws, etc.\n\nNot all variables are numerical. For example, I could take a poll asking people’s opinions on the movie Dune 2- the observations of this variable will most certainly not be numerical.\n\nRather, the observations of this variable will fall into one of a series of fixed categories (e.g. “Enjoyed the movie”, “Neutral about the movie”, “Too much sand”, etc.).\nAs such, we describe non-numerical variables as categorical variables."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#a-note-on-language",
    "href": "Pages/Lectures/Lecture01/Lec01.html#a-note-on-language",
    "title": "PSTAT 5A: Lecture 01",
    "section": "A Note on Language",
    "text": "A Note on Language\n\nQuestion: can we say that data is numerical? Or, can we say we have “categorical data”?\nSure- if our data consists of just a single variable!\nThat is to say- the classification terms we learned (and will learn) can be used to describe data, provided our data contains only one variable.\nThe definition of data we are using (i.e. in the context of the data matrix) is that data is comprised of several variables. As such, we cannot simply take the classification of variables and apply that to the entire dataset (unless our dataset consists of only one variable).\n\nThis may seem like a subtle point… and it is! I’m just pointing it out so you are aware of it."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#continuous-vs.-discrete-variables",
    "href": "Pages/Lectures/Lecture01/Lec01.html#continuous-vs.-discrete-variables",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Continuous vs. Discrete Variables",
    "text": "Continuous vs. Discrete Variables\n\nThere is a way we can further subdivide numerical variables.\nAs an example, let us consider two different variables, both of which are numerical: heights, and number of accidents on a stretch of highway.\n\nIt is perfectly conceivable to observe a height of 5.15 feet, or 5.1302 feet, or 5.02391829 feet. In other words, there are an infinite number of possible heights between, say, 5 feet and 6 feet.\nOn the other hand, it doesn’t make sense to talk about “1.5 accidents” occurring on a stretch of highway; the number of accidents needs to be an integer."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#ordinal-vs.-nominal-variables",
    "href": "Pages/Lectures/Lecture01/Lec01.html#ordinal-vs.-nominal-variables",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Ordinal vs. Nominal Variables",
    "text": "Ordinal vs. Nominal Variables\n\nJust as there was a way to subdivide numerical variables, there is a way to further subdivide categorical variables as well.\nAs an example, consider the following two categorical variables: color, and letter grades (i.e. A, B+, etc.)\n\nFirstly, I hope you can see that both of these variables are indeed categorical: there are only a fixed set of values that “color” and “letter grade” can take, with nothing in between.\nNow, clearly letter grades can be ordered: that is, an A is better than a B, which is better than a C, and so on and so forth.\nIn contrast, “green” isnt inherently better than “red”, which isn’t inherently better than “grey”, and so on and so forth."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#full-classification-scheme",
    "href": "Pages/Lectures/Lecture01/Lec01.html#full-classification-scheme",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Full Classification Scheme",
    "text": "Full Classification Scheme\n\n\nHere is a diagram of the full classification scheme:\n\n\n\n\n\n\n\n\n\ndata_classification\n\n\ncluster_main\n\n\n\ncluster_0\n\n\n\ncluster_1\n\n\n\ncluster_2\n\n\n\ncluster_3\n\n\n\n\nData\n\nVariable\n\n\n\nnumerical\n\nNumerical\n\n\n\nData-&gt;numerical\n\n\n\n\n\ncategorical\n\nCategorical\n\n\n\nData-&gt;categorical\n\n\n\n\n\ncontinuous\n\nContinuous\n\n\n\nnumerical-&gt;continuous\n\n\n\n\n\ndiscrete\n\nDiscrete\n\n\n\nnumerical-&gt;discrete\n\n\n\n\n\nnominal\n\nNominal\n\n\n\ncategorical-&gt;nominal\n\n\n\n\n\nordinal\n\nOrdinal\n\n\n\ncategorical-&gt;ordinal"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#time-for-an-exercise",
    "href": "Pages/Lectures/Lecture01/Lec01.html#time-for-an-exercise",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 1\n\n\n\nClassify each of the following variables as either discrete, continuous, ordinal, or nominal.\n\n\nThe number of times a computer program returns an error\nThe time it takes an experienced swimmer to complete 4 laps of a pool\nThe favorite flavor of donut of a randomly selected person\nThe months of the year, as written in MM format (e.g. “01” for “January”, “02” for “February”, etc.)\n\n\nDiscuss with your neighbors!"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#important-note",
    "href": "Pages/Lectures/Lecture01/Lec01.html#important-note",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Important Note",
    "text": "Important Note\n\nIt is important to note that categorical data can be encoded using numbers (as we saw in the previous slide).\n\nIndeed, this is a fairly common practice as computers are more adept at dealing with numbers than things like words or symbols.\nAs such, when classifying data, it is not always enough to just check whether the data consists of numbers or not- it is important to think critically about what the data itself represents.\nAs a quick rule-of-thumb: check whether adding two numbers in your dataset makes interpretive sense. 12in \\(+\\) 13in is 15in, whereas blue + gold does not equal anything, regardless of whether blue is being encoded as 0 and gold is being encoded as 1."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#real-world-data-set",
    "href": "Pages/Lectures/Lecture01/Lec01.html#real-world-data-set",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Real-World Data Set",
    "text": "Real-World Data Set\n\nLet’s return to the palmerpenguins dataset.\nSpecifically, let’s examine the species variable:\n\n\n\n\n  [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n  [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n[155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n[281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[344] Chinstrap\nLevels: Adelie Chinstrap Gentoo"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#descriptive-statistics",
    "href": "Pages/Lectures/Lecture01/Lec01.html#descriptive-statistics",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n\nThis is the goal of Descriptive Statistics- to find different summarizing techniques to desribe the data.\n\n\n\nThere are two ways we can seek to summarize data: numerically (using numbers), and graphically.\nLet’s start with the latter- that is, let’s discuss how we might summarize our data using graphs."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#back-to-penguins",
    "href": "Pages/Lectures/Lecture01/Lec01.html#back-to-penguins",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Back To Penguins",
    "text": "Back To Penguins\n\nHere is the species variable one more time:\n\n\n\n\n  [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n  [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n[155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n[281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[344] Chinstrap\nLevels: Adelie Chinstrap Gentoo"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#bargraphsbarplots",
    "href": "Pages/Lectures/Lecture01/Lec01.html#bargraphsbarplots",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Bargraphs/Barplots",
    "text": "Bargraphs/Barplots\n\n\nThis is an example of what is known as a bargraph or barplot.\n\n\n\n\n\n\n\n\nResult\n\n\n\nA bargraph is the best type of visualization for categorical data.\n\n\n\n\n\nIn general, if you have \\(k\\) categories, then you will have \\(k\\) bars in your bargraph, each with height propotional to the number of observations within the corresponding category.\nAs you can see, computing software is very useful when it comes to data visualization! In a few weeks, you will explore how to generate visualizations of your own in Python during Lab."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#time-for-another-exercise",
    "href": "Pages/Lectures/Lecture01/Lec01.html#time-for-another-exercise",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Time For Another Exercise!",
    "text": "Time For Another Exercise!\n\n\n\n\n\n\nExercise 2\n\n\n\nA recent survey asked 120 different PSTAT students what their favorite color is. The bargraph of the results is displayed below:\n\n\n\n\n\n\n\n\n\nApproximately what proportion of the students in the sample reported either blue or gold as their favorite color? Discuss with your neighbor!"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#leadup",
    "href": "Pages/Lectures/Lecture01/Lec01.html#leadup",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Leadup",
    "text": "Leadup\n\n\nAll of our discussions above were related to categorical variables.\n\n\n\nAs we discussed at the beginning of this lecture, not all variables are categorical- how do we visualize numerical variables?\nAgain, I find it useful to consider a concrete example: this time, let’s use the bill_length_mm variable from the palmerpenguins dataset."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#discretizationbinning",
    "href": "Pages/Lectures/Lecture01/Lec01.html#discretizationbinning",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Discretization/Binning",
    "text": "Discretization/Binning\n\n\nThis is what is known as discretizing or binning our variable.\n\n\n\nIn other words, when we discretize our data, we carve it up into a bunch of chunks of equal width and see how many observations fall in each chunk.\n\nThe width of each chunk is what we call the binwidth. For example, if my categories are “between 30 and 35”, “between 35 and 40”, etc., then the binwidth is 5mm as each category spans a width of 5mm."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#the-importance-of-binwidth",
    "href": "Pages/Lectures/Lecture01/Lec01.html#the-importance-of-binwidth",
    "title": "PSTAT 5A: Lecture 01",
    "section": "The Importance of Binwidth",
    "text": "The Importance of Binwidth\n\nNotice that our notion of a histogram is intimately tied with our choice of binwidth.\nDifferent binwidths can produce wildly different histograms!\nHere is a demo\nIn practice, it is a good idea to play around with different binwidths to find one that results in a histogram that displays a moderate amount of detail without becoming so detailed as to lose sight of the bigger picture."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#boxplots",
    "href": "Pages/Lectures/Lecture01/Lec01.html#boxplots",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Boxplots",
    "text": "Boxplots\n\nIt turns out there is another way to summarize numerical data visually: using what is known as a boxplot.\nBoxplots can be a seem a bit peculiar at first, so let’s take a look at one together. Before diving back into the palmerpenguins dataset, let’s look at a slightly different dataset.\n\nThis dataset contains only one variable, which records the scores (out of 100 points) of 140 different students on a final exam.\n\n\n\n\n\n  [1] 88.236 77.348 81.050 74.431 75.083 79.569 74.998 80.099 74.264 83.850\n [11] 89.857 81.427 79.439 84.260 78.565 77.570 78.224 73.780 88.085 79.341\n [21] 80.554 77.317 81.155 83.842 87.051 78.362 81.528 72.148 74.131 78.927\n [31] 75.446 79.791 78.199 90.769 85.640 78.420 83.484 79.045 97.909 86.736\n [41] 73.723 76.973 81.320 79.238 85.803 86.621 85.781 81.844 82.896 80.478\n [51] 75.903 84.565 76.302 83.432 85.448 69.695 81.049 85.575 84.791 82.525\n [61] 78.361 77.803 86.542 84.171 86.103 72.772 78.730 76.189 75.187 79.194\n [71] 77.159 82.048 82.661 84.021 76.008 79.474 79.015 86.992 72.524 76.094\n [81] 78.765 80.623 82.497 75.776 70.614 79.677 81.182 77.943 76.863 85.561\n [91] 89.569 96.695 73.680 77.770 81.584 81.965 78.373 76.295 73.212 79.229\n[101] 87.273 87.364 82.706 83.843 75.864 82.791 82.637 78.685 72.626 69.302\n[111] 93.408 73.189 83.764 77.832 82.803 80.278 94.962 79.616 85.667 82.710\n[121] 86.823 76.656 74.623 71.508 91.131 78.318 81.058 86.239 76.585 85.652\n[131] 77.122 86.036 83.127 83.234 80.746 83.878 75.544 73.780 81.106 85.523"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#anatomy-of-a-boxplot",
    "href": "Pages/Lectures/Lecture01/Lec01.html#anatomy-of-a-boxplot",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Anatomy of a Boxplot",
    "text": "Anatomy of a Boxplot"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#understanding-boxplots",
    "href": "Pages/Lectures/Lecture01/Lec01.html#understanding-boxplots",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Understanding Boxplots",
    "text": "Understanding Boxplots\n\nLet’s discuss each of the quantities represented on the boxplot separately.\nBefore we do, there’s a bit of math we need to cover.\nThe first quantity we will define is a term you may have heard before- percentile.\nLet’s return to our histogram of scores (since we’re a bit more comfortable with reading histograms than boxplots, at this point)"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#percentiles",
    "href": "Pages/Lectures/Lecture01/Lec01.html#percentiles",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Percentiles",
    "text": "Percentiles\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe pth percentile of a set of observations \\(X\\) is the value \\(\\pi_{x, \\ 0.5}\\) such that p% of observations lie to the left of (i.e. are less than) \\(\\pi_{x, \\ p}\\).\n\n\n\n\n\n\nMaybe now you can see why I switched over to this data of scores- I think percentiles are sometimes easier to interpret in the context of exam scores, since they are very commonly reported with standardized testing scores (e.g. SAT, GRE, etc.)\n\nIn the context of scores: someone who scored at the pth percentile performed better than p% of all test-takers."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#quartiles",
    "href": "Pages/Lectures/Lecture01/Lec01.html#quartiles",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Quartiles",
    "text": "Quartiles\n\nWe give a special name to the 25th and 75th percentiles of a set of observations- we call these the first quartile and third quartile, respectively, and use the notation \\(Q_1\\) and \\(Q_3\\) to denote them, respectively.\n\nSo, \\(Q_1\\) is the value such that 25% of observations are less than \\(Q_1\\), and \\(Q_3\\) is the value such that 75% of observations are less than \\(Q_3\\)\n\nThe second quartile (i.e. the 50th percentile) is called the median.\n\nAs such, the median is the value that “splits the data in half”.\nWe’ll talk more about the median in the next lecture."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#small-caveat",
    "href": "Pages/Lectures/Lecture01/Lec01.html#small-caveat",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Small Caveat",
    "text": "Small Caveat\n\nI should quickly mention one small caveat- computing softwares often use a different procedure for computing quartiles.\nThis procedure is quite long and complicated, and is based off an entire paper written back in the 90’s.\nFor example, if we consider the set \\(S = \\{1, 2, 3, 4, 5, 6\\}\\), we would (based on the definition from the previous slide) call the first quartile \\(2\\), whereas most softwares would return a value of \\(2.25\\).\nDon’t worry about why this is- whenever we talk about quartiles in this class, you can just think of the definition I posed on the previous slide."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#whiskers",
    "href": "Pages/Lectures/Lecture01/Lec01.html#whiskers",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Whiskers",
    "text": "Whiskers\n\nFinally, we discuss the role of the whiskers on the boxplot.\nThere are several different conventions for how far the whiskers extend. In some conventions, the whiskers extend to the minimum and maximum values of the data.\nThe convention we will use is the following: the whiskers will never reach farther than \\(\\boldsymbol{1.5 \\times (Q_3 - Q_1)}\\).\n\nWhat this means is that there may be points in our dataset that lie beyond the reach of the whiskers. These points are what we call outliers."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#whiskers-1",
    "href": "Pages/Lectures/Lecture01/Lec01.html#whiskers-1",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Whiskers",
    "text": "Whiskers\n\n\nThe rationale for constructing the whiskers in this way is to try and highlight any points that are unusually distant from the rest of the data.\n\n\n\nFor example, returning to our dataset of scores, we can see that though the median score was around 80.3% there was one person who scored a 97.9%. Because this score is unusually large, we would label it an outlier."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#time-for-an-exercise-1",
    "href": "Pages/Lectures/Lecture01/Lec01.html#time-for-an-exercise-1",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 4\n\n\n\nHere is a boxplot of the bill_length_mm variable from the palmerpenguins dataset:\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the median bill length?\nApproximately what percent of penguins had bills shorter than 37mm in length?\nAre there any outliers?"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#summary",
    "href": "Pages/Lectures/Lecture01/Lec01.html#summary",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Summary",
    "text": "Summary\n\nWe started off by talking about the structure of data, and the data matrix.\nWe then discussed how to classify variables.\nNext, we explored graphical methods for summarizing data.\n\nBargraphs are best-suited for categorical data\nHistograms and boxplots are best-suited for numerical data\n\nWe also introduced the notions of percentiles, the median, and outliers.\nNext time we’ll discuss how to visualize the relationship between two variables.\nWe’ll also discuss some numerical summaries for data, including the mean, median, standard deviation, and IQR."
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#course-staff",
    "href": "Pages/Lectures/Lecture00/Lec00.html#course-staff",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Course Staff",
    "text": "Course Staff\n\n\n\n\nInstructor:\n\nMallory (she/her)\nmallorywang@ucsb.edu\nT 9:30 - 10:30a (zoom) and 4 - 5p (zoom)\n\n\n\n\n\n\n\nTeaching Assistants:\n\n\n\n\nDaniel Silva\ndcsilva@ucsb.edu\nOH: TBD\n\n\n\n\n\nHezhong Zhang\nhzhang586@umail.ucsb.edu\nOH: TBD"
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#course-resources",
    "href": "Pages/Lectures/Lecture00/Lec00.html#course-resources",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Course Resources",
    "text": "Course Resources\n\nCanvas: for grades\nGradescope: for homework, quizzes, and labs\nCourse Website: https://pstat5a.github.io\n\nAll relevant course material will be posted to the website!\nOne exception: quizzes, which will be administered through Gradescope\n\nPlease read the syllabus fully and carefully!"
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#what-is-data-science-1",
    "href": "Pages/Lectures/Lecture00/Lec00.html#what-is-data-science-1",
    "title": "PSTAT 5A: Lecture 00",
    "section": "What is Data Science?",
    "text": "What is Data Science?\n\nNot a bad definition! To be fair, data science means different things to different people / companies.\nThere isn’t a single agreed-upon definition of what data science is.\nMost people agree that Data science is cross-disciplinary, drawing experience and expertise from a wide variety of different fields.\n\nPerhaps the two main fields from which Data Science draws are Statistics and Computer Science\n\nLike ChatGPT suggested, computation is an integral part of Data Science.\n\nAs we will soon see, the data that is being analyzed these days is huge; certainly too large to be able to do anything with it on pen and paper."
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#the-path-forward",
    "href": "Pages/Lectures/Lecture00/Lec00.html#the-path-forward",
    "title": "PSTAT 5A: Lecture 00",
    "section": "The Path Forward",
    "text": "The Path Forward\n\nSo, how does this course factor into the discourse surrounding Data Science?\nFrom the course description:\n\n\n\nIntroduction to data science. Concepts of statistical thinking. Topics include random variables, sampling distributions, hypothesis testing, correlation and regression. Visualizing, analyzing and interpreting real world data using Python. Computing labs required.\n\n\n\nIndeed, this course will serve as a sort of “table of contents” of Data Science, touching on many (but still not all) of the wonderful subfields and subtopics that comprise the field."
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#why-should-i-care",
    "href": "Pages/Lectures/Lecture00/Lec00.html#why-should-i-care",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Why Should I Care?",
    "text": "Why Should I Care?\n\n\nI suspect not all of you are necessarily pursuing a degree in Statistics or Data Science. However, this day in age, data is truly everywhere, and having strong mathematical thinking will give you a leg up in any role you want.\n\n\n\nHowever, wherever there is data, there is the need for a Data Scientist (or, at least, some of the principles from Data Science).\n\nSo, even if you are working in (what you might think is) a field that is far removed from Statsitics, the minute you start dealing with Data is the minute you start needing to know Data Science!\n\nHere’s a perhaps more pragmatic answer: even if you think you want to go straight into industry right after this course, no company wants to hire someone to just mindlessly crunch numbers - though computing experience is absolutely crucial in making yourself a good candidate, employers would much rather have someone who is both skilled at running code but also understands why they are running the code they are running!"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#last-time",
    "href": "Pages/Lectures/Lecture21/Lec21.html#last-time",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Last Time",
    "text": "Last Time\n\nLast time, we began our foray into statistical modeling.\nGiven data \\(\\{(x_i, \\ y_i)\\}_{i=1}^{n}\\) on a response variable y and an explanatory variable x, we model the relationship between y and x as \\[ \\texttt{y} = f(\\texttt{x}) + \\texttt{noise} \\] for some signal function \\(f()\\).\nWhen the response variable is numerical, we call the model a regression model; when the variable is categorical, we call the model a classification model.\nUlimately, we wish to fit a signal function \\(\\widehat{f}()\\) to our data."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#simple-linear-regression",
    "href": "Pages/Lectures/Lecture21/Lec21.html#simple-linear-regression",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nSimple Linear Regression refers to a situation in which we have:\n\na numerical response variable y\na single explanatory variable x\na linear signal function; i.e. \\(f(x) = \\beta_0 + \\beta_1 \\cdot x\\)\n\nThat is, the model in a simple linear regression setting is \\[ \\texttt{y} = \\beta_0 + \\beta_1 \\cdot x  + \\texttt{noise} \\]\n\nFitting a signal function \\(\\widehat{f}()\\) to the data is therefore equivalent to finding suitable estimators \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) of \\(\\beta_0\\) and \\(\\beta_1\\) respectively."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#a-noteconnection",
    "href": "Pages/Lectures/Lecture21/Lec21.html#a-noteconnection",
    "title": "PSTAT 5A: Lecture 21",
    "section": "A Note/Connection",
    "text": "A Note/Connection\n\nI’d like to stress- writing \\(f(x) = \\beta_0 + \\beta_1 \\cdot x\\) is exactly the same as our familiar \\(mx + b\\) form for a line!\n\nThe reason we use \\(\\beta_0\\) and \\(\\beta_1\\) in place of \\(b\\) and \\(m\\), respectively, is to allow for an extension of the same notation practices to a multivariate setting.\nThat is, if we have \\(k\\) explanatory variables x1 through xk, it is easier to write a linear model as \\[ \\texttt{y} = \\beta_0 + \\beta_1 \\cdot x_1 + \\cdots + \\beta_k \\cdot x_k + \\texttt{noise} \\] instead of having to find new letters for the coefficients of x1 through xk."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#ols-estimators-and-the-ols-regression-line",
    "href": "Pages/Lectures/Lecture21/Lec21.html#ols-estimators-and-the-ols-regression-line",
    "title": "PSTAT 5A: Lecture 21",
    "section": "OLS Estimators and the OLS Regression Line",
    "text": "OLS Estimators and the OLS Regression Line\n\nBack to the model fitting problem: we seek to find “good” estimators \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) for \\(\\beta_0\\) and \\(\\beta_1\\) respectively.\nOne quantification of “good” is “minimizing the residual sum of squares”:\n\n\n \\[ \\mathrm{RSS} = \\sum_{i=1}^{n} e_i^2 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#ols-estimators-and-the-ols-regression-line-1",
    "href": "Pages/Lectures/Lecture21/Lec21.html#ols-estimators-and-the-ols-regression-line-1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "OLS Estimators and the OLS Regression Line",
    "text": "OLS Estimators and the OLS Regression Line\n\nThe estimators that minimize the RSS are \\[\\begin{align*}\n\\widehat{\\beta}_1   & = \\frac{\\sum\\limits_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum\\limits_{i=1}^{n} (x_i - \\overline{x})^2}    \\\\\n\\widehat{\\beta}_0   & = \\overline{y} - \\widehat{\\beta}_1 \\overline{x}\n\\end{align*}\\] which are called the ordinary least squares (or just OLS) estimators of \\(\\beta_0\\) and \\(\\beta_1\\).\nWe write the OLS regression line as \\[ \\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot x \\] which is just the line that “best” fits the data (where, again, “best” is quantified by minimizing the RSS)."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#correlation",
    "href": "Pages/Lectures/Lecture21/Lec21.html#correlation",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Correlation?",
    "text": "Correlation?\n\nBy the way: remember that we also talked about Pearson’s correlation coefficient last lecture.\nIt is defined as \\[ r = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\overline{x}}{s_X} \\right)  \\left( \\frac{y_i - \\overline{y}}{s_Y} \\right) \\] and gives a way of quantifying the strength of the linear association between two lists of numbers \\(\\{x_i\\}_{i=1}^{n}\\) and \\(\\{y_i\\}_{i=1}^{n}\\).\nI mentioned last time that \\(r\\) does NOT give the slope of the line that best fits the data- that role is given to \\(\\widehat{\\beta}_1\\)!\nHowever, there is in fact a connection between \\(\\widehat{\\beta}_1\\) and \\(r\\): it turns out (after a bit of math), we can equivalently compute \\(\\widehat{\\beta}_1\\) as \\[ \\widehat{\\beta} = \\frac{s_Y}{s_X} \\cdot r \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#fitted-values",
    "href": "Pages/Lectures/Lecture21/Lec21.html#fitted-values",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Fitted Values",
    "text": "Fitted Values\n\nWe also introduced the notion of fitted values:\n\n\n\n\n\nThis is why we write the OLS regression line as \\[ \\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot x \\] the fitted values are simply the points along the OLS regression line!"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#prediction",
    "href": "Pages/Lectures/Lecture21/Lec21.html#prediction",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Prediction",
    "text": "Prediction\n\nIn this way, we can perhaps see how the OLS regression line can be used to perform prediction.\nTo see how this works, let’s return to a toy example from last lecture: \\[\\begin{align*}\n\\boldsymbol{x}    & = \\{3, \\ 7, \\ 8\\}   \\\\\n\\boldsymbol{y}    & = \\{20, \\ 14, \\ 17\\}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#prediction-1",
    "href": "Pages/Lectures/Lecture21/Lec21.html#prediction-1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Prediction",
    "text": "Prediction\n\nNotice that we do not have an x-observation of 5. As such, we don’t know what the y-value corresponding to an x-value of 5 is.\nHowever, we do have a decent guess as to what the y-value corresponding to an x-value of 5 is- the corresponding fitted value!"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#prediction-2",
    "href": "Pages/Lectures/Lecture21/Lec21.html#prediction-2",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Prediction",
    "text": "Prediction\n\nIn other words, our best guess at (i.e. our best prediction of) the \\(y-\\)value corresponding to an \\(x-\\)value of \\(5\\) is the point \\(\\widehat{y}^{(5)}\\) obtained by plugging \\(x = 5\\) into the OLS regression line:\n\n\n\\[ \\widehat{y}^{(5)} = \\frac{1}{7} (155 - 6 \\cdot 5) \\approx 17.857 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#leadup",
    "href": "Pages/Lectures/Lecture21/Lec21.html#leadup",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s look at another toy dataset:"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#leadup-1",
    "href": "Pages/Lectures/Lecture21/Lec21.html#leadup-1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Leadup",
    "text": "Leadup\n\nSay we want to predict the corresponding y value of an x value of, 40.\nFollowing our steps from before, we would just find the fitted value corresponding to x = 40:"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#leadup-2",
    "href": "Pages/Lectures/Lecture21/Lec21.html#leadup-2",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Leadup",
    "text": "Leadup\n\nHere’s the kicker: the true fit was actually NOT linear!"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#extrapolation-and-the-dangers-thereof",
    "href": "Pages/Lectures/Lecture21/Lec21.html#extrapolation-and-the-dangers-thereof",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Extrapolation, and the Dangers Thereof",
    "text": "Extrapolation, and the Dangers Thereof\n\nSpecifically, the true signal function was quadratic. (When you zoom in close enough, parabolas look linear!)\nThis is why it is a bad idea to try to extrapolate too far.\n\nExtrapolation is the name we give to trying to apply a model estimate to values that are very far outside the realm of the original data.\nHow far is “very far”? Statisticians disagree on this front. For the purposes of this class, just use your best judgment."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#your-turn",
    "href": "Pages/Lectures/Lecture21/Lec21.html#your-turn",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\nExercise 1\n\n\n\n\nAn airline is interested in determining the relationship between flight duration (in minutes) and the net amount of soda consumed (in oz.). Letting x denote flight duration (the explanatory variable) and y denote amount of soda consumed (the response variable), a sample yielded the following results: \\[ \\begin{array}{cc}\n  \\displaystyle \\sum_{i=1}^{102} x_i  = 20,\\!190.55;   & \\displaystyle \\sum_{i=1}^{102} (x_i - \\overline{x})^2 =  101,\\!865    \\\\\n  \\displaystyle \\sum_{i=1}^{102} y_i  = 166,\\!907.8   & \\displaystyle \\sum_{i=1}^{102} (y_i - \\overline{y})^2 =  120,\\!794.2   \\\\\n\\displaystyle \\sum_{i=1}^{100} (x_i - \\overline{x})(y_i - \\overline{y}) = 80,\\!184.62 \\\\\n\\end{array} \\]\n\nFind the equation of the OLS Regression line.\nIf a particular flight has a duration of 130 minutes, how many ounces of soda would we expect to be consumed on the flight? (Suppose the \\(x-\\)observations ranged from around \\(114\\) to around \\(271\\))"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#back-to-widehatbeta_1",
    "href": "Pages/Lectures/Lecture21/Lec21.html#back-to-widehatbeta_1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Back to \\(\\widehat{\\beta}_1\\)",
    "text": "Back to \\(\\widehat{\\beta}_1\\)\n\nRecall that, by virtue of being the slope of the OLS regression line, \\(\\widehat{\\beta}_1\\) has the following interpretation:\n\n\n\nA one-unit change in x corresponds to a predicted \\(\\widehat{\\beta}_1\\)-unit change in y.\n\n\n\nNotice the word “predicted” there- remember that \\(\\widehat{\\beta}_1\\) is an estimator, not the true slope!\n\nThat is to say: different datasets generated from the same model may produce different values of \\(\\widehat{\\beta}_1\\).\n\nThat’s right: \\(\\widehat{\\beta}_1\\) can be thought of as a random variable."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#back-to-widehatbeta_1-1",
    "href": "Pages/Lectures/Lecture21/Lec21.html#back-to-widehatbeta_1-1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Back to \\(\\widehat{\\beta}_1\\)",
    "text": "Back to \\(\\widehat{\\beta}_1\\)\n\nWe know that \\(\\widehat{\\beta}_1\\) seeks to estimate \\(\\beta_1\\).\nIt seems plausible, then, that we might be able to perform inference on \\(\\beta_1\\) (the true slope) using \\(\\widehat{\\beta}_1\\) (the OLS estimator).\nIndeed, we can!\nLet’s start off by considering confidence intervals for the true slope \\(\\beta_1\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#confidence-intervals-for-beta_1",
    "href": "Pages/Lectures/Lecture21/Lec21.html#confidence-intervals-for-beta_1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Confidence Intervals for \\(\\beta_1\\)",
    "text": "Confidence Intervals for \\(\\beta_1\\)\n\nRecall that (at least in the confines of this course), given a parameter \\(\\theta\\) and an estimator \\(\\widehat{\\theta}\\) of \\(\\theta\\), we construct a confidence interval for \\(\\theta\\) as \\[ \\widehat{\\theta} \\pm c \\cdot \\mathrm{SD}(\\widehat{\\theta}) \\] where \\(c\\) is a constant that depends on both the sampling distribution of \\(\\widehat{\\theta}\\) along with the confidence level.\nThis means we can construct a confidence interval for \\(\\beta_1\\) using \\[ \\widehat{\\beta}_1 \\pm c \\cdot \\mathrm{SD}(\\widehat{\\beta}_1) \\] where \\(\\widehat{\\beta}_1\\) is the OLS estimator of \\(\\beta_1\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#confidence-intervals-for-beta_1-1",
    "href": "Pages/Lectures/Lecture21/Lec21.html#confidence-intervals-for-beta_1-1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Confidence Intervals for \\(\\beta_1\\)",
    "text": "Confidence Intervals for \\(\\beta_1\\)\n\nIt turns out that finding \\(\\mathrm{SD}(\\widehat{\\beta}_1)\\) is fairly involved. As such, I won’t expect you to compute it- you will be provided with its value for a given problem (see the practice problems for an example of what I mean).\nWe also need access to the sampling distribution of \\(\\widehat{\\beta}_1\\).\nAssuming both the x-observations and y-observations are roughly normal, then \\[ \\frac{\\widehat{\\beta}_1 - \\beta_1}{\\mathrm{SD}(\\widehat{\\beta_1})} \\sim t_{n - 2} \\]\nThis means our critical value should be the appropriately-selected quantiles of the tn-2 distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#worked-out-example",
    "href": "Pages/Lectures/Lecture21/Lec21.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nConsider the same setup as Exercise 1. Suppose it is known that \\[ \\mathrm{Var}(\\widehat{\\beta}_1) \\approx 0.006135 \\] Construct a 95% confidence interval for \\(\\beta_1\\), the true amount of change in y (amount of soda consumed) associated with a one-unit change in x (flight duration)"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#solutions",
    "href": "Pages/Lectures/Lecture21/Lec21.html#solutions",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Solutions",
    "text": "Solutions\n\nWe previously saw that \\(\\widehat{\\beta}_1 \\approx 0.7871656\\).\nWe know to use the \\(t_{100}\\) distribution; since we are using a 95% confidence level, we take \\(1.98\\) as our confidence coefficient (make sure you know where this came from!)\nHence, our confidence interval is \\[(0.7871656) \\pm 1.98 \\cdot \\sqrt{0.006135} = \\boxed{[0.6321 \\ , \\ 0.9423]}\\]\nThe interpretation of this interval is similar to the interpretation of our confidence intervals thus far:\n\n\n\nWe are 95% confident that the interval \\([0.6321 \\ , \\ 0.9423]\\) covers the true value of \\(\\beta_1\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#example",
    "href": "Pages/Lectures/Lecture21/Lec21.html#example",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Example",
    "text": "Example\n\n\n\\(\\widehat{\\beta_0} =\\) -2.5884231; \\(\\widehat{\\beta_1} =\\) -1.733778"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#example-1",
    "href": "Pages/Lectures/Lecture21/Lec21.html#example-1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Example",
    "text": "Example\n\n\n\\(\\widehat{\\beta_0} =\\) -2.5884231; \\(\\widehat{\\beta_1} =\\) -0.483778"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#example-2",
    "href": "Pages/Lectures/Lecture21/Lec21.html#example-2",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Example",
    "text": "Example\n\n\n\\(\\widehat{\\beta_0} =\\) -2.5884231; \\(\\widehat{\\beta_1} =\\) 0.266222\nDo we really believe the slope, though?"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#leadup-3",
    "href": "Pages/Lectures/Lecture21/Lec21.html#leadup-3",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Leadup",
    "text": "Leadup\n\nWithout the OLS regression line, the scatterplot on the previous page would likely be one we classify as exhibiting “no relationship” between x and y.\nHowever, the OLS regression line has picked up a positive slope.\nWhat’s going on?\nIn other words, does our data (i.e. the data that gave rise to the value of \\(\\widehat{\\beta}_1\\) computed) support our claim that there is no relationship?"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#hypothesis-testing-on-beta_1",
    "href": "Pages/Lectures/Lecture21/Lec21.html#hypothesis-testing-on-beta_1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Hypothesis Testing on \\(\\beta_1\\)",
    "text": "Hypothesis Testing on \\(\\beta_1\\)\n\nWhat do you know- we’ve entered the realm of hypothesis testing!\nSpecifically, we are trying to use our data to test the hypotheses \\[ \\left[ \\begin{array}{rl}\nH_0:    & \\beta_1 = 0   \\\\\nH_A:    & \\beta_1 \\neq 0\n\\end{array} \\right. \\]\n\nWhy is our null \\(\\beta_1 = 0\\)?\nWell, our null is that there is no relationship.\nRecall that \\(\\beta_1\\) is the slope of the linear relationship assumed to exist between y and x.\nHence, saying “no relationship” is equivalent to saying “a change in x corresponds to no change in y”; i.e. that \\(\\beta_1 = 0\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#hypothesis-testing-on-beta_1-1",
    "href": "Pages/Lectures/Lecture21/Lec21.html#hypothesis-testing-on-beta_1-1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Hypothesis Testing on \\(\\beta_1\\)",
    "text": "Hypothesis Testing on \\(\\beta_1\\)\n\nWe already have a pretty good candidate for a test statistic: \\[ \\mathrm{TS} = \\frac{\\widehat{\\beta}_1}{\\mathrm{SD}(\\widehat{\\beta}_1)} \\] since, under the null, this follows a \\(t_{n - 2}\\) distribution: \\[ \\mathrm{TS} = \\frac{\\widehat{\\beta}_1}{\\mathrm{SD}(\\widehat{\\beta}_1)} \\stackrel{H_0}{\\sim} t_{n - 2} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#hypothesis-testing-on-beta_1-2",
    "href": "Pages/Lectures/Lecture21/Lec21.html#hypothesis-testing-on-beta_1-2",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Hypothesis Testing on \\(\\beta_1\\)",
    "text": "Hypothesis Testing on \\(\\beta_1\\)\n\nOur test then takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| &gt; c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where \\(c\\) is the appropriately-selected quantile of the tn-2 distribution.\nEquivalently, we compute p-values using the tn-2 distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#hypothesis-testing-on-beta_1-3",
    "href": "Pages/Lectures/Lecture21/Lec21.html#hypothesis-testing-on-beta_1-3",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Hypothesis Testing on \\(\\beta_1\\)",
    "text": "Hypothesis Testing on \\(\\beta_1\\)\n\nIndeed, many computer softwares (and statistical papers!) report a table resembling the following after running a regression:\n\n\n\n\n\n\nEstimate\nStd. Error\nt-value\nPr(&gt;|t|)\n\n\n\n\nIntercept\n-2.588\n2.327\n-1.112\n0.269\n\n\nSlope\n-1.734\n0.222\n-7.811\n6.41e-12\n\n\n\n\n\nThe first column is the raw estimated value (i.e. \\(\\widehat{\\beta_0}\\) and \\(\\widehat{\\beta_1}\\), respectively)\nThe second column is the standard error (i.e. standard deviation) of the estimator\nThe third column is the test statistic (i.e. the first column divided by the second)\nThe fourth column is the p-value in a two-sided test, testing whether or not the given parameter is actually zero or not."
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#task",
    "href": "Pages/Lectures/Lecture21/Lec21.html#task",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Task",
    "text": "Task\n\nHere’s a task for you: write a function called regtab() that takes in two inputs, x and y, and returns a regression table resulting from regressing y on x.\n\nYou can use the scipy.stats.linregress() function"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#some-exercises",
    "href": "Pages/Lectures/Lecture21/Lec21.html#some-exercises",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Some Exercises",
    "text": "Some Exercises\n\n\n\n\n\n\nExercise 2 (modified from StatClass)\n\n\n\n\nConsider the following regression equation, obtained from a sample of size \\(50\\): \\[ \\widehat{y} = 3.8 - 0.277 x \\] and the standard deviation of \\(\\widehat{\\beta}_1\\) is 0.39.\nUsing a 5% level of significance, perform a test of the hypotheses \\[ \\left[ \\begin{array}{rl}\n  H_0:    & \\beta_1 = 0   \\\\\n  H_A:    & \\beta_1 \\neq 0\n\\end{array} \\right.\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture21/Lec21.html#some-exercises-1",
    "href": "Pages/Lectures/Lecture21/Lec21.html#some-exercises-1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Some Exercises",
    "text": "Some Exercises\n\n\n\n\n\n\nExercise 3 (modified from StatClass)\n\n\n\n\nTen towns were the subject of a study to determine whether or not an increased number of stores selling liquor in their downtown areas is linked with a higher number of DUI arrests downtown during one month. The data and summary information is provided below.\n\n\n\nx\n0\n5\n6\n5\n11\n9\n10\n3\n7\n4\n\n\n\n\ny\n40\n50\n55\n64\n73\n75\n88\n25\n20\n10\n\n\n\n\\[ \\begin{array}{lll}\n  \\overline{x} = 6    & \\displaystyle \\sum_{i=1}^{10} (x_i - \\overline{x})^2 = 102   \\\\\n  \\overline{y} = 50    & \\displaystyle \\sum_{i=1}^{10} (y_i - \\overline{y})^2 = 6,\\!024 & \\displaystyle \\sum_{i=1}^{10} (x_i - \\overline{x})(y_i - \\overline{y}) = 513\n\\end{array} \\]\n\nWhat is the explanatory variable?\nWhat is the response variable?\nFind the equation of the OLS regression line.\nIf the standard deviation of \\(\\widehat{\\beta}_1\\) is \\(0.37\\), construct a 95% confidence interval for \\(\\beta_1\\).\nIs the slope significant? (Use the standard deviation from part (d) if necessary.)"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#last-time",
    "href": "Pages/Lectures/Lecture10/Lec10.html#last-time",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Last Time",
    "text": "Last Time\n\nLast lecture we started talking about random variables.\nA random variable is a numeric outcome of some random process or experiment.\n\nFor example, “number of heads observed in \\(5\\) independent tosses of a fair coin”\n\nThe state space of a random variable \\(X\\) is the set \\(S_X\\) of possible values the random variable could attain.\n\nIf \\(S_X\\) has jumps, we say \\(X\\) is a “discrete random variable”\nOtherwise, we say \\(X\\) is a “continuous random variable.”\n\nToday we’ll talk about continuous random variables."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#your-turn",
    "href": "Pages/Lectures/Lecture10/Lec10.html#your-turn",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nClassify the following random variables as either discrete or continuous. Make sure to provide appropriate justification!\n\n\\(X =\\) the number of times a computer program crashes in a given day.\n\\(Y =\\) the height of a randomly-selected skyscraper in downtown Los Angeles\n\\(Z =\\) the weight of a randomly-selected fish from a lake\n\\(W =\\) the number of cats that are adopted out of the Santa Barbara location of the Santa Barbara Humane Society each year."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#continuous-random-variables",
    "href": "Pages/Lectures/Lecture10/Lec10.html#continuous-random-variables",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nContinuous random variables are described by their so-called probability density function (or p.d.f. for short).\n\nThe graph of a p.d.f. is called the density curve.\n\nThe p.d.f. is such that probabilities are found as areas underneath the density curve.\nFor example, if the random variable \\(X\\) has the following density curve…"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#two-properties",
    "href": "Pages/Lectures/Lecture10/Lec10.html#two-properties",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Two Properties",
    "text": "Two Properties\n\nSince probabilities are areas underneath the density curve, we arrive at the following two properties (which themselves follow from the Axioms of Probability):\n\n\n\n\n\n\n\n\nProperties of a P.D.F.\n\n\n\n\nDensity curves must always be nonnegative; i.e. the corresponding p.d.f. \\(f_X(x)\\) must obey \\(f_X(x) \\geq 0\\) for every \\(x\\).\nThe area underneath a density curve must be \\(1\\).\n\n\n\n\n\n\n\nIn this lecture, we will examine two continuous distributions: the uniform distribution, and the normal distribution.\n\nWe will see that the density curves/p.d.f.’s of these two distributions will satisfy the above two properties."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#uniform-distribution-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#uniform-distribution-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\nThe uniform distribution takes two parameters: \\(a\\) and \\(b\\), with \\(a &lt; b\\).\n\nWe denote the fact that a random variable \\(X\\) follows the uniform distribution with parameters \\(a\\) and \\(b\\) using the notation \\[ X \\sim \\mathrm{Unif}(a, \\ b) \\]\n\nThe \\(\\mathrm{Unif}(a, \\ b)\\) distribution has the following p.d.f.: \\[ f_X(x) = \\begin{cases} \\displaystyle \\frac{1}{b - a} & \\text{if } a \\leq x \\leq b \\\\[3mm] 0 & \\text{otherwise} \\\\ \\end{cases} \\] which corresponds to a rectangular density curve:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#uniform-density-curves",
    "href": "Pages/Lectures/Lecture10/Lec10.html#uniform-density-curves",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Uniform Density Curves",
    "text": "Uniform Density Curves\n\nOftentimes, we will be a bit lazy with our density curve and omit the open/closed circles. For example, we might sketch the density curve of the \\(\\mathrm{Unif}(1, \\ 2.15)\\) distribution as"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#effect-of-changing-a-and-b",
    "href": "Pages/Lectures/Lecture10/Lec10.html#effect-of-changing-a-and-b",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Effect of Changing \\(a\\) and \\(b\\)",
    "text": "Effect of Changing \\(a\\) and \\(b\\)\n\nviewof a = Inputs.range(\n  [-3, 3], \n  {value: 0, step: 0.1, label: \"a=\"}\n)\n\nviewof b = Inputs.range(\n  [-3, 3], \n  {value: 1, step: 0.1, label: \"b=\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmargin2 = ({top: 20, right: 30, bottom: 30, left: 40})\n\nheight2 = 400\n\nx_values2 = d32.scaleLinear()\n.domain(d32.extent(data2, d =&gt; d.x))\n.range([margin2.left, width - margin2.right])\n\ny_values2 = d32.scaleLinear()\n.domain([Math.min(d32.min(data2, d =&gt; d.y),0), Math.max(1,d32.max(data2, d =&gt; d.y))]).nice()\n.range([height2 - margin2.bottom, margin2.top])\n\nline2 = d32.line()\n.x(d =&gt; x_values2(d.x))\n.y(d =&gt; y_values2(d.y))\n\nxAxis2 = g =&gt; g\n.attr(\"transform\", `translate(0,${height2 - margin2.bottom})`)\n.call(d32.axisBottom(x_values2)\n      .ticks(width / 80)\n      .tickSizeOuter(0))\n\nyAxis2 = g =&gt; g\n.attr(\"transform\", `translate(${margin2.left},0)`)\n.call(d32.axisLeft(y_values2)\n      .tickValues(d32.scaleLinear().domain(y_values2.domain()).ticks()))\n\nfunction unif_pdf (input_value, mu, sigsq) {\nif(input_value &lt; a){\n  return 0\n} else if(input_value &gt; b){\n  return 0\n} else{\n  return 1 / (b - a)\n}\n}\n\nabs_x2=6\n\ndata2 = {\n  let values = [];\n  for (let x = -abs_x2; x &lt; abs_x2; x=x+0.01) values.push({\"x\":x,\"y\":unif_pdf(x, µ, sigsquared)});\n  return values;\n}\n\nd32 = require(\"https://d3js.org/d3.v5.min.js\")\n\nchart2 = {\n  const svg = d32.select(DOM.svg(width, height2));\n  \n  svg.append(\"g\")\n  .call(xAxis2);\n  \n  svg.append(\"g\")\n  .call(yAxis2);\n  \n  svg.append(\"path\")\n  .datum(data2)\n  .attr(\"fill\", \"none\")\n  .attr(\"stroke\", \"steelblue\")\n  .attr(\"stroke-width\", 4)\n  .attr(\"stroke-linejoin\", \"round\")\n  .attr(\"stroke-linecap\", \"round\")\n  .attr(\"d\", line);\n  \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit to https://observablehq.com/@dswalter/normal-distribution for the base of the applet code"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#uniform-probabilities",
    "href": "Pages/Lectures/Lecture10/Lec10.html#uniform-probabilities",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Uniform Probabilities",
    "text": "Uniform Probabilities\n\nRecall, from our initial discussion on continuous random variables, that probabilities are found as areas underneath the density curve.\nDue to the rectangular shape of the Uniform density curves, finding probabilities under the Uniform distribution ends up being relatively straightforward (so long as we remember how to find the area of a rectangle!)\nLet’s work through an example together.\n\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nIf \\(X \\sim \\mathrm{Unif}(-1, \\ 1)\\), compute \\(\\mathbb{P}(X \\leq 0.57)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#solution",
    "href": "Pages/Lectures/Lecture10/Lec10.html#solution",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Solution",
    "text": "Solution\n\nWhen working through probability problems involving continuous distributions, sketching a picture is always a good first step.\n\nSometimes, we will explicitly make that the first step of a problem, meaning failure to sketch a relevant picture may result in less-than-full marks!\n\nThe density curve of the \\(\\mathrm{Unif}(-1, \\ 1)\\) distribution is given by"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#solution-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#solution-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Solution",
    "text": "Solution\n\nThe desired probability is thus\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a rectangle with base \\((0.57 - (-1)) = 1.57\\) and height \\(1 / (1 - (-1)) = 1/2\\). Therefore, the area of this rectangle - and, also, the desired probability - is \\[ (1.57) \\times \\frac{1}{2} = \\boxed{0.785 = 78.5\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#another-example",
    "href": "Pages/Lectures/Lecture10/Lec10.html#another-example",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Another Example",
    "text": "Another Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nIf \\(X \\sim \\mathrm{Unif}(0, 1)\\), compute \\(\\mathbb{P}(0.25 \\leq X \\leq 0.75)\\).\n\n\n\n\n\n\n\nWe are going to solve this problem in two different ways.\nAgain, we always begin with a sketch of the desired probability as an area underneath the density curve:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#tail-probabilities",
    "href": "Pages/Lectures/Lecture10/Lec10.html#tail-probabilities",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Tail Probabilities",
    "text": "Tail Probabilities\n\nThis is not a coincidence!\nFor a more arbitrary distribution:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncan be decomposed as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[ \\huge - \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#tail-probabilities-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#tail-probabilities-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Tail Probabilities",
    "text": "Tail Probabilities\n\nIn math, what we have found is:\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\\[ \\mathbb{P}(x_1 \\leq X \\leq x_2)  = \\mathbb{P}(X \\leq x_2) - \\mathbb{P}(X \\leq x_1) \\]\n\n\n\n\n\n\nThe quantity \\(\\mathbb{P}(X \\leq x)\\), where we view \\(x\\) as an arbitrary input (and hence the quantity \\(\\mathbb{P}(X \\leq x)\\) as a function of \\(x\\)) is called the cumulative distribution function (or c.d.f. for short) of \\(X\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#your-turn-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nThe time (in minutes) spent waiting in line at Starbucks is found to vary uniformly between 5mins and 15mins.\n\nDefine the random variable of interest, and call it \\(X\\).\nIf a person is selected at random from the line at Starbucks, what is the probability that they spend between 3 and 7 minutes waiting in line?\nOptional What is the c.d.f. of wait times? (I.e., find the probability that a randomly selected person spends less than \\(x\\) minutes waiting in line, for an arbitrary value \\(x\\). Yes, your final answer will depend on \\(x\\); that’s why the c.d.f. is a function!)"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#probability-of-attaining-an-exact-value",
    "href": "Pages/Lectures/Lecture10/Lec10.html#probability-of-attaining-an-exact-value",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Probability of Attaining an Exact Value",
    "text": "Probability of Attaining an Exact Value\n\nIf \\(X \\sim \\mathrm{Unif}[0, 1]\\), what is the probability that \\(X\\) equals, say \\(0.5\\)?\n\nThe area this corresponds to is a rectangle of height \\(1 / (1 - 0) = 1\\), but with width \\(0\\).\nTherefore, the probability is zero.\n\nThis is not unique to the Uniform distribution!\n\n\n\n\n\n\n\n\nProbability of Attaining an Exact Value\n\n\n\nIf \\(X\\) is a continuous random variable, \\(\\mathbb{P}(X = x) = 0\\) for any value \\(x\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#mean-and-variance-of-the-uniform-distribution",
    "href": "Pages/Lectures/Lecture10/Lec10.html#mean-and-variance-of-the-uniform-distribution",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Mean and Variance of the Uniform Distribution",
    "text": "Mean and Variance of the Uniform Distribution\n\nIf \\(X \\sim \\mathrm{Unif}[a, b]\\), we have the following results:\n\n\\(\\displaystyle \\mathbb{E}[X] = \\frac{a + b}{2}\\)\n\\(\\displaystyle \\mathrm{Var}(X) = \\frac{1}{12}(b - a)^2\\)\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nConsider again the setup of Exerise 2: the time (in minutes) spent waiting in line at Starbucks is found to vary uniformly on between 5mins and 15mins.\n\nIf we select a person at random, what is the expected amount of time (in minutes) they will spend waiting in line? What about the variance and standard deviation of the time (in minutes) they will spend waiting in line?"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#normal-distribution-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#normal-distribution-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\nThe normal distribution takes two parameters \\(\\mu\\) and \\(\\sigma\\). We use the notation \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\) to denote “\\(X\\) follows the normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\).”\nThe normal distribution has distribution function given by \\[ f(x) = \\frac{1}{\\sigma \\cdot \\sqrt{2 \\pi}} \\cdot \\exp\\left\\{ - \\frac{1}{2} \\cdot \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 \\right\\} \\]\nLet’s determine how the parameters affect the shape of the density curve."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#changing-mu-and-sigma",
    "href": "Pages/Lectures/Lecture10/Lec10.html#changing-mu-and-sigma",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Changing \\(\\mu\\) and \\(\\sigma\\)",
    "text": "Changing \\(\\mu\\) and \\(\\sigma\\)\n\nviewof µ = Inputs.range(\n  [-3, 3], \n  {value: 0, step: 0.1, label: \"µ:\"}\n)\n\nviewof σ = Inputs.range(\n  [0.2, 3.1], \n  {value: 1, step: 0.01, label: \"σ:\"}\n)\n\nsigsquared = σ**2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmargin = ({top: 20, right: 30, bottom: 30, left: 40})\n\nheight = 400\n\nx_values = d3.scaleLinear()\n    .domain(d3.extent(data, d =&gt; d.x))\n    .range([margin.left, width - margin.right])\n\ny_values = d3.scaleLinear()\n    .domain([Math.min(d3.min(data, d =&gt; d.y),0), Math.max(1,d3.max(data, d =&gt; d.y))]).nice()\n    .range([height - margin.bottom, margin.top])\n    \nline = d3.line()\n    .x(d =&gt; x_values(d.x))\n    .y(d =&gt; y_values(d.y))\n\nxAxis = g =&gt; g\n  .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n  .call(d3.axisBottom(x_values)\n      .ticks(width / 80)\n      .tickSizeOuter(0))\n\nyAxis = g =&gt; g\n  .attr(\"transform\", `translate(${margin.left},0)`)\n  .call(d3.axisLeft(y_values)\n      .tickValues(d3.scaleLinear().domain(y_values.domain()).ticks()))\n    \nfunction normal_pdf (input_value, mu, sigsq) {\n  let left_chunk = 1/(Math.sqrt(2*Math.PI*sigsq))\n  let right_top = -((input_value-mu)**2)\n  let right_bottom = 2*sigsq\n  return left_chunk * Math.exp(right_top/right_bottom)\n}\n\nabs_x=6\n\ndata = {\n  let values = [];\n  for (let x = -abs_x; x &lt; abs_x; x=x+0.01) values.push({\"x\":x,\"y\":normal_pdf(x, µ, sigsquared)});\n  return values;\n}\n\nd3 = require(\"https://d3js.org/d3.v5.min.js\")\n\nchart = {\n  const svg = d3.select(DOM.svg(width, height));\n\n  svg.append(\"g\")\n      .call(xAxis);\n\n  svg.append(\"g\")\n      .call(yAxis);\n  \n  svg.append(\"path\")\n      .datum(data)\n      .attr(\"fill\", \"none\")\n      .attr(\"stroke\", \"steelblue\")\n      .attr(\"stroke-width\", 4)\n      .attr(\"stroke-linejoin\", \"round\")\n      .attr(\"stroke-linecap\", \"round\")\n      .attr(\"d\", line);\n  \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit to https://observablehq.com/@dswalter/normal-distribution for the majority of the applet code"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#changing-mu",
    "href": "Pages/Lectures/Lecture10/Lec10.html#changing-mu",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Changing \\(\\mu\\)",
    "text": "Changing \\(\\mu\\)\nHolding \\(\\sigma = 1\\) fixed and varying \\(\\mu\\), we find:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#changing-sigma",
    "href": "Pages/Lectures/Lecture10/Lec10.html#changing-sigma",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Changing \\(\\sigma\\)",
    "text": "Changing \\(\\sigma\\)\nHolding \\(\\mu = 0\\) fixed and varying \\(\\sigma\\), we find:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#standard-normal-distribution",
    "href": "Pages/Lectures/Lecture10/Lec10.html#standard-normal-distribution",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe standard normal distribution is the normal distribution with \\(\\mu = 0\\) and \\(\\sigma = 1\\); i.e. \\(\\mathcal{N}(0, 1)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#normal-probabilities",
    "href": "Pages/Lectures/Lecture10/Lec10.html#normal-probabilities",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Normal Probabilities",
    "text": "Normal Probabilities\n\nRecall that for continuous variables, probabilities are found as areas underneath the density curve. For example, if \\(X \\sim \\mathcal{N}(0, 1)\\), then \\(\\mathbb{P}(X \\leq -1)\\) is found by computing the area below:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#normal-probabilities-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#normal-probabilities-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Normal Probabilities",
    "text": "Normal Probabilities\n\nNow, unlike with the Uniform density curve, we don’t have a simple closed-form formula for areas under the Normal curve.\nFor instance, how would you get a numerical value for the area shaded on the previous slide?\nThe answer is by way of what is known as a normal table, or z-table.\nTo illustrate how to read a normal table, let’s work through an example:\n\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nIf \\(Z \\sim \\mathcal{N}(0, 1)\\), compute \\(\\mathbb{P}(Z \\leq 0.83)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#normal-table",
    "href": "Pages/Lectures/Lecture10/Lec10.html#normal-table",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Normal Table",
    "text": "Normal Table"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#reading-the-normal-table",
    "href": "Pages/Lectures/Lecture10/Lec10.html#reading-the-normal-table",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Reading the Normal Table",
    "text": "Reading the Normal Table\n\nTo find \\(\\mathbb{P}(Z \\leq 0.83)\\), we break up \\(0.83\\) as \\[ 0.83 = 0.8 + 0.03 \\]\nThis tells us to find the desired probability in the intersection of the \\(0.8\\) row and the \\(0.03\\) column:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#another-example-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#another-example-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Another Example",
    "text": "Another Example\n\n\n\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nIf \\(Z \\sim \\mathcal{N}(0, 1)\\), find\n\n\\(\\mathbb{P}(Z \\leq -1.01)\\)\n\\(\\mathbb{P}(Z \\leq -2.25)\\)\n\\(\\mathbb{P}(-2.25 \\leq Z \\leq -1.01)\\)\n\\(\\mathbb{P}(X \\geq -0.7)\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#standardization",
    "href": "Pages/Lectures/Lecture10/Lec10.html#standardization",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Standardization",
    "text": "Standardization\n\nNow, all of our considerations above were in the case of the standard normal distribution. How do we find areas under nonstandard normal density curves?\nThe answer: we use a process called standardization.\n\n\n\n\n\n\n\n\nStandardization\n\n\n\nIf \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\), then \\[ \\left( \\frac{X - \\mu}{\\sigma} \\right) \\sim \\mathcal{N}(0, 1) \\] That is, if we take a normally distributed random variable, subtract off its mean, and divide by its standard deviation, we obtain a random variable whose distribution is the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#normal-probabilities-general-case",
    "href": "Pages/Lectures/Lecture10/Lec10.html#normal-probabilities-general-case",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Normal Probabilities; General Case",
    "text": "Normal Probabilities; General Case\n\nThus, if \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\), here are the steps we use to compute \\(\\mathbb{P}(X \\leq x)\\):\n\nCompute the \\(z-\\)score \\(z = \\frac{x - \\mu}{\\sigma}\\), rounded to two decimal places.\nLook up the corresponding entry in a standard normal table."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#your-turn-2",
    "href": "Pages/Lectures/Lecture10/Lec10.html#your-turn-2",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nIt is found that the scores on a particular exam are normally distributed with a mean of 83 and a standard deviation of 5.\n\nDefine the random variable of interest, and call it \\(X\\).\nIf a student is selected at random, what is the probability that they scored 81 or lower?\nIf a student is selected at random, what is the probability that they scored 75 or higher?"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#mean-and-variance-of-the-normal-distribution",
    "href": "Pages/Lectures/Lecture10/Lec10.html#mean-and-variance-of-the-normal-distribution",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Mean and Variance of the Normal Distribution",
    "text": "Mean and Variance of the Normal Distribution\n\nIf \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\), we have the following results:\n\n\\(\\displaystyle \\mathbb{E}[X] = \\mu\\)\n\\(\\displaystyle \\mathrm{Var}(X) = \\sigma^2\\)\n\nSo, the two parameters we use to describe the normal distribution are the mean and the variance.\nWe’ll talk more about parameters in the next lecture."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#taking-stock",
    "href": "Pages/Lectures/Lecture20/Lec20.html#taking-stock",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Taking Stock",
    "text": "Taking Stock\n\nLet’s start off today’s lecture by taking stock of what we’ve done in this class so far.\nWe started off by talking about descriptive statistics, which seems to describe data.\n\nThese descriptions can be visual (e.g. boxplots, scatterplots, etc.) or numerical (e.g. measures of central tendency, measures of spread, etc.)\n\nNext, we talked about probability, which sought to give us a framework with which to discuss randomness.\n\nKey notions include: random variables, and distributions (e.g. uniform, normal, etc.)"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#taking-stock-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#taking-stock-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Taking Stock",
    "text": "Taking Stock\n\nAfter that, we spent a few weeks talking about inference.\n\nThis refers to the situation in which we have a population governed by a parameter, and we wish to take samples from this population and use these samples to make inferences about the population parameter.\nWe also discussed how to compare population means across multiple populations, using two-sample t-tests and ANOVA."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#scatterplots-and-trends",
    "href": "Pages/Lectures/Lecture20/Lec20.html#scatterplots-and-trends",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Scatterplots and Trends",
    "text": "Scatterplots and Trends\n\nLet’s take a moment to revisit some material from Lecture 2.\nSpecifically, suppose we have data of the form \\(\\{(y_i, \\ x_i)\\}_{i=1}^{n}\\).\n\nFor simplicity’s sake, let’s assume both x and y are numerical.\n\nWe have previously seen that the best way to visualize the relationship between y and x is with a scatterplot"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#no-relationship",
    "href": "Pages/Lectures/Lecture20/Lec20.html#no-relationship",
    "title": "PSTAT 5A: Lecture 20",
    "section": "No Relationship",
    "text": "No Relationship\n\nSometimes, two variables will have no relationship at all:"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#statistical-modeling",
    "href": "Pages/Lectures/Lecture20/Lec20.html#statistical-modeling",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Statistical Modeling",
    "text": "Statistical Modeling\n\nThe goal of statistical modeling, loosely speaking, is to try and model the relationship between x and y.\nSpecifically, we assume the relationship takes the form \\[ \\texttt{y} = f(\\texttt{x}) + \\texttt{noise} \\] where \\(f()\\) is some function (e.g. linear, nonlinear, etc.)\nHang on, what’s the noise term doing?\nWell, take a look at the previous scatterplots. Even though many of these display (what we visually would describe as) relationships between x and y, the datapoints do not fall perfectly along a single curve."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#statistical-modeling-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#statistical-modeling-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Statistical Modeling",
    "text": "Statistical Modeling\n\nAs a concrete example, suppose y represents weight and x represents height.\nWe do believe there would be some sort of a positive association between weight and height (taller people tend to, in general, weigh more) and we may even assume the relationship is linear.\nHowever, just because we know someone’s height, we don’t exactly know what their weight will be.\n\nAgain, this is because there is uncertainty and randomness inherent in pretty much everything!"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#terminology",
    "href": "Pages/Lectures/Lecture20/Lec20.html#terminology",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Terminology",
    "text": "Terminology\n\nAlright, so: given variables y and x, we assume the relationship between them can be modeled as \\[ \\texttt{y} = f(\\texttt{x}) + \\texttt{noise} \\]\nHere is some terminology we use:\n\ny is referred to as the response variable, or sometimes the dependent variable.\nx is referred to as the explanatory variable, or sometimes the independent variable.\nThe function \\(f()\\) is sometimes referred to as the signal function."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#terminology-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#terminology-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Terminology",
    "text": "Terminology\n\nWe also have some additional terminology about the entire model, based on the type of the response variable.\n\nIf y is numerical, we call the resulting model a regression model (or we just say we are in a regression setting)\nIf y is categorical, we call the resulting model a classification model (or we just say we are in a classification setting)\n\nSo, for example, trying to model the relationship between weight and height (assuming weight is our response variable) is a regression problem, since weight is a numerical variable."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#classification-problem",
    "href": "Pages/Lectures/Lecture20/Lec20.html#classification-problem",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Classification Problem",
    "text": "Classification Problem\n\nAs an illustration of a classic classification problem, suppose we have access to a roster of passengers on the Titanic.\n\nFor those who don’t know, the RMS Titanic was a British passenger line that crahsed into an iceberg on April 15, 1912.\n\nTragically, not all passengers survived.\nOne question we may want to ask is: how did various factors (e.g. class, gender, etc.) affect whether or not a given passenger survived?\n\nFor instance, since women and children boarded lifeboats first, it may be plausible surmise that women and children had a higher likelihood of surviving."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#classification-problem-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#classification-problem-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Classification Problem",
    "text": "Classification Problem\n\nNotice how here, the response variable (i.e. whether or not someone survived) is a binary variable; that is, it takes only one of two values (survived or did not survive). As such, this is a classification problem.\n\nI should note: perhaps confusingly, a problem in which the response is binary is sometimes called logistic regression.\nBut, that is a bit of a misnomer- it is technically a classification problem, since the response variable is categorical.\n\nBy the way, there is an actual dataset on the Titanic at https://www.kaggle.com/competitions/titanic\nAgain, we won’t have a chance to talk about logistic regression in this class- for that, I refer you to a Machine Learning class like PSTAT 131/231!"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#multivariate-models",
    "href": "Pages/Lectures/Lecture20/Lec20.html#multivariate-models",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Multivariate Models",
    "text": "Multivariate Models\n\nWe don’t have to restrict ourselves to modeling the relationship between a response variable and a single explanatory variable.\n\nFor example, suppose y measures income.\nThere are several factors that might contribute to someone’s monthly income: things like education_level, gender, industry, etc.\n\nWe can easily adapt a linear regression model to allow for multiple explanatory variables, which leads to what is known as a multivariate regression/classification model: \\[ \\texttt{y} = f(\\texttt{x}_1, \\ \\cdots , \\ \\texttt{x}_k) + \\texttt{noise} \\]\n\nA full treatment of multivariate models requires linear alebra, so I refer you to a course like PSTAT 126 or PSTAT 131/231 for further details."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#linear-models",
    "href": "Pages/Lectures/Lecture20/Lec20.html#linear-models",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Linear Models",
    "text": "Linear Models\n\nFor now, let’s focus on modeling the relationship between a numerical response variable and a single numerical explanatory variable: \\[ \\texttt{y} = f(\\texttt{x}) + \\texttt{noise} \\]\nOf particular interest to statisticians are the class of linear models, which assume a linear form for the signal function.\nRecall that a line is described by two parameters: an intercept and a slope. As such, to say we are assuming a “linear form” for \\(f()\\) is to say we are assuming \\(f(x) = \\beta_0 + \\beta_1 \\cdot x\\), so our model becomes \\[ \\texttt{y} = \\beta_0 + \\beta_1 \\cdot \\texttt{x} + \\texttt{noise} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#strength-of-a-relationship",
    "href": "Pages/Lectures/Lecture20/Lec20.html#strength-of-a-relationship",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Strength of a Relationship",
    "text": "Strength of a Relationship\n\nNow, before we delve into the mathematics and mechanics of model fitting, there is another thing we should be aware of.\nAs an example, consider the following two scatterplots:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth scatterplots display a positive linear trend. However, the relationship between Y2 and X2 seems to be “stronger” than the relationship between Y1 and X1, does it not?"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#correlation-coefficient",
    "href": "Pages/Lectures/Lecture20/Lec20.html#correlation-coefficient",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\nUltimately, we would like to develop a mathematical metric to quantify not only the relationship between two variables, but also the strength of the relationship between these two variables.\nThis quantity is referred to as the correlation coefficient.\nNow, it turns out there are actually a few different correlation coefficients out there. The one we will use in this class (and one of the metrics that is very widely used by statisticians) is called Pearson’s Correlation Coefficient, or often just Pearson’s r (as we use the letter r to denote it.)"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#pearsons-r",
    "href": "Pages/Lectures/Lecture20/Lec20.html#pearsons-r",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Pearson’s r",
    "text": "Pearson’s r\n\nGiven two sets \\(X = \\{y_i\\}_{i=1}^{n}\\) and \\(Y = \\{y_i\\}_{i=1}^{n}\\) (note that we require the two sets to have the same number of elements!), we compute r using the formula \\[ r = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\overline{x}}{s_X} \\right) \\left( \\frac{y_i - \\overline{y}}{s_Y} \\right)  \\] where:\n\n\\(\\overline{x}\\) and \\(\\overline{y}\\) denote the sample means of \\(X\\) and \\(Y\\), respectively\n\\(s_X\\) and \\(s_Y\\) denote the sample standard deviations of \\(X\\) and \\(Y\\), respectively."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#example",
    "href": "Pages/Lectures/Lecture20/Lec20.html#example",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Example",
    "text": "Example\n\nI find it useful to sometimes consider extreme cases, and ensure that the math matches up with our intuition.\nFor example, consider the sets \\(X = \\{1, 2, 3\\}\\) and \\(Y = \\{1, 2, 3\\}\\).\nFrom a scatterplot, I think we would all agree that \\(X\\) and \\(Y\\) have a positive linear relationship, and that the relationship is very strong!"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#example-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#example-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Example",
    "text": "Example\n\nIndeed, \\(\\overline{x} = 2 = \\overline{y}\\) and \\(s_X = 1 = s_Y\\), meaning \\[\\begin{align*}\nr   & = \\frac{1}{3 - 1} \\left[ \\left( \\frac{1 - 2}{1} \\right) \\left( \\frac{1 - 2}{1} \\right)  + \\left( \\frac{2 - 2}{1} \\right) \\left( \\frac{2 - 2}{1} \\right)  \\right.   \\\\\n  & \\hspace{45mm} \\left. +  \\left( \\frac{3 - 2}{1} \\right) \\left( \\frac{3 - 2}{1} \\right)  \\right] \\\\\n  & = \\frac{1}{2} \\left[ 1 + 0 + 1 \\right] = \\boxed{1}\n\\end{align*}\\]\nIt turns out, r will always be between \\(-1\\) and \\(1\\), inclusive, regardless of what two sets we are comparing!"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#interpretation",
    "href": "Pages/Lectures/Lecture20/Lec20.html#interpretation",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Interpretation",
    "text": "Interpretation\n\nSo, here is how we interpret the value of r.\n\nThe sign of r (i.e. whether it is positive or negative) indicates whether or not the linear association between the two variables is positive or negative.\nThe magnitude of r indicates how strong the linear relationship between the two variables is, with magnitudes close to \\(1\\) or \\(-1\\) indicating very strong linear relationships.\nAn r value of 0 indicates no linear relationship between the variables."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#important-distinction",
    "href": "Pages/Lectures/Lecture20/Lec20.html#important-distinction",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Important Distinction",
    "text": "Important Distinction\n\nNow, something that is very important to mention is that r only quantifies linear relationships- it is very bad at quantifying nonlinear relationships.\nFor example, consider the following scatterplot:"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#important-distinction-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#important-distinction-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Important Distinction",
    "text": "Important Distinction\n\nI think we would all agree that Y and X have a fairly strong relationship.\nHowever, the correlation between Y and X is actually only 0.1953333!\nSo, again- r should only be used as a determination of the strength of linear trends, not nonlinear trends."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#your-turn",
    "href": "Pages/Lectures/Lecture20/Lec20.html#your-turn",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Your Turn!",
    "text": "Your Turn!"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#your-turn-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nCompute the correlation between the following two sets of numbers: \\[\\begin{align*}\n  \\boldsymbol{x}    & = \\{-1, \\ 0, \\ 1\\}    \\\\\n  \\boldsymbol{y}    & = \\{1, \\ 2, \\ 0\\}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#leadup",
    "href": "Pages/Lectures/Lecture20/Lec20.html#leadup",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Leadup",
    "text": "Leadup\n\nThere is another thing to note about correlation.\nLet’s see this by way of an example: consider the following two scatterplots:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth cor(X, Y1) and cor(X, Y2) are equal to 1, despite the fact that a one unit increase in x corresponds to a different unit increase in y1 as opposed to y2."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#leadup-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#leadup-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Leadup",
    "text": "Leadup\n\nSo, don’t be fooled- the magnitude of r says nothing about how a one-unit increase in x translates to a change in y!\n\nAgain, the magnitude of r only tells us how strongly the two variables are related.\n\nTo figure out exactly how a change in x translates to a change in y, we need to return to our model."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#regression-problem",
    "href": "Pages/Lectures/Lecture20/Lec20.html#regression-problem",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Regression Problem",
    "text": "Regression Problem\n\nAs a concrete example, let’s examine a dataset that contains several measurements on heights and weights:"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#regression-model",
    "href": "Pages/Lectures/Lecture20/Lec20.html#regression-model",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Regression Model",
    "text": "Regression Model\n\nLetting y denote our response variable (weight) and x denote our explanatory variable (height), our model is \\[ \\texttt{y} = \\beta_0 + \\beta_1 \\cdot \\texttt{x} + \\texttt{noise} \\]\n\nIf you wanted to be a little more explicit, you could also write \\[ \\texttt{weight} = \\beta_0 + \\beta_1 \\cdot (\\texttt{height}) + \\texttt{noise} \\]\n\nUltimately, we would like to know the values of \\(\\beta_0\\) and \\(\\beta_1\\).\nHowever, we will never be able to determine their values exactly!\nWhy? Well, take a look at the model again- though we are assuming a linear relationship between height and weight, our weight observations contain some noise, the magnitude of which we never get to see!\n\nIn this way, we can think of \\(\\beta_0\\) and \\(\\beta_1\\) as population parameters."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#fitting-the-model",
    "href": "Pages/Lectures/Lecture20/Lec20.html#fitting-the-model",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Fitting the Model",
    "text": "Fitting the Model\n\nOur goal, therefore, is to try and determine good estimators \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) of \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\nIn the more general setting, given a model \\[ \\texttt{y} = f(\\texttt{x}) + \\texttt{noise} \\] the goal of model fitting is to take data \\(\\{(y_i, \\ x_i)\\}_{i=1}^{n}\\) and use it to construct a function \\(\\widehat{f}()\\) that we believe best approximates the function \\(f():\\) \\[ \\widehat{\\texttt{y}} = \\widehat{f}(\\texttt{x}) + \\texttt{noise} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#height-and-weight",
    "href": "Pages/Lectures/Lecture20/Lec20.html#height-and-weight",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Height and Weight",
    "text": "Height and Weight\n\nOkay, back to our height and weight example:"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#height-and-weight-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#height-and-weight-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Height and Weight",
    "text": "Height and Weight\n\nThe regression problem basically boils down to finding the line that best fits the data.\nThe specific line we will discuss in a bit is called the ordinary least squares line (or just OLS line):"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#simple-linear-regression",
    "href": "Pages/Lectures/Lecture20/Lec20.html#simple-linear-regression",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nHere is how simple linear regression works.\nWe are given observations \\(\\{(y_i, \\ x_i)\\}_{i=1}^{n}\\) on a response variable y and an explanatory variable x.\nIn simple linear regression, we adopt the following model: \\[ \\texttt{y} = \\beta_0 + \\beta_1 \\cdot \\texttt{x} + \\texttt{noise} \\]\nOur goal is to use the data \\(\\{(y_i, \\ x_i)\\}_{i=1}^{n}\\) to determine suitable estimators for \\(\\beta_0\\) and \\(\\beta_1\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#simple-linear-regression-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#simple-linear-regression-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nHere’s a visual way of thinking about this. Consider the following scatterplot:"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#goals",
    "href": "Pages/Lectures/Lecture20/Lec20.html#goals",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Goals",
    "text": "Goals\n\n\nWe are assuming that there exists some true linear relationship (i.e. some “fit”) between y and x. But, because of natural variability due to randomness, we cannot figure out exactly what the true relationship is."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#goals-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#goals-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Goals",
    "text": "Goals\n\n\nFinding the “best” estimate of the signal function is, therefore, akin to finding the line that “best” fits the data."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#line-of-best-fit",
    "href": "Pages/Lectures/Lecture20/Lec20.html#line-of-best-fit",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Line of Best Fit",
    "text": "Line of Best Fit\n\nNow, if we are to find the line that best fits the data, we first need to quantify what we mean by “best”.\nHere is one idea: consider minimizing the average distance from the datapoints to the line.\nAs a measure of “average distance from the points to the line”, we will use the so-called residual sum of squares (often abbreviated as RSS)."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#residuals",
    "href": "Pages/Lectures/Lecture20/Lec20.html#residuals",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Residuals",
    "text": "Residuals\n\nThe ith residual is defined to be the quantity \\(e_i\\) below:\n\n\n\n\n\nRSS is then just \\(\\displaystyle \\mathrm{RSS} = \\sum_{i=1}^{n} e_i^2\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#results",
    "href": "Pages/Lectures/Lecture20/Lec20.html#results",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Results",
    "text": "Results\n\nIt turns out, using a bit of Calculus, the estimators we seek (i.e. the ones that minimize the RSS) are \\[\\begin{align*}\n\\widehat{\\beta_1}   & = \\frac{\\sum\\limits_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum\\limits_{i=1}^{n} (x_i - \\overline{x})^2}    \\\\\n\\widehat{\\beta_0}   & = \\overline{y} - \\widehat{\\beta_1} \\overline{x}\n\\end{align*}\\]\nThese are what are known as the ordinary least squares estimators of \\(\\beta_0\\) and \\(\\beta_1\\), and the line \\(\\widehat{\\beta_0} + \\widehat{\\beta_1} x\\) is called the ordinary least-squares regression line (or just OLS regression line, for short).\nPerhaps an example may illustrate what I am talking about."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#example-2",
    "href": "Pages/Lectures/Lecture20/Lec20.html#example-2",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#example-3",
    "href": "Pages/Lectures/Lecture20/Lec20.html#example-3",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Example",
    "text": "Example\n\n\n\\(\\widehat{\\beta_0} =\\) -0.2056061; \\(\\widehat{\\beta_1} =\\) -2.1049432.\nI.e. the equation of the line in blue is -0.2056061 + -2.1049432 * x."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#fitted-values",
    "href": "Pages/Lectures/Lecture20/Lec20.html#fitted-values",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Fitted Values",
    "text": "Fitted Values\n\nLet’s return to our cartoon picture of OLS regression:"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#fitted-values-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#fitted-values-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Fitted Values",
    "text": "Fitted Values\n\nNotice that each point in our dataset (i.e. the blue points) have a corresponding point on the OLS regression line:"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#fitted-values-2",
    "href": "Pages/Lectures/Lecture20/Lec20.html#fitted-values-2",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Fitted Values",
    "text": "Fitted Values\n\nThese points are referred to as fitted values; the y-values of the fitted values are denoted as \\(\\widehat{y}_i\\).\nIn this way, the OLS regression line is commonly written as a relationship between the fitted values and the x-values: \\[ \\widehat{y} = \\widehat{\\beta_0} + \\widehat{\\beta_1} x \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#back-to-height-and-weight",
    "href": "Pages/Lectures/Lecture20/Lec20.html#back-to-height-and-weight",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Back to height and weight",
    "text": "Back to height and weight\n\nBefore we work through the math once, let’s apply this technique to the height and weight data from before."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#back-to-height-and-weight-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#back-to-height-and-weight-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Back to height and weight",
    "text": "Back to height and weight\n\nUsing a computer software, the OLS regression line can be found to be:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecifically, \\(\\widehat{\\beta_0} =\\) 3.366744 and \\(\\widehat{\\beta_1} =\\) 0.9790114\n\n\n\n\nWe will return to the notion of fitted values in a bit."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#back-to-height-and-weight-2",
    "href": "Pages/Lectures/Lecture20/Lec20.html#back-to-height-and-weight-2",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Back to height and weight",
    "text": "Back to height and weight\n\nA quick note:\nThough there was no way to know this, the true \\(\\beta_1\\) was actually \\(1.0\\). Again, this is just to demonstrate that the OLS estimate \\(\\widehat{\\beta_1}\\) is just that- an estimate!"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#worked-out-example",
    "href": "Pages/Lectures/Lecture20/Lec20.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\nAlright, let’s work through a computation by hand once.\nSuppose we have the variables \\[\\begin{align*}\n\\boldsymbol{x}    & = \\{3, \\ 7, \\ 8\\}   \\\\\n\\boldsymbol{y}    & = \\{20, \\ 14, \\ 17\\}\n\\end{align*}\\] and suppose we wish to construct the least-squares regression line when regressing \\(\\boldsymbol{y}\\) onto \\(\\boldsymbol{x}\\).\nFirst, we compute \\[\\begin{align*}\n\\overline{x}    & = 6   \\\\\n\\overline{y}    & = 17\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#worked-out-example-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#worked-out-example-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\nNext, we compute \\[\\begin{align*}\n\\sum_{i=1}^{n} (x_i - \\overline{x})^2   & = (3 - 6)^2 + (7 - 6)^2 + (8 - 6)^2 = 14   \\\\\n\\sum_{i=1}^{n} (y_i - \\overline{y})^2   & = (20 - 17)^2 + (14 - 17)^2 + (17 - 17)^2 = 18\n\\end{align*}\\]\nAdditionally, \\[\\begin{align*}\n\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})   & = (3 - 6)(20 - 17) + (7 - 6)(14 - 17)    \\\\[-7mm]\n  & \\hspace{10mm} + (8 - 6)(17 - 17)    \\\\[5mm]\n  & = -12\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture20/Lec20.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\nTherefore, \\[ \\widehat{\\beta_1} = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^{n} (x_i - \\overline{x})^2}   = \\frac{-12}{14} = - \\frac{6}{7} \\]\nAdditionally, \\[ \\widehat{\\beta_0} = \\overline{y} - \\widehat{\\beta_1} \\overline{x} = 17 - \\left( - \\frac{6}{7} \\right) (6) = \\frac{155}{7} \\]\nThis means that the ordinary least-squares regression line is \\[ \\boxed{\\widehat{y} = \\frac{1}{7} ( 155 - 6 x )} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#interpreting-the-coefficients",
    "href": "Pages/Lectures/Lecture20/Lec20.html#interpreting-the-coefficients",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Interpreting the Coefficients",
    "text": "Interpreting the Coefficients\n\nAlright, so how do we interpret the OLS regression line? \\[\\widehat{y} = \\widehat{\\beta_0} + \\widehat{\\beta_1} x\\]\nWe can see that a one-unit increase in x corresponds to a \\(\\widehat{\\beta_1}\\)-unit increase in y.\n\nFor example, in our height and weight example we found \\[ \\widehat{\\texttt{weight}} = 3.367 + 0.979 \\cdot \\texttt{height} \\]\nThis means that a one-cm change in height is associated with a (predicted/estimated) 0.979 lbs change in weight."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#next-time",
    "href": "Pages/Lectures/Lecture20/Lec20.html#next-time",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Next Time",
    "text": "Next Time\n\nNext time, we’ll discuss how to use the OLS regression line to make predictions."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#leadup",
    "href": "Pages/Lectures/Lecture05/Lec05.html#leadup",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Leadup",
    "text": "Leadup\n\nRemember how, last week, we discussed ways to compare two variables?\nAt the time, we only considered comparing two numerical variables and comparing one numerical and one categorical variable.\nWhat about comparing two categorical variables?\nAs a concrete example, let’s return to…"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#penguins-revisited",
    "href": "Pages/Lectures/Lecture05/Lec05.html#penguins-revisited",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Penguins, Revisited",
    "text": "Penguins, Revisited\n\n\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#penguins-subsetted",
    "href": "Pages/Lectures/Lecture05/Lec05.html#penguins-subsetted",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Penguins, Subsetted",
    "text": "Penguins, Subsetted\n\n\n   species island   \n 1 Adelie  Torgersen\n 2 Adelie  Torgersen\n 3 Adelie  Torgersen\n 4 Adelie  Torgersen\n 5 Adelie  Torgersen\n 6 Adelie  Torgersen\n 7 Adelie  Torgersen\n 8 Adelie  Torgersen\n 9 Adelie  Torgersen\n10 Adelie  Torgersen\n# ℹ 334 more rows"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#some-questions",
    "href": "Pages/Lectures/Lecture05/Lec05.html#some-questions",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Some Questions",
    "text": "Some Questions\n\nHere are some questions we could ask:\n\nHow many Adelie penguins were found on Biscoe island?\nWere any Gentoo penguins found on Torgersen island?\nWhat proportion of Chinstrap penguins were found on Dream island?\n\nThese sorts of questions are very nicely answered by way of what is known as a contingency table:\n\n\n\n\n           \n            Biscoe Dream Torgersen\n  Adelie        44    56        52\n  Chinstrap      0    68         0\n  Gentoo       124     0         0\n\n\n\n\nThus, the answers to the questions above are: “44”, “no”, and “100%”, respectively."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#why-bring-this-up-now",
    "href": "Pages/Lectures/Lecture05/Lec05.html#why-bring-this-up-now",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Why Bring This Up Now?",
    "text": "Why Bring This Up Now?\n\nYou may be asking yourselves: “why bring this up now? Weren’t we talking about probability?”\nLet’s re-examine the third question we asked on the previous slide: What proportion of Chinstrap penguins were found on Dream island?\nWhat we really did when we answered this was to first restrict ourselves to the row of the contingency table corresponding to Chinstrap penguins, tally up the entries in that row, and then divided the number of penguins that were both Chinstrap and found on Dream Island by the row total."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#conditional-probability",
    "href": "Pages/Lectures/Lecture05/Lec05.html#conditional-probability",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nThis leads us to the main topic of today’s lecture: conditional probabilities.\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nIf \\(E\\) and \\(F\\) are two events with \\(\\mathbb{P}(F) \\neq 0\\), then we define the probability of \\(E\\) given \\(F\\), notated \\(\\mathbb{P}(E \\mid F)\\), to be \\[ \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)} \\] If \\(\\mathbb{P}(F) = 0\\), then \\(\\mathbb{P}(E \\mid F)\\) is not defined."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#interpretation",
    "href": "Pages/Lectures/Lecture05/Lec05.html#interpretation",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Interpretation",
    "text": "Interpretation\n\n\\(\\mathbb{P}(E \\mid F)\\) essentially gives us the proportion of \\(F\\) that is explained by \\(E\\).\n\nAs such, another way to think about conditional probabilities is as an “if-then” statement: if \\(F\\) has occurred, what is the probability that \\(E\\) also occurs?\n\nIf we adopt the classical approach to probability, we have \\[\\begin{align*}\n\\mathbb{P}(E \\mid F)    & = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}    \\\\\n  & = \\frac{\\left( \\frac{\\#(E \\cap F)}{\\#(\\Omega)} \\right)}{\\left( \\frac{\\#(F)}{\\#(\\Omega)} \\right)} = \\frac{\\#(E \\cap F)}{\\#(F)}\n\\end{align*}\\]\nThis is also why contingency tables are so useful in the context of probability- the numerator above will be an entry in the table, and the denominator will be either a row-sum or a column-sum (depending on how the table was constructed)."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example",
    "href": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Exercise 1\n\n\n\n75 UCSB students were surveyed about whether they like pineapple on pizza or not. In addition to their pineapple preference, their standing was also recorded.\n\n\n         Standing\nPineapple Freshman Junior Senior Sophomore\n      No        15      6      1        18\n      Yes       10      7      5        13\n\n\nA student is to be randomly selected. If the student is a Freshman, what is the probability that they like pineapple on pizza?\n\n\n\n\n\nAs always, we begin by defining events and notation. Let \\(P =\\) “the student likes pineapple on pizza” and \\(F =\\) “the student is a freshman”. We then seek \\(\\mathbb{P}(P \\mid F)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#multiplication-rule",
    "href": "Pages/Lectures/Lecture05/Lec05.html#multiplication-rule",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Multiplication Rule",
    "text": "Multiplication Rule\n\nRecall that. provided \\(\\mathbb{P}(F) \\neq 0\\) \\[ \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)} \\]\nWe can multiply both sides of this equation by \\(\\mathbb{P}(F)\\) to obtain the so-called multiplication rule:\n\n\n\n\n\n\n\n\nThe Multiplication Rule\n\n\n\n\\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F)\\), for any events \\(E\\) and \\(F\\) with \\(\\mathbb{P}(F) \\neq 0\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#leadup-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#leadup-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Leadup",
    "text": "Leadup\n\nNote that “\\(E\\) and \\(F\\)” is the same as “\\(F\\) and \\(E\\)”.\nThat is: \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(F \\cap E)\\).\nSo, if we interchange the place of \\(E\\) and \\(F\\) in the multiplication rule, we obtain \\[ \\mathbb{P}(E \\cap F) = \\mathbb{P}(F \\cap E) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) \\]\nThat is to say, we have \\[\\begin{align*}\n\\mathbb{P}(E \\cap F)    & = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)\n\\end{align*}\\]\nDividing the last equation by \\(\\mathbb{P}(F)\\) yields an important result:"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#bayes-rule",
    "href": "Pages/Lectures/Lecture05/Lec05.html#bayes-rule",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\n\n\n\n\n\n\nBayes’ Rule\n\n\n\n\\[ \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)}{\\mathbb{P}(F)} \\] for events \\(E\\) and \\(F\\) with \\(\\mathbb{P}(E) \\neq 0\\) and \\(\\mathbb{P}(F) \\neq 0\\).\n\n\n\n\n\n\nIn a sense, Bayes’ Rule gives us a way to “reverse the order of a conditional”"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\nAs an illustration, let’s return to our pineapple-on-pizza contingency table:\n\n\n\n\n         Standing\nPineapple Freshman Junior Senior Sophomore\n      No        15      6      1        18\n      Yes       10      7      5        13\n\n\n\n\nLetting \\(P\\) and \\(F\\) be defined as before, let’s compute \\(\\mathbb{P}(P \\mid F)\\) using Bayes’ Rule.\nWe need to first compute \\(\\mathbb{P}(F \\mid P)\\), which we see to be \\[ \\mathbb{P}(F \\mid P) = \\frac{10}{10 + 7 + 5 + 13} = \\frac{10}{35} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#hang-in-there",
    "href": "Pages/Lectures/Lecture05/Lec05.html#hang-in-there",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Hang In There!",
    "text": "Hang In There!\n\nAt the moment, it may not seem obvious why Bayes’ Rule is helpful.\n\nIt seems like it just makes more work!\n\nBut, rest assured, we will see a very practical application of Bayes’ Rule in a few slides.\nBefore we do, there’s just one more concept we need to discuss."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#leadup-2",
    "href": "Pages/Lectures/Lecture05/Lec05.html#leadup-2",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Leadup",
    "text": "Leadup\n\nConsider an event \\(F\\), and another event \\(E\\).\nIf \\(F\\) happened, it could have happened along with \\(E\\) or it could have happened along with not-\\(E\\).\nThat is, \\[ F = (F \\cap E) \\cup (F \\cap E^\\complement)\\]\nNow, let’s take the probability of both sides. Since the events on the RHS are disjoint, the probability on the RHS just becomes a sum of probabilities: \\[ \\mathbb{P}(F) = \\mathbb{P}(F \\cap E) + \\mathbb{P}(F \\cap E^\\complement) \\]\nFinally, we apply the Multiplication Rule to the probabilities on the RHS to obtain \\[ \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) + \\mathbb{P}(F \\mid E^\\complement) \\cdot \\mathbb{P}(E^\\complement) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#law-of-total-probability",
    "href": "Pages/Lectures/Lecture05/Lec05.html#law-of-total-probability",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\n\n\n\n\n\n\nThe Law of Total Probability\n\n\n\nGiven two events \\(E\\) and \\(F\\) with \\(\\mathbb{P}(E) \\neq 0\\) and \\(\\mathbb{P}(F) \\neq 0\\), we have \\[ \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) + \\mathbb{P}(F \\mid E^\\complement) \\cdot \\mathbb{P}(E^\\complement) \\]\n\n\n\n\n\n\nThis is often useful in the context of a Bayes’ Rule problem."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\nAlright, let’s get down to business and tackle a slightly more real-world problem.\n\n\n\n\n\n\n\n\nWorked-Out Exercise 1\n\n\n\nIt is known that a particular disease affects 5% of the population. There exists a test for this disease, but it is not perfect: there is a 10% chance it will return a “negative” result for a person who is actually infected, and there is a 8% chance it will return a “positive” result for a person who is actually healthy.\n\nArasha has taken a test for the disease, and it has indicated a “positive” result. What is the probability that Arasha actually has the disease?"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-1-define-events",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-1-define-events",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 1: Define Events",
    "text": "Step 1: Define Events\n\nAs always, we start by defining events.\nLet + denote “the test returns a positive result” and let \\(D\\) denote “Arasha actually has the disease.”\nFirst of all, note that we are not interested in simply finding \\(\\mathbb{P}(D)\\); rather, we are interested in finding \\(\\mathbb{P}(D \\mid +)\\).\n\nThis is because Arasha has already been tested and received a positive result; this is information we need to incorporate into our beliefs!"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 2: Translate the Information",
    "text": "Step 2: Translate the Information\n\nWith our events from Step 1, we now turn our attention to translating the information provided in the problem.\nSince there is a \\(10\\%\\) chance that the test returns a negative result given that a person actually has the disease, we have \\[ \\mathbb{P}(+^\\complement \\mid D) = 0.1 \\]\nAdditionally, we are told that there is an \\(8\\%\\) chance that the test returns a positive result given that a person does not have disease, we have \\[ \\mathbb{P}(+ \\mid D^\\complement) = 0.08 \\]\nFinally, we are told that 5% of the population has the disease; hence, \\[ \\mathbb{P}(D) = 0.05 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 2: Translate the Information",
    "text": "Step 2: Translate the Information\n\nBut wait- there’s more!\nGiven that a person has the disease, they will either test positive or test negative.\n\nWhat that means is that \\[ \\mathbb{P}(+ \\mid D) = 1 - \\mathbb{P}(+^\\complement \\mid D) = 1 - 0.1 = 0.9 \\]\nThink of this as a modified complement rule\n\nSimilarly, \\[ \\mathbb{P}(+^\\complement \\mid D^\\complement) = 1 - \\mathbb{P}(+ \\mid D^\\complement) = 1 - 0.08 = 0.92 \\]\nAdditionally, \\[ \\mathbb{P}(D^\\complement) =  1 - \\mathbb{P}(D) = 1 - 0.05 = 0.95 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information-2",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information-2",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 2: Translate the Information",
    "text": "Step 2: Translate the Information\n\nSo, here’s a summary of everything we know, just from the problem statement: \\[\\begin{align*}\n\\mathbb{P}(+ \\mid D) = 0.9    & \\hspace{15mm} \\mathbb{P}(+^\\complement \\mid D) = 0.1    \\\\\n\\mathbb{P}(+ \\mid D^\\complement) = 0.08   & \\hspace{15mm}  \\mathbb{P}(P^\\complement \\mid D^\\complement) = 0.92   \\\\\n\\mathbb{P}(D) = 0.05    & \\hspace{15mm}  \\mathbb{P}(D^\\complement) = 0.95\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 3: Solve the Problem",
    "text": "Step 3: Solve the Problem\n\nNow we are in a position to begin solving the problem.\nRecall that we seek \\(\\mathbb{P}(D \\mid +)\\).\nBut, we only have information on \\(\\mathbb{P}(+ \\mid D)\\).\nAny ideas what rule/tool we should use?\n\nThat’s right; Bayes’ Rule!\n\nWe use Bayes’ Rule to write \\[ \\mathbb{P}(D \\mid +) = \\frac{\\mathbb{P}(+ \\mid D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(+)} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 3: Solve the Problem",
    "text": "Step 3: Solve the Problem\n\nDo we have \\(\\mathbb{P}(+)?\\)\n\nNo…\nBut how can we get it?\nYup- Law of Total Probability!\n\nWe use the Law of Total Probability to write\n\n\n\\[\\begin{align*}\n  \\mathbb{P}(+)   & = \\mathbb{P}(+ \\mid D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(+ \\mid D^\\complement) \\cdot \\mathbb{P}(D^\\complement)    \\\\\n    & = (0.9) \\cdot (0.05) + (0.08) \\cdot (0.95) = 0.121\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem-2",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem-2",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 3: Solve the Problem",
    "text": "Step 3: Solve the Problem\n\nFinally, putting everything together:\n\n\n\\[\\begin{align*}\n  \\mathbb{P}(D \\mid +)    & = \\frac{\\mathbb{P}(+ \\mid D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(+)}   \\\\\n    & = \\frac{(0.9) \\cdot (0.05)}{0.121} \\boxed{\\approx 37.19\\%}\n\\end{align*}\\]\n\n\nIf that seems low… you’re right! But, it is actually in line with the problem- the test for the disease is pretty bad, considering how often it gets things wrong. This is why this probability is low- because the test is so bad, we cannot be confident that Arasha actually has the disease, even though she tested positive!"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#some-terminology",
    "href": "Pages/Lectures/Lecture05/Lec05.html#some-terminology",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Some Terminology",
    "text": "Some Terminology\n\nBy the way, there’s some terminology I’d like to quickly introduce to make our lives easier going forward.\nThe False Positive Rate of a test is the proportion of times it returns a “positive” result, when the truth is actually “negative”.\n\nSo, in the context of epidemiology, the false positive rate of a test is the proportion of times it says someone has a disease when they do not actually have the disease.\n\nAnalogously, the False Negative Rate of a test is the proportion of times it returns a “negative” result, when the truth is actually “positive”.\n\nSo, in the context of epidemiology, the false negative rate of a test is the proportion of times it says someone does not have a disease when they do actually have the disease."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#some-problems-to-think-on",
    "href": "Pages/Lectures/Lecture05/Lec05.html#some-problems-to-think-on",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Some Problems To Think On:",
    "text": "Some Problems To Think On:\n\n\n\n\n\n\nExercise 1\n\n\n\n\nA recent survey interviewed several UCSB students about their pets. The following data was collected:\n\n\n             Animal\nAdopted       Bunny Cat Dog Hamster\n  Adopted         3   5   8       4\n  Not Adopted     1   5   7       7\n\n\n\nIf a pet is to be selected at random, what is the probability that it is either a cat or adopted?\nA pet is selected at random: what is the probability that it is an adopted dog?\nA pet is selected at random: if it is a dog, what is the probability that it was adopted?"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#leadup-3",
    "href": "Pages/Lectures/Lecture05/Lec05.html#leadup-3",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Leadup",
    "text": "Leadup\n\nThere is a very important concept we need to discuss before concluding our initial discussion on Conditional Probabilities.\nConsider two events \\(E\\) and \\(F\\).\n\n\\(\\mathbb{P}(E)\\) represents our beliefs on the event \\(E\\).\n\\(\\mathbb{P}(E \\mid F)\\) represents our beliefs on the event \\(E\\), in the presence of the additional information contained in \\(F\\).\n\nWhat happens if \\(\\mathbb{P}(E) = \\mathbb{P}(E \\mid F)\\)?\n\nThis asserts that our beliefs on \\(E\\) remain unchanged in the presence of \\(F\\).\nThat is, \\(E\\) is “unaffected” by \\(F\\) ; i.e. \\(E\\) is independent of \\(F\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#independence-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#independence-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Independence",
    "text": "Independence\n\n\n\n\n\n\n\nDefinition\n\n\n\nTwo events \\(E\\) and \\(F\\) are defined to be independent (notated \\(E \\perp F\\)) if any of the following hold:\n\n\\(\\mathbb{P}(E \\mid F) = \\mathbb{P}(E)\\)\n\\(\\mathbb{P}(F \\mid E) = \\mathbb{P}(F)\\)\n\\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\cdot \\mathbb{P}(F)\\)\n\n\n\n\n\n\n\nNote- the probability of an intersection equals the product of unconditional (i.e. marginal) probabilities ONLY WHEN THE EVENTS ARE INDEPENDENT.\n\nIn general, the only way to compute the probability of an intersection is to use the Multiplication Rule."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#uncertainty",
    "href": "Pages/Lectures/Lecture03/Lec03.html#uncertainty",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Uncertainty",
    "text": "Uncertainty\n\n\nUncertainty surrounds us!\n\n\n\nStatistics is, in many ways, the study of uncertainty.\nProbability is the language of uncertainty; it gives us a way to quantify exactly how this uncertainty factors into our decision making process."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#experiment",
    "href": "Pages/Lectures/Lecture03/Lec03.html#experiment",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Experiment",
    "text": "Experiment\n\n\nWe begin with the notion of an experiment. In the context of Probability, we have the following definition:\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nAn experiment is any procedure that can be repeated an infinite number of times, and each time the procedure is repeated there are a fixed set of things that could occur.\n\n\n\n\n\n\nAn example of an experiment is tossing a coin:\n\nI could go into Storke field and toss a coin an infinite number of times, and each time I toss the coin it will land on either heads or tails."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#outcome-space",
    "href": "Pages/Lectures/Lecture03/Lec03.html#outcome-space",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Outcome Space",
    "text": "Outcome Space\n\n\nThe things that could occur on each repetition of an experiment are called outcomes.\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe outcome space of an experiment is the set \\(\\Omega\\) consisting of all outcomes of the experiment.\n\n\n\n\n\n\nFor instance, in the coin tossing example the outcome space is  \\(\\Omega =\\{\\)heads,  tails\\(\\}\\).\nAs an aside: some textbooks/professors refer to the outcome space as the sample space, and use the letter \\(S\\) to denote it.\n\nSo, if you are doing self-study and encounter the term “sample space”, know that it is the same thing as what we are calling the “outcome space”!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#worked-out-example",
    "href": "Pages/Lectures/Lecture03/Lec03.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\nLet’s do an example together.\n\n\n\n\n\n\n\n\nWorked-Out Exercise 1\n\n\n\nConsider the experiment of rolling two four-sided dice and recording the faces that appear. What is an appropriate outcome space for this experiment?\n\n\n\n\n\nFirst, let’s find the outcome space.\nOn each dice roll, we will observe either a \\(1\\), \\(2\\), \\(3\\), or \\(4\\).\nBut, we cannot simply say that our outcome space is \\(\\{1, 2, 3, 4\\}\\) as this does not take into account the fact that we rolled two dice!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#now-its-your-turn",
    "href": "Pages/Lectures/Lecture03/Lec03.html#now-its-your-turn",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Now it’s Your Turn!",
    "text": "Now it’s Your Turn!\n\n\n\n\n\n\nExercise 1\n\n\n\nConsider the experiment of tossing a coin, picking a number from the set \\(\\{1, 2\\}\\), and then tossing another coin. What is the outcome space of this experiment?"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#other-ways-of-describing-outcome-spaces",
    "href": "Pages/Lectures/Lecture03/Lec03.html#other-ways-of-describing-outcome-spaces",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Other Ways of Describing Outcome Spaces",
    "text": "Other Ways of Describing Outcome Spaces\n\nThere are a few other ways we can use to describe the outcome space of an experiment.\nLet’s return to the tossing four dice example from a few slides ago. Another way we could have kept track of the outcomes was by using a table, recording the outcome of the first die roll in the rows and the outcomes of the second in the columns:\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n1\n\n\n(1, 1)\n\n\n(1, 2)\n\n\n(1, 3)\n\n\n(1, 4)\n\n\n\n\n2\n\n\n(2, 1)\n\n\n(2, 2)\n\n\n(2, 3)\n\n\n(2, 4)\n\n\n\n\n3\n\n\n(3, 1)\n\n\n(3, 2)\n\n\n(3, 3)\n\n\n(3, 4)\n\n\n\n\n4\n\n\n(4, 1)\n\n\n(4, 2)\n\n\n(4, 3)\n\n\n(4, 4)"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#other-ways-of-describing-outcome-spaces-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#other-ways-of-describing-outcome-spaces-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Other Ways of Describing Outcome Spaces",
    "text": "Other Ways of Describing Outcome Spaces\n\nSo, tables are a good way of keeping track of outcomes.\nBut, they really only work when we have two of something (e.g. two dice, two coins, etc.). What happens if we, for example, toss three coins?\nThis is where tree diagrams can become useful.\n\n\n\n\n\n\n\n\n\ntree_diagram\n\n\n\nbase\no\n\n\n\nH1\nH\n\n\n\nbase-&gt;H1\n\n\n\n\n\nT1\nT\n\n\n\nbase-&gt;T1\n\n\n\n\n\nH21\nH\n\n\n\nH1-&gt;H21\n\n\n\n\n\nT21\nT\n\n\n\nH1-&gt;T21\n\n\n\n\n\nH22\nH\n\n\n\nT1-&gt;H22\n\n\n\n\n\nT22\nT\n\n\n\nT1-&gt;T22\n\n\n\n\n\nH311\nH\n\n\n\nH21-&gt;H311\n\n\n\n\n\nT311\nT\n\n\n\nH21-&gt;T311\n\n\n\n\n\nH321\nH\n\n\n\nT21-&gt;H321\n\n\n\n\n\nT321\nT\n\n\n\nT21-&gt;T321\n\n\n\n\n\nH312\nH\n\n\n\nH22-&gt;H312\n\n\n\n\n\nT312\nT\n\n\n\nH22-&gt;T312\n\n\n\n\n\nH322\nH\n\n\n\nT22-&gt;H322\n\n\n\n\n\nT322\nT\n\n\n\nT22-&gt;T322"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#events",
    "href": "Pages/Lectures/Lecture03/Lec03.html#events",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Events",
    "text": "Events\n\nSometimes, it will be useful to consider quantities that are a bit more complex than single outcomes.\nFor example, consider the experiment of rolling two 4-sided dice. I could ask myself: in how many outcomes does the second dice roll result in a higher number than the first?\nThis leads us to the notion of an event.\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nAn event is a subset of the outcome space. In other words, an event is just a set consisting of one or more outcomes."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#unions-and-intersections",
    "href": "Pages/Lectures/Lecture03/Lec03.html#unions-and-intersections",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Unions and Intersections",
    "text": "Unions and Intersections\n\nRemember how we talked about the union of two sets last lecture?\nWell, since events are just sets, we can talk about the union of two sets.\n\nIn words, the union corresponds to an “or” statement.\nFor example, let \\(E\\) denote the event “it is raining” and \\(F\\) denote the event “the ground is wet”, then the event \\(E \\cup F\\) would be the event “it is raining or the ground is wet”.\n\nThe intersection of two events (denoted with the \\(\\cap\\) symbol), corresponds to an “and” statement\n\nFor example, if \\(E\\) and \\(F\\) are defined as in the bullet point above, then \\(E \\cap F\\) denotes the event “it is raining and the ground is wet”."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#complements",
    "href": "Pages/Lectures/Lecture03/Lec03.html#complements",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Complements",
    "text": "Complements\n\nThe complement of an event \\(E\\), denoted \\(E^{\\complement}\\), represents the event “not \\(E\\)”\n\nFor instance, if \\(E\\) again denotes the event “it is raining”, then \\(E^{\\complement}\\) denotes the event “it is not raining”."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#leadup",
    "href": "Pages/Lectures/Lecture03/Lec03.html#leadup",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Leadup",
    "text": "Leadup\n\nRecall that \\(E \\cap F\\) denotes the event “both \\(E\\) and \\(F\\) occurred.”\nAlso recall that \\(A^{\\complement}\\) denotes “not \\(A\\)”; i.e. “\\(A\\) did not occur”\nAs such, \\((E \\cap F)^{\\complement}\\) denotes the event “it is not the case that both \\(E\\) or \\(F\\) occurred.”\n\nThis means that either \\(E\\) did not occur, or \\(F\\) did not occur (or both).\nMathematically, this is equivalent to \\(E^\\complement \\cup F^\\complement\\).\n\nAs such, it seems we have arrived at the following equality: \\[ (E \\cap F)^{\\complement} = E^\\complement \\cup F^\\complement \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#demorgans-laws",
    "href": "Pages/Lectures/Lecture03/Lec03.html#demorgans-laws",
    "title": "PSTAT 5A: Lecture 03",
    "section": "DeMorgan’s Laws",
    "text": "DeMorgan’s Laws\n\n\nThis is one of what are known as DeMorgan’s Laws.\n\n\n\n\n\n\n\n\n\n\nDeMorgan’s Laws\n\n\n\nGiven two events \\(E\\) and \\(F\\), we have the following:\n\n\\((E \\cap F)^{\\complement} = E^\\complement \\cup F^\\complement\\)\n\\((E \\cup F)^{\\complement} = E^\\complement \\cap F^\\complement\\)\n\n\n\n\n\n\n\n\nWe will not prove these in this class. However, please familiarize yourself with them as they will be incredibly useful!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#leadup-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#leadup-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Leadup",
    "text": "Leadup\n\nNow, there is something interesting about the events defined in the previous example.\nThe outcome space of the underlying experiment is \\[ \\Omega = \\{ (H, H), \\ (H, T), \\ (T, H), \\ (T, T)\\} \\] and\n\n\\(A = \\{(H, H), \\ (T, T)\\}\\)\n\\(B = \\{(H, T), \\ (T, H)\\}\\)\n\\(C = \\{(H, H)\\}\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#probability",
    "href": "Pages/Lectures/Lecture03/Lec03.html#probability",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Probability",
    "text": "Probability\n\nNow, you may note that we have yet to mention the term “probability.”\nTo get a better sense of “probability”, let’s examine how we use the word in everyday speech:\n\n“the chance of rain is 50%”\n“odds of winning big at a Casino is 1%”\n“probability of scoring a 100% on the PSTAT 5A Midterm 1 is 95%”\n\nNotice that “rain”, “winning big at a Casino”, and “scoring 100% on the PSTAT 5A Midterm 1” are all events.\nAs such, “probability” seems to take in an event and spit out a number."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#probability-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#probability-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Probability",
    "text": "Probability\n\n\nIn other words, we can think of “probability” (or, more accurately, what we refer to as a probability measure) as a function that takes in an event and outputs a number.\n\n\n\nThe symbol we use for a probability measure is \\(\\mathbb{P}\\); i.e. we write \\(\\mathbb{P}(E)\\) to denote “the probability of event \\(E\\)”.\nNow, this doesn’t really tell us how to define \\(\\mathbb{P}(E)\\) for an arbitrary event \\(E\\).\nThere are (roughly) two schools of thought when it comes to defining the probability of an event: the long-run frequency approach, and the classical approach."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#long-run-frequency-approach",
    "href": "Pages/Lectures/Lecture03/Lec03.html#long-run-frequency-approach",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Long-Run-Frequency Approach",
    "text": "Long-Run-Frequency Approach\n\nThe long-run frequency approach defines the probability of an event \\(E\\) to be the proportion of times \\(E\\) occurs, if the underlying experiment were to be repeated a large number of times.\nTo help us understand the notion of long-run frequencies, let’s go through an example together. Suppose we toss a coin and record whether the outcome lands heads or tails, and further suppose we observe the following tosses:\n\n\n\nH,  T,   T,   H,   T,   H,   H,   H,   T,   T\n\n\n\nTo compute the relative frequency of heads after each toss, we count the number of times we observed heads and divide by the total number of tosses observed."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#classical-approach",
    "href": "Pages/Lectures/Lecture03/Lec03.html#classical-approach",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Classical Approach",
    "text": "Classical Approach\n\nThe second way to define probabilities is what is known as the classical approach.\nAs an important note: we can only apply the classical approach if we believe all outcomes in our experiment to be equally likely.\n\nExamples of situations in which it is safe to assume equally likely outcomes include: tossing a fair coin some number of times, rolling a fair \\(k\\)-sided die, selecting a card at random from a deck of cards, etc.\n\nIf we make the equally likely outcomes assumption, then the classical approach to probability tells us to define \\(\\mathbb{P}(E)\\) as \\[ \\mathbb{P}(E) = \\frac{\\text{number of ways $E$ can occur}}{\\text{total number of outcomes}} \\]\nSo, for example, if we toss a fair coin once, then the classical approach to probability (which can be used since the coin is fair) states that \\[ \\mathbb{P}(\\texttt{heads}) = \\frac{1}{2} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#comparisons",
    "href": "Pages/Lectures/Lecture03/Lec03.html#comparisons",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Comparisons",
    "text": "Comparisons\n\nLet’s quickly compare these two approaches to defining the probability of an event.\nThe long-run frequencies definition has the benefit of not requiring the assumption of equally likely outcomes.\n\nHowever, it relies on the (perhaps odd) consideration of considering what happens when we repeat an experiment a large number of times.\n\nThe classical approach does not rely on such considerations, making the definitions it produces perhaps a bit more easily interpretable.\n\nHowever, it crucially requires the assumption of equally likely outcomes."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#comparisons-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#comparisons-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Comparisons",
    "text": "Comparisons\n\n\nFor the purposes of this class, we won’t be too concerned with defining the probability of an event: in many cases, we will just give you the probability.\n\n\n\nIn situations where we do not provide a probability a priori, there will likely be some key word or phrase that lets you know we are looking for the classical definition.\n\nAgain, important words/phrases to look out for are: fair, at random, etc."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#axioms-of-probability",
    "href": "Pages/Lectures/Lecture03/Lec03.html#axioms-of-probability",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Axioms of Probability",
    "text": "Axioms of Probability\n\nIt turns out that there are three axioms that a probability measure must satisfy, collectively called the axioms of probability:\n\n\\(\\mathbb{P}(E) \\geq 0\\) for any event \\(E\\)\n\\(\\mathbb{P}(\\Omega) = 1\\)\nFor disjoint events \\(E\\) and \\(F\\), \\(\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F)\\).\n\nIf you are not familiar with the notion of axioms: an axiom is a fundamental “truth” of math, that does not need to be proven.\n\nAxioms are like the building blocks, or base assumptions on which a system of math is predicated."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#summary",
    "href": "Pages/Lectures/Lecture03/Lec03.html#summary",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Summary",
    "text": "Summary\n\nLet’s quickly summarize the concepts/terms we’ve covered:\n\nExperiment: any procedure that can be repeated an infinite number of times, where each time the procedure is repeated there are a fixed set of outcomes that can occur.\nOutcome Space: the set of all outcomes associated with a particular experiment.\nEvent: a subset of the outcome space\nProbability (measure): a function that takes an event and outputs a number\n\nThese are the basic building blocks of probability.\nWe will now combine them!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#notational-reminder",
    "href": "Pages/Lectures/Lecture03/Lec03.html#notational-reminder",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Notational Reminder",
    "text": "Notational Reminder\n\nBefore we go any further, I’d like to stress something:\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this class, using proper notation is very important.\n\n\n\n\n\n\n\nFor example, if we have an event \\(E\\) whose probability of occurring is, say, \\(0.5\\), we must write \\(\\mathbb{P}(E) = 0.5\\); it is NOT correct to say \\(E = 0.5\\), or \\(\\mathbb{P} = 0.5\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#the-complement-rule",
    "href": "Pages/Lectures/Lecture03/Lec03.html#the-complement-rule",
    "title": "PSTAT 5A: Lecture 03",
    "section": "The Complement Rule",
    "text": "The Complement Rule\n\nThe first result we will explore is the so-called complement rule.\n\n\n\n\n\n\n\n\n\nThe Complement Rule\n\n\n\nGiven an event \\(E\\), we have \\(\\mathbb{P}(E^\\complement) = 1 - \\mathbb{P}(E)\\)\n\n\n\n\n\n\n\nAs an example: if we roll a fair six-sided die and if \\(E =\\) “rolling a \\(1\\)”, then \\(E^\\complement =\\) “not rolling a \\(1\\)” and \\[\\mathbb{P}(E^\\complement) = 1 - \\mathbb{P}(E) = 1 - 1/6 = \\boxed{5/6}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#the-probability-of-the-empty-set",
    "href": "Pages/Lectures/Lecture03/Lec03.html#the-probability-of-the-empty-set",
    "title": "PSTAT 5A: Lecture 03",
    "section": "The Probability of the Empty Set",
    "text": "The Probability of the Empty Set\n\nRecall that the empty set (\\(\\varnothing\\)) is the set containing no elements.\n\n\n\n\n\n\n\n\n\nThe Probability of the Empty Set\n\n\n\n\\(\\mathbb{P}(\\varnothing) = 0\\).\n\n\n\n\n\n\n\nThe “proof” of this is relatively simple: note that \\(\\Omega^\\complement = \\varnothing\\) (the opposite of “everything” is “nothing”); we also know that \\(\\mathbb{P}(\\Omega) = 1\\) so \\[ \\mathbb{P}(\\Omega^\\complement) = \\mathbb{P}(\\varnothing) = 1 - 1 = 0 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#the-addition-rule",
    "href": "Pages/Lectures/Lecture03/Lec03.html#the-addition-rule",
    "title": "PSTAT 5A: Lecture 03",
    "section": "The Addition Rule",
    "text": "The Addition Rule\n\nThe next result we will explore is the so-called addition rule.\n\n\n\n\n\n\n\n\n\nThe Addition Rule\n\n\n\nGiven events \\(E\\) and \\(F\\), we have \\(\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F) - \\mathbb{P}(E \\cap F)\\)\n\n\n\n\n\n\n\nNote that if \\(E\\) and \\(F\\) are disjoint, then we recover the third axiom of probability!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#worked-out-example-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#worked-out-example-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nA recent survey at the Isla Vista Co-Op revealed that 50% of shoppers buy bread, 30% buy jam, and 20% buy both bread and jam.\n\nWhat is the probability that a randomly selected shopper will not purchase jam?\nWhat is the probability that a randomly selected shopper will purchase either bread or jam (or both)?"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#solution-to-part-a",
    "href": "Pages/Lectures/Lecture03/Lec03.html#solution-to-part-a",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Solution to Part (a)",
    "text": "Solution to Part (a)\n\nLet \\(J\\) denote the event “a randomly selected shopper will purchase jam”.\n\nFrom the problem statement, we have that \\(\\mathbb{P}(J) = 0.3\\)\n\nThe event “a randomly selected shopper will not purchase jam” is given by \\(J^\\complement\\), meaning the quantity we seek is \\(\\mathbb{P}(J^\\complement)\\).\nBy the Complement Rule, we have \\[ \\mathbb{P}(J^\\complement) = 1 - \\mathbb{P}(J) = 1 - 0.3 = \\boxed{0.7 = 70\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#solution-to-part-b",
    "href": "Pages/Lectures/Lecture03/Lec03.html#solution-to-part-b",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Solution to Part (b)",
    "text": "Solution to Part (b)\n\nLet \\(J\\) be defined as before, and let \\(B\\) denote the event “a randomly selected shopper will purchase bread or jam”\n\nThe first quantity provided in the problem statement tells us that \\(\\mathbb{P}(B) = 0.5\\)\nThe final quantity provided in the problem statement tells us that \\(\\mathbb{P}(B \\cap J) = 0.2\\)\n\nThe event “a randomly selected shopper will purchase either bread or jam” is given by \\(B \\cup J\\), meaning we seek \\(\\mathbb{P}(B \\cup J)\\).\nBy the Addition Rule,\n\n\n\\[\\begin{align*}\n  \\mathbb{P}(B \\cup J)     & = \\mathbb{P}(B) + \\mathbb{P}(J) - \\mathbb{P}(B \\cap J)   \\\\\n  & = 0.5 + 0.3 - 0.2 = \\boxed{0.6 = 60\\%}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#general-strategy",
    "href": "Pages/Lectures/Lecture03/Lec03.html#general-strategy",
    "title": "PSTAT 5A: Lecture 03",
    "section": "General Strategy",
    "text": "General Strategy\n\n\n\n\n\n\n\n\nGeneral Strategy for Probability Word Problems\n\n\n\n\nStart by defining events\nNext, translate the information provided to you (through the problem statement) to be in terms of the events you defined above\nThen, identify the quantity you are trying to obtain\nFinally, apply the various probability rules to solve for the desired quantity."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#time-to-put-everything-together",
    "href": "Pages/Lectures/Lecture03/Lec03.html#time-to-put-everything-together",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Time to Put Everything Together!",
    "text": "Time to Put Everything Together!\n\n\n\n\n\n\nExercise 5\n\n\n\n\nTwo fair six-sided dice are rolled.\n\nWhat is the outcome space of this experiment?\nWhat is the probability that the first die lands on the number 2?\nWhat is the probability that the first die lands on the number 2, or the second die lands on an even number?"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#venn-diagrams",
    "href": "Pages/Lectures/Lecture03/Lec03.html#venn-diagrams",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Venn Diagrams",
    "text": "Venn Diagrams\n\nI’d like to impart one additional tool that can help with visualizing the relationship between events: Venn Diagrams\nWe denote the outcome space \\(\\Omega\\) by a large rectangle, and denote events by circles.\nSince events are subsets of \\(\\Omega\\), we draw them inside (physically) the rectangle representing \\(\\Omega\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#summary-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#summary-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n \\(A^\\complement\\)  (complement)\n\n\n\n\n\n\n\n\n \\(A \\cap B\\)  (intersection)\n\n\n\n\n\n\n\n\n \\(A \\cup B\\)  (union)"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#demorgans-laws-2",
    "href": "Pages/Lectures/Lecture03/Lec03.html#demorgans-laws-2",
    "title": "PSTAT 5A: Lecture 03",
    "section": "DeMorgan’s Laws",
    "text": "DeMorgan’s Laws\n\nThis can also help give us some intuition on DeMorgan’s Laws as well!\n\nLet’s do this on the whiteboard together.\n\nWe will return to Venn Diagrams periodically throughout this course- for now, I hope they provide a useful tool to help you visualize the set operations we learned this lecture."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#lecture-summary-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#lecture-summary-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Lecture Summary",
    "text": "Lecture Summary\n\nToday, we began our introduction to the field of probability.\n\nProbability, loosely speaking, provides us with a way to quantify uncertainty.\n\nWe discussed the notions of experiments, outcomes, outcome spaces, events, and probabilities.\n\nRemember that there are certain tools/diagrams (namely, tables and tree diagrams) that can help us determine the outcome space of an experiment.\n\nWe then discussed three probability rules: the complement rule, the probability of the empty set, and the addition rule.\nWe also saw how Venn Diagrams can help us visualize set operations.\nNext time, we will start talking about ways to compute the probability of more complex events under the assumption of equally likely outcomes."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#last-time",
    "href": "Pages/Lectures/Lecture04/Lec04.html#last-time",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Last Time",
    "text": "Last Time\n\nLast time we discussed the basics of probability.\n\nThese included things like: experiments, outcome spaces, events, and probability.\n\nWe briefly talked about the different approaches to defining the probability of an even.\nFinally, we also saw how Venn Diagrams can help visualize set relationships and operations."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#leadup",
    "href": "Pages/Lectures/Lecture04/Lec04.html#leadup",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Leadup",
    "text": "Leadup\n\nNow, let’s return to the classical approach to probability.\nAssuming the outcomes in our outcome space \\(\\Omega\\) are equally likely, the classical approach tells us to compute the probability of any event \\(E\\) as \\[ \\mathbb{P}(E) = \\frac{\\text{number of ways $E$ can occur}}{\\text{total number of elements in $\\Omega$}} \\]\nUp until now, we’ve computed both the numerator and the denominator by explicitly listing out the elements contained in the respective sets, and then counting the number of elements.\nThis works decently for small sets, but is highly inefficient for large sets."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#ice-cream",
    "href": "Pages/Lectures/Lecture04/Lec04.html#ice-cream",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Ice Cream",
    "text": "Ice Cream\n\nBefore diving fully into the principles of counting, let’s examine a simple situation.\nSuppose we are at a small boutique ice cream parlor that offers only 3 flavors (Vanilla, Chocolate, and Matcha), and 2 toppings (sprinkles or coconut).\n\nFurther suppose that an order of ice cream must contain only 1 flavor and 1 topping.\n\nWe can list out the different orders that are possible (i.e. the outcome space of the experiment of ordering an ice cream from this shop) using a tree diagram:"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#fundamental-principle-of-counting",
    "href": "Pages/Lectures/Lecture04/Lec04.html#fundamental-principle-of-counting",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Fundamental Principle of Counting",
    "text": "Fundamental Principle of Counting\n\n\nThis is no accident!\n\n\n\n\n\n\n\n\n\n\nFundamental Principle of Counting\n\n\n\nIf an experiment consists of \\(k\\) stages, where the \\(i\\)th stage has \\(n_i\\) possible configurations, then the total number of elements in the outcome space is \\[ n_1 \\times n_2 \\times \\cdots \\times n_k \\]\n\n\n\n\n\n\n\nSo, when we obtained our answer of \\(6\\) on the previous slide, we were implicitly using the Fundamental Principle of Counting with 2 stages (picking a flavor, and picking a topping) where the first stage (picking a flavor) had 3 possible configurations (Vanilla, Chocolate, or Matcha) and the second stage (picking a topping) had two possible configurations (sprinkles or coconut)."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#slot-diagrams",
    "href": "Pages/Lectures/Lecture04/Lec04.html#slot-diagrams",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Slot Diagrams",
    "text": "Slot Diagrams\n\nWhen dealing with the Fundamental Principle of Counting, I find it useful to utilize what are sometimes referred to as slot diagrams.\nHere’s how we use slot diagrams:\n\nFirst put down as many slots as there are stages in our experiment: \\[ \\underline{\\ \\ \\ \\ \\ } \\ \\ \\  \\underline{\\ \\ \\ \\ \\ } \\ \\ \\ \\cdots \\ \\ \\   \\underline{\\ \\ \\ \\ \\ }  \\]\nThen, fill in each slot with the corresponding number of configurations: \\[ \\underline{\\ n_1 \\ } \\ \\ \\  \\underline{\\ n_2 \\ } \\ \\ \\ \\cdots \\ \\ \\  \\underline{\\ n_k \\ }  \\]\nFinally, invoke the Fundamental Principle of Counting to multiply the slots together: \\[ \\underline{\\ n_1 \\ } \\ \\times \\  \\underline{\\ n_2 \\ } \\ \\times \\ \\cdots \\ \\times  \\underline{\\ n_k \\ }\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example",
    "href": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSuppose a (different) ice cream parlor has 32 flavors, 5 toppings, and 3 drizzles. If a “scoop” consists of a flavor, topping, and drizzle, how many scoops can be created?\n\n\n\n\n\n\nThere are 3 stages: picking a flavor, picking a topping, and picking a drizzle. Hence, we draw three slots: \\[ \\underline{\\ \\ \\ \\ \\ } \\ \\ \\ \\underline{\\ \\ \\ \\ \\ } \\ \\ \\ \\underline{\\ \\ \\ \\ \\ } \\]\nThe first stage has 32 configurations, the second has 5, and the third has 3: \\[ \\underline{\\ 32 \\ } \\ \\ \\ \\underline{\\ 5 \\ } \\ \\ \\ \\underline{\\ 3 \\ } \\]\nFinally, we multiply through: \\[ \\underline{\\ 32 \\ } \\ \\times \\ \\underline{\\ 5 \\ } \\ \\times \\ \\underline{\\ 3 \\ } = \\boxed{480} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#ordering",
    "href": "Pages/Lectures/Lecture04/Lec04.html#ordering",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Ordering",
    "text": "Ordering\n\nHere’s a question: given \\(n\\) tickets (labeled \\(1\\) through \\(n\\)), how many different ways are there to arrange them in a line?\nLet’s answer this using a slot diagram!\nWe can think of \\(n\\) stages, where the first stage corresponds to placing the first ticket down, the second stage corresponds to placing the second ticket down, and so on and so forth. \\[ \\underbrace{ \\underline{\\ \\ \\ \\ \\ } \\ \\ \\  \\underline{\\ \\ \\ \\ \\ } \\ \\ \\ \\cdots \\ \\ \\   \\underline{\\ \\ \\ \\ \\ } \\ \\ \\   \\underline{\\ \\ \\ \\ \\ } }_{\\text{$n$ slots}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#factorials",
    "href": "Pages/Lectures/Lecture04/Lec04.html#factorials",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Factorials",
    "text": "Factorials\n\n\n\n\n\n\n\nDefinition\n\n\n\nFor a positive integer \\(n\\), we define \\(n\\) factorial (denoted \\(n!\\)), to be \\[ n! = n \\times (n - 1) \\times (n - 2) \\times \\cdots \\times 2 \\times 1 \\]\n\n\n\n\n\n\nFor example:\n\n\\(3! = 3 \\times 2 \\times 1 = 6\\)\n\\(4! = 4 \\times 3 \\times 2 \\times 1 = 24\\)\n\\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSiobhan has 4 shirts in her closet: 2 purple shirts and 2 red shirts. When organizing these shirts in her closet she wants to keep the purple shirts together and the red shirts together, but doesn’t care if the purple group is to the left or the right of the red group. How many ways are there for Siobhan to arrange these shirts in her closet?\n\n\n\n\n\n\nLet’s answer this question two ways: using direct enumeration, and then using counting techniques.\nLabel the two purple shirts \\(P_1\\) and \\(P_2\\) respectively, and label the two red shirts \\(R_1\\) and \\(R_2\\) respectively. Then here are all of the possible reorderings of the shirts:\n\n\n\\[\\begin{align*}\n  \\Omega = \\{ & P_1 P_2 R_1 R_2, \\ P_1 P_2 R_2 R_1, \\ P_2 P_1 R_1 R_2, \\ P_2 P_1 R_2 R_1 , \\\\\n  & R_1 R_2 P_1 P_2, \\ R_2 R_1P_1 P_2, \\ R_1 R_2 P_2 P_1, \\  R_2 R_1P_2 P_1 \\}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#does-order-matter",
    "href": "Pages/Lectures/Lecture04/Lec04.html#does-order-matter",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Does Order Matter?",
    "text": "Does Order Matter?\n\nLet’s now consider a slightly more abstract experiment: consider drawing two tickets from a box with tickets labeled \\(A\\) through \\(C\\), not replacing my first ticket after I draw it.\n\nHow many elements are in the outcome space of this experiment?\n\nWell, the answer is…. it depends!\nSpecifically, we need to know: does order matter?\nHere’s what I mean by order mattering: in a license plate, 123ABC and ABC123 are clearly two different license plates, despite the fact that they are comprised of the same letters and numbers!\n\nAn example of a situation in which order does not matter is drawing cards from a deck of cards: whether I get the Ace of Hearts before or after the King of Diamonds doesn’t matter- all that mattes is that I have the Ace of Hearts and the King of Diamonds!\nSpeaking of cards, you’ll discuss playing cards a bit more on future assignments."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#if-order-does-matter",
    "href": "Pages/Lectures/Lecture04/Lec04.html#if-order-does-matter",
    "title": "PSTAT 5A: Lecture 04",
    "section": "If Order Does Matter",
    "text": "If Order Does Matter\n\nLet’s examine what happens when we assume order does matter.\nIn the context of our drawing tickets example, this means that getting \\(A\\) followed by \\(C\\) is different than getting \\(C\\) followed by \\(A\\).\nThen, letting \\((X, Y)\\) denote the outcome “I drew the ticket labelled \\(X\\) first, then the ticket labelled \\(Y\\) second” (for \\(X \\in \\{A, B, C\\}\\) and \\(Y \\in \\{A, B, C\\}\\)), we have \\[\\begin{align*}\n  \\Omega   = \\{ & (A, B), \\ (A, C) \\\\\n      & (B, A), \\ (B, C)  \\\\\n      & (C, A), \\ (C, B) \\}\n\\end{align*}\\]\n\nBy the way, can anyone tell me why I didn’t include outcomes like \\((A, A)\\)?"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#generalizing",
    "href": "Pages/Lectures/Lecture04/Lec04.html#generalizing",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Generalizing",
    "text": "Generalizing\n\nLet’s generalize to picking \\(k\\) tickets from a total \\(n\\): \\[ \\underline{\\ \\ \\ \\ \\ {\\color{blue} {n}} \\ \\ \\ \\ \\ } \\ {\\times} \\\n  \\underline{\\ \\ \\ \\ \\ {\\color{blue}{n - 1}} \\ \\ \\ \\ \\ } \\ {\\times}  \\\n  \\underline{\\ \\ \\ \\ \\ {\\color{blue}{n - 2}} \\ \\ \\ \\ \\ } \\ {\\times}  \\\n  \\cdots \\ {\\times}  \n  \\underline{\\ \\ \\ \\ \\ {\\color{blue} {n - k + 1}} \\ \\ \\ \\ \\ } \\]\nWe can write this a little more succinctly using factorials: \\[ n \\times (n - 1) \\times \\cdots \\times (n - k + 1) = \\frac{n!}{(n - k)!} \\]\nThis is yet another quantity that arises so often, we give it a name: this time we call it \\(n\\) order \\(k\\), and write \\((n)_k\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#n-order-k",
    "href": "Pages/Lectures/Lecture04/Lec04.html#n-order-k",
    "title": "PSTAT 5A: Lecture 04",
    "section": "\\(n\\) order \\(k\\)",
    "text": "\\(n\\) order \\(k\\)\n\n\n\n\n\n\n\nDefinition\n\n\n\nFor a positive integer \\(n\\) and another positive integer \\(k\\) that is less than \\(n\\), \\[ (n)_k = \\frac{n!}{(n - k)!}  = n \\times (n - 1) \\times \\cdots \\times (n - k + 1)  \\]\n\n\n\n\n\n\nFor example:\n\n\\((5)_3 = 5 \\times 4 \\times 3 = 60\\)\n\\((6)_2 = 6 \\times 5 = 30\\)\n\\((4)_4 = 4 \\times 3 \\times 2 \\times 1 = 24\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nSuppose now that I have 5 tickets, labeled \\(A\\) through \\(E\\), and I now want to draw 3. How many ways are there to do this, assuming order matters?\n\n\n\n\n\n\nBy our work above, the answer is \\((5)_3 = 5 \\times 4 \\times 3 = \\boxed{60}\\).\nDon’t believe me?\n\n\n\\[{\\tiny \\begin{aligned}[t]\n\\Omega  & = \\{ (A, B, C), \\ (A, B, D), \\ (A, B, E), \\ (A, C, B), \\ (A, C, D), \\ (A, C, E), \\ (A, D, B), \\ (A, D, C), \\ (A, D, E), \\ (A, E, B), \\ (A, E, C), \\ (A, E, D), \\\\\n    %\n    & \\hspace{5mm} (B, A, C), \\ (B, A, D), \\ (B, A, E), \\ (B, C, A), \\ (B, C, D), \\ (B, C, E), \\ (B, D, A), \\ (B, D, C), \\ (B, D, E), \\ (B, E, A), \\ (B, E, C), \\ (B, E, D), \\\\\n    %\n    & \\hspace{5mm} (C, A, B), \\ (C, A, D), \\ (C, A, E), \\ (C, B, A), \\ (C, B, D), \\ (C, B, E), \\ (C, D, A), \\ (C, D, B), \\ (C, D, E), \\ (C, E, A), \\ (C, E, B), \\ (C, E, D), \\\\\n    %\n    & \\hspace{5mm} (D, A, B), \\ (D, A, C), \\ (D, A, E), \\ (D, B, A), \\ (D, B, C), \\ (D, B, E), \\ (D, C, A), \\ (D, C, B), \\ (D, C, E), \\ (D, E, A), \\ (D, E, B), \\ (D, E, C), \\\\\n    %\n    & \\hspace{5mm} (E, A, B), \\ (E, A, C), \\ (E, A, D), \\ (E, B, A), \\ (E, B, C), \\ (E, B, D), \\ (E, C, A), \\ (E, C, B), \\ (E, C, D), \\ (E, D, A), \\ (E, D, B), \\ (E, D, C) \\}\n    \\end{aligned}}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#order-doesnt-matter",
    "href": "Pages/Lectures/Lecture04/Lec04.html#order-doesnt-matter",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Order Doesn’t Matter",
    "text": "Order Doesn’t Matter\n\nLet’s return to our example of drawing \\(2\\) tickets from a set of tickets labeled \\(A\\) through \\(C\\).\nWe previously saw that if order does matter, there are 6 possible outcomes: \\[\\begin{align*}\n  \\Omega   = \\{ & (A, B), \\ (A, C) \\\\\n      & (B, A), \\ (B, C)  \\\\\n      & (C, A), \\ (C, B) \\}\n\\end{align*}\\]\nIf order doesn’t matter, we actually have fewer outcomes! Specifically:\n\n\\((A, C)\\) and \\((C, A)\\) become equivalent\n\\((A, B)\\) and \\((B, A)\\) become equivalent\n\\((B, C)\\) and \\((C, B)\\) become equivalent\n\nSo, \\(\\Omega\\) becomes \\(\\{(A, B), \\ (A, C), \\ (B, C)\\}\\), so we have only 3 elements in \\(\\Omega\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#dont-worry",
    "href": "Pages/Lectures/Lecture04/Lec04.html#dont-worry",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Don’t Worry!",
    "text": "Don’t Worry!\n\nI know that was a lot of math, pretty quickly.\nDon’t worry! We will be returning to a few of these concepts over the coming weeks.\nMy main goal for today’s lecture was to give you an overview of the types of arguments we make when dealing with counting problems, as well as to give you some tools to answer counting problems."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#an-exercise",
    "href": "Pages/Lectures/Lecture04/Lec04.html#an-exercise",
    "title": "PSTAT 5A: Lecture 04",
    "section": "An Exercise",
    "text": "An Exercise\n\n\n\n\n\n\nExercise 1\n\n\n\n\nCalifornia state license plates consist of 7 characters: a digit, followed by 3 letters, followed by 3 digits.\n\nSuppose we do not allow repeated letters or digits in a license plate: i.e. A123BCD456 is a valid plate whereas A122BCC345 is not. How many license plates can be created using this scheme?\nRealistically, license plates are allowed to contain repeated letters or digits. Re-answer the question of how many license plates can be created using this scheme.\nUsing the scheme outlined in part (b), what is the probability of picking a random license plate and having it be B131GHA?"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#lecture-summary-1",
    "href": "Pages/Lectures/Lecture04/Lec04.html#lecture-summary-1",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Lecture Summary",
    "text": "Lecture Summary\n\nThe main topic of today’s lecture was counting, which refers to the tools we use to systematically count the elements in a set without having to list out all of the elements contained in it.\nWe talked briefly about what it means for order to matter (or, consequently, not matter).\n\nThis lead us to the notations \\(n!\\), \\((n)_k\\), and \\(\\binom{n}{k}\\).\n\nNext time, we’ll see how we can use new information to update our beliefs on certain events by way of what are known as conditional probabilities.\n\nWe’ll also be able to work through a few more interesting examples."
  },
  {
    "objectID": "Pages/hw.html",
    "href": "Pages/hw.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Here you will find all homework assignments and their solutions. Please remember that solutions to homework will only be released during Exam Weeks; we encourage you to come to Office Hours to ask any questions you may have about the assignments!\n\nHomework 1: Blank",
    "crumbs": [
      "Homework",
      "HW and Solns"
    ]
  },
  {
    "objectID": "Pages/exam_prep.html",
    "href": "Pages/exam_prep.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Information Document: .pdf\nList of Topics (not comprehensive): .pdf\nSpring 2023 Exams:\n\nBlue Version: Blank Solns\n\nTypo in Solutions: In 12(b), the \\(\\frac{1}{5 - 1}\\) should be \\(\\frac{1}{4 - 1}\\)\nTypo in Solutions: Answer to 13(d) should be \\(A = \\{ (1, -1, \\ \\ 1), \\ (1, -1, -1)\\}\\)\n\nYellow Version: Blank Solns\n\nTypo in Solutions: In 12(b), the \\(\\frac{1}{5 - 1}\\) should be \\(\\frac{1}{4 - 1}\\)\nTypo in Solutions: Answer to 14(d) should be \\(A = \\{ (1, -1, \\ \\ 1), \\ (1, -1, -1)\\}\\)\n\nPlease note that the format of exams this quarter will be slightly different; see the coverpages below\n\nExam Coverpages: Multiple Choice Free Response\nFormula Sheet: .pdf\nPractice Problems: .pdf Solns",
    "crumbs": [
      "Exam Prep",
      "Exam Prep"
    ]
  },
  {
    "objectID": "Pages/exam_prep.html#midterm-1",
    "href": "Pages/exam_prep.html#midterm-1",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Information Document: .pdf\nList of Topics (not comprehensive): .pdf\nSpring 2023 Exams:\n\nBlue Version: Blank Solns\n\nTypo in Solutions: In 12(b), the \\(\\frac{1}{5 - 1}\\) should be \\(\\frac{1}{4 - 1}\\)\nTypo in Solutions: Answer to 13(d) should be \\(A = \\{ (1, -1, \\ \\ 1), \\ (1, -1, -1)\\}\\)\n\nYellow Version: Blank Solns\n\nTypo in Solutions: In 12(b), the \\(\\frac{1}{5 - 1}\\) should be \\(\\frac{1}{4 - 1}\\)\nTypo in Solutions: Answer to 14(d) should be \\(A = \\{ (1, -1, \\ \\ 1), \\ (1, -1, -1)\\}\\)\n\nPlease note that the format of exams this quarter will be slightly different; see the coverpages below\n\nExam Coverpages: Multiple Choice Free Response\nFormula Sheet: .pdf\nPractice Problems: .pdf Solns",
    "crumbs": [
      "Exam Prep",
      "Exam Prep"
    ]
  },
  {
    "objectID": "Pages/exam_prep.html#midterm-2",
    "href": "Pages/exam_prep.html#midterm-2",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Midterm 2",
    "text": "Midterm 2\n\nInformation Document: .pdf\nList of Topics (not comprehensive): .pdf\nSpring 2023 Exams\n\nSalmon Version: Blank Solns\nYellow Version: Blank Solns\n\nExam Coverpages: Multiple Choice Free Response\nFormula Sheet: .pdf\nPractice Problems: .pdf Solns",
    "crumbs": [
      "Exam Prep",
      "Exam Prep"
    ]
  },
  {
    "objectID": "Pages/exam_prep.html#final-exam",
    "href": "Pages/exam_prep.html#final-exam",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Final Exam",
    "text": "Final Exam\n\nInformation Document: .pdf\nList of Topics (not comprehensive): .pdf\nSpring 2023 Exams:\n\nFree Response, Version A: Blank\nMultiple Choice, Version A: Blank\nPlease note: There were a few versions last quarter; I’ve only posted one version above. Additionally, please note that full solutions for last quarter’s exam will not be posted.\nFree Response Final Answers: .pdf\n\nPlease note: This was updated at 6pm on Thursday August 3.\n\n\nFormula Sheet: .pdf\n\nPlease note: This was updated at 6pm on Thursday August 3.\n\nPractice Problems: .pdf (for additional practice problems, including multiple choice ones, please consult the Spring 2023 final exam above.) Solns",
    "crumbs": [
      "Exam Prep",
      "Exam Prep"
    ]
  },
  {
    "objectID": "Pages/Labs/Lab06/lab06.html",
    "href": "Pages/Labs/Lab06/lab06.html",
    "title": "Lab06",
    "section": "",
    "text": "Welcome to the final PSTAT 5A Lab of the quarter! In this lab, we will put everything we learned together and work on a mini-project of sorts. In addition to being a way for you to review the material in the course thus far, I hope this also provides you with a fun opportunity to start engaging in real-world data science applications!"
  },
  {
    "objectID": "Pages/Labs/Lab06/lab06.html#what-to-submit",
    "href": "Pages/Labs/Lab06/lab06.html#what-to-submit",
    "title": "Lab06",
    "section": "What To Submit",
    "text": "What To Submit\nAt the end of this lab, you should have a well-formatted lab report containing selected figures and code. As a general practice, though, most Data Scientists perefer not to include the entirety of their code in their reports, opting instead to only summarize the code they used and then provide the full code in an appendix. We will not ask you to do this for this report; you may leave your full code in your report.\n\nPlease pay attention to the formatting of your document. Make sure your equations render properly, and make sure any hyperlinks you include function properly. Also ensure you update your Notebook Metadata to include your name on your .pdf; failure to do so will incur a 50% penalty.\n\nMake sure all plots you generate are properly labelled and include titles.\n\n\n\n\n\n\nCaution\n\n\n\nHere is how this lab will be graded:\n\nYou will get 0.5pts of the total points for submitting both a .pdf and a .ipnyb file.\nYour TA will then examine two of your plots (exactly which plots will not be revealed to you until after grading is done) and award you 0.15pts for each correct plot.\nYour TA will then examine one of your statistical analyses (exactly which analyses will not be revealed to you until after grading is done) and award you 0.2pts for each correct analysis."
  },
  {
    "objectID": "Pages/Labs/Lab06/lab06.html#getting-to-know-the-dataset",
    "href": "Pages/Labs/Lab06/lab06.html#getting-to-know-the-dataset",
    "title": "Lab06",
    "section": "Getting To Know the Dataset",
    "text": "Getting To Know the Dataset\nThe Internet Movie Database (IMdB) very graciously provides a quite comprehensive suite of media-related datasets, available to download at this link. The dataset we will work with in this lab has been truncated to only include movies released in the 2000s, and has the following data dictionary (with entried copied from the official IMdB documentation):\n\ntconst (string) - alphanumeric unique identifier of the title\ntitleType (string) – the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc)\nprimaryTitle (string) – the more popular title / the title used by the filmmakers on promotional materials at the point of release\noriginalTitle (string) - original title, in the original language\nisAdult (boolean) - 0: non-adult title; 1: adult title\nstartYear (YYYY) – represents the release year of a title. In the case of TV Series, it is the series start year\nendYear (YYYY) – TV Series end year. ‘’ for all other title types\nruntimeMinutes – primary runtime of the title, in minutes\ngenres (string array) – includes up to three genres associated with the title genres (string array) – includes up to three genres associated with the title\naverageRating – weighted average of all the individual user ratings\nnumVotes - number of votes the title has received\n\nThe dataset itself can be found at this link: https://pstat5a.github.io/Files/Datasets/movies_2000s.csv\n\n\n\n\n\n\nAction Item\n\n\n\nImport any necessary modules, using nicknames if you so choose. (You may need to continually modify this code cell as you progress through the lab, if you encounter the need for a module you haven’t imported.)\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is considered good practice (by some Data Scientists) to load all modules at the start of a report.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nImport the data as a datascience table, and call the table movies."
  },
  {
    "objectID": "Pages/Labs/Lab06/lab06.html#exploratory-data-analysis",
    "href": "Pages/Labs/Lab06/lab06.html#exploratory-data-analysis",
    "title": "Lab06",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nMost data science projects begin with what is known as exploratory data analysis. John Tukey, one of the pioneers of modern statistics and also one of the first people to introduce the notion of EDA (as it is commonly abbreviate) describes it as\n\n“[…] actively incisive rather than passively descriptive, with a real emphasis on the discovery of the unexpected - if necessary by figuratively knocking the analyst’s head against the wall until he notices it.”\n\nPerhaps more concretely, EDA is designed to summarize the main characteristics of a dataset. Crucially, as Tukey indicates, this does not just mean generating plots and numerical summaries (though these are often integral parts of EDA), but also interpreting these plots to draw conclusions about the data. Tukey goes as far as to insinuate that, when good EDA is performed, statistical modeling can be thought of as merely “confirmatory” in the sense that it only confirms what EDA originally revealed! (see Exploratory Data Analysis, by John Tukey; ISBN 0-201-07616-0)\n\nAs such, let us start our project with EDA.\n\n\n\n\n\n\nAction Item\n\n\n\nAdd a first-level header to your document titled “Exploratory Data Analysis”\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nUse code to figure out how many movies are included in the dataset. Below your code cell, add a Markdown cell in which you explicitly state “There were ____ movies represented in the dataset”, where you replace the “____” with the number of movies.\n\n\n\nTimes\nLet’s start by getting a feel for the years that are represented in our dataset.\n\n\n\n\n\n\nAction Item\n\n\n\nGenerate a histogram of the years (more specifically, the startYear variable) included in the dataset. Use this to discuss which year or years seem to be the most represented in the dataset.\n\n\n\n\n\n\n\n\nTip\n\n\n\nEvery plot in a report should contain some discussion below it; no plot should simply be displayed and left alone! For this report, use Markdown cells to format your discussions.\n\n\n\n\nTimes\nLet’s start by getting a feel for the years that are represented in our dataset.\n\n\n\n\n\n\nAction Item\n\n\n\nGenerate a histogram of the years (more specifically, the startYear variable) included in the dataset. Use this to discuss which year or years seem to be the most represented in the dataset.\n\n\n\n\n\n\n\n\nTip\n\n\n\nEvery plot in a report should contain some discussion below it; no plot should simply be displayed and left alone! For this report, use Markdown cells to format your discussions.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nAre any of the startYear values missing (i.e. are any of the values nan)? Write a line of code to answer this; as a hint, consider how you can combine the sum() and numpy.isnan() functions."
  },
  {
    "objectID": "Pages/Labs/Lab06/lab06.html#average-rating",
    "href": "Pages/Labs/Lab06/lab06.html#average-rating",
    "title": "Lab06",
    "section": "Average Rating",
    "text": "Average Rating\nLet’s also take a look at the average ratings of movies included in the dataset.\n\n\n\n\n\n\nAction Item\n\n\n\nGenerate a histogram of the average ratings included in the dataset, and use this to discuss the distribution of ratings. Some things to include in your discussion:\n\nIs the distribution of ratings symmetric?\nDoes there appear to be a tendency to give higher or lower average ratings?\n\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nCompute the mean and median of the average ratings.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nWhat proportion of movies received an average rating higher than 8.0?\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nWhat average rating is at the first quartile of all average ratings? (Hint: take a look at Lab02 and see what functions we used to compute percentiles.)\n\n\n\nRuntimes\nWe should also take a look at the runtimes of the movies included in the dataset.\n\n\n\n\n\n\nAction Item\n\n\n\nGenerate a histogram of the runtimes (in minutes) of the movies included in the dataset.\n\n\nHm…. that’s a peculiar-looking histogram! Let’s see if we can get to the bottom of this.\n\n\n\n\n\n\nAction Item\n\n\n\nWhat is the longest runtime included in the dataset? What is the title of the movie with this runtime?\n\n\nIndeed, you are reading that correct- there is actually a movie that has a runtime of nearly 100 hours! (If you’re curious- here is its IMdB page.)\nNow that’s going to throw off a lot of our observations. Typically, it’s not considered a good idea to simply remove data from our dataset, but since this is an introductory course and we want to make our lives simpler let’s just go ahead and do some trimming.\n\n\n\n\n\n\nAction Item\n\n\n\nRemove all rows from the movies table corresponding to movies with runtimes longer than 300 minutes. Some hints:\n\nIf k represents the number of rows in the movies table, what does numpy.arange(k)[movies.column(\"runtimeMinutes\") &gt; 300] give? What about list(numpy.arange(k)[movies.column(\"runtimeMinutes\") &gt; 300])?\nTake a look at how the .remove() method works by consulting its help file on this page: http://www.data8.org/datascience/tables.html\n\nAfter removing these rows, check that the maximum runtime is now 300 minutes.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nDepending on how you structure your code, you may encounter an error when running your code more than once. A simple way to avoid this error is to recompute the number of rows in the dataset at the start of this cell."
  },
  {
    "objectID": "Pages/Labs/Lab06/lab06.html#comparisons-across-years",
    "href": "Pages/Labs/Lab06/lab06.html#comparisons-across-years",
    "title": "Lab06",
    "section": "Comparisons across Years",
    "text": "Comparisons across Years\n\n\n\n\n\n\nAction Item\n\n\n\nGenerate a plot that displays how the average averageRating changes over years. That is, on the x-axis include the years 2000 through 2023 and for each year have point whose y-value is equal to the average of averageRating values for that corresponding year. Hint: Take a look at how we did a similar problem in last week’s lab.\nUse this to comment on whether or not you think the average average rating changed over the years.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nGenerate a plot that displays the total number of films released in each year. That is, on the x-axis include the years 2000 through 2023 and for each year have point whose y-value is equal to the number of films released in that particular year.\nUse this plot to discuss any trends you see in the relationship between year of release and the number of movies released.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nGenerate a plot that displays how the average runtime of movies changes over years. That is, on the x-axis include the years 2000 through 2023 and for each year have point whose y-value is equal to the average of runtimes of movies that were released in that year. Hint: This will be very similar to your work from the previous action item above. Additionally, you may want to make use of the numpy.nanmean() function as opposed to the standard numpy.mean() function as numpy.nanmean() computes the mean of a list of numbers after ignoring missing values.\nUse this to comment on whether or not you think the average movie runtime has changed over the years."
  },
  {
    "objectID": "Pages/Labs/Lab06/lab06.html#statistical-tests",
    "href": "Pages/Labs/Lab06/lab06.html#statistical-tests",
    "title": "Lab06",
    "section": "Statistical Tests",
    "text": "Statistical Tests\nBased on the plot generated from our section on EDA, it seems that there may have been some changes in the average runtime of films across the years.\n\nLet’s focus on the following claim:\n\nThe average runtime of a film in 2001 is the same as the average runtime of a film in 2012.\n\nThis sounds an awful like a hypothesis test… meaning we should be able to statistically test its validity!\n\nSpecifically, let us test the above claim against the one-sided alternative that the average runtime of a film in 2012 is smaller than the average runtime of a film in 2001. Let us also use a 5% level of significance wherever necessary.\n\n\n\n\n\n\nAction Item\n\n\n\nAdd a first-level header to your document titled “Statistical Tests”\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nIn a markdown cell, define the parameters \\(\\mu_1\\) and \\(\\mu_2\\). Also state the null and alternative hypotheses. Make sure your equations render properly, both in your notebook file but also your .pdf!\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nCompute the observed value of the test statistic. (Hint: np.nanstd() also exists, in addition to np.std().)\nBy the way, you can ignore the fact that when we remove NA values the sample sizes change; it turns out that there weren’t too many missing values for that to be a problem.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nIdentify the distribution that the test statistic follows under the null.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nCalculate the \\(p-\\)value of the observed test statistic, and use this to conduct the test. Phrase your conclusions in the context of the problem, using a Markdown cell."
  },
  {
    "objectID": "Pages/Labs/Lab06/lab06.html#a-regression",
    "href": "Pages/Labs/Lab06/lab06.html#a-regression",
    "title": "Lab06",
    "section": "A Regression",
    "text": "A Regression\nIt is plausible that perhaps titles with more votes have higher scores. Let’s see if the data supports this.\n\n\n\n\n\n\nAction Item\n\n\n\nGenerate a scatterplot with the number of votes on the x-axis and the average rating on the y-axis. Comment on whether there appears to be an association between these two variables and, if applicable, what type of association there appears to be (e.g. non/linear, positive/negative).\n\n\nRegardless of the results of your previous plot, let’s go ahead and run a regression using “average rating” as the response variable and “number of votes” as the explanatory variable.\n\n\n\n\n\n\nAction Item\n\n\n\nIn a Markdown cell, write down the model assuming a linear form for the signal function.\n\n\nThe specific function we will use to perform the regression is called .linregress() from the scipy.stats module. You can read more about the function and its documentation here.\n\n\n\n\n\n\nAction Item\n\n\n\nRun a regression of “average rating” on “number of votes”. Report the estimated slope and intercept, as well as the \\(p-\\)value associated with testing \\(H_0: \\ \\beta_1 = 0\\) against \\(H_A: \\ \\beta_1 \\neq 0\\). Also re-generate a scatterplot of the original data with the OLS line superimposed on top (the aforementioned help file will be of IMMENSE help for this).\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nInclude a QQ-plot of both the explanatory and response variables, and comment."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#data-classes",
    "href": "Pages/Labs/Lab02/lab02.html#data-classes",
    "title": "Lab02",
    "section": "Data Classes",
    "text": "Data Classes\nLast week, we were introduced to the notion of data types. Recall that “data type” can be thought of as the category (or type) of data- i.e. integer, float, character, etc.\n\nIn Python, however, we often need to aggregate data into larger structures, often referred to as data classes.\n\nLists\nPerhaps the most fundamental data structure in Python is that of a list. Just like lists in real life or in mathematics, Python lists are just collections of items enclosed in square brackets:\n\n[&lt;item 1&gt;, &lt;item 2&gt;, ..., &lt;item n&gt;]\n\nAgain, the items in a list can be of any data type; we can even mix and match data types!\n\n\n\n\n\n\nTask 1\n\n\n\nCreate a list containing the elements 1, \"hi\", 3.4, and \"PSTAT 5A\". Assign this list to a variable called list1.\n\n\nJust as we were able to use a Python function (type()) to check the type of a particular piece of data, we can also use Python to check the structure or class of a piece of data. It turns out that we use the same function as before- namely, type()!\n\n\n\n\n\n\nTask 1 (cont’d)\n\n\n\nRun the code type(list1)."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#indexing",
    "href": "Pages/Labs/Lab02/lab02.html#indexing",
    "title": "Lab02",
    "section": "Indexing",
    "text": "Indexing\nAlright, now that we can store data in lists, how can we access elements in a list? The answer is to use what is known as indexing.\n\nGiven a list x, we access the ith element using the code\n\nx[i]\n\nThe reason we call this “indexing” is because the number that goes between the brackets is the index of the element that we want.\n\n\n\n\n\n\nCaution\n\n\n\nPython begins indexing at 0.\n\n\nWhat does this mean? Well, let’s see by way of an example.\n\n\n\n\n\n\nTask 2\n\n\n\n\nCreate a list with the numbers 1 through 10, inclusive, and assign this to a variable called x.\nRun the code x[1].\nRun the code x[0].\n\n\n\nSo, what we would colloquially call the first element of a list, Python calls the zeroeth element.\n\n\nAlright, let’s put together some of the concepts we just learned.\n\n\n\n\n\n\nTask 3\n\n\n\nCreate a list called x that contains the elements 1, \"two\", 3.5, \"four\", and \"five five\". Answer the following questions WITHOUT running any code, writing your answers as a comment in a code cell:\n\nWhat would be the output of type(x)?\nWhat would be the output of type(x[1])?\nWhat would be the output of x[0]?\n\nNow, run code to verify your answers to the above three questions."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#tables",
    "href": "Pages/Labs/Lab02/lab02.html#tables",
    "title": "Lab02",
    "section": "Tables",
    "text": "Tables\nAnother very useful data structure in Python is that of a table. Python tables behave pretty much the same as the tables we’ve used in, say, math- they are a grid of values arranged sequentially.\n\nTables can be created using the Table() function in Python, which itself comes from the datascience module. The general syntax of creating a table with the Table() function is:\n\nTable().with_columns(\n  \"&lt;col 1 name&gt;\", [&lt;col 1, val 1&gt;, &lt;col 1, val 2&gt;, ... ],\n  \"&lt;col 2 name&gt;\", [&lt;col 2, val 1&gt;, &lt;col 2, val 2&gt;, ... ],\n  ...\n)\n\nFor example,\n\nTable().with_columns(\n  \"Name\", [\"Ethan\", \"Morgan\", \"Amy\"],\n  \"ID\", [12345, 10394, 20343],\n  \"Office\", [\"South Hall\", \"South Hall\", \"North Hall\"]\n)\n\n\n\n\nName\nID\nOffice\n\n\n\n\nEthan\n12345\nSouth Hall\n\n\nMorgan\n10394\nSouth Hall\n\n\nAmy\n20343\nNorth Hall\n\n\n\n\n\nThere is nothing stopping us from assigning a table to a variable! For example, after running\n\ntable1 = Table().with_columns(\n  \"Name\", [\"Ethan\", \"Morgan\", \"Amy\"],\n  \"ID\", [12345, 10394, 20343],\n  \"Fav_Drink\", [\"Iced Tea\", \"Coffee\", \"Sprite\"]\n)\n\nthe variable table1 is equivalent to the table displayed above:\n\ntable1\n\n\n\n\nName\nID\nFav_Drink\n\n\n\n\nEthan\n12345\nIced Tea\n\n\nMorgan\n10394\nCoffee\n\n\nAmy\n20343\nSprite\n\n\n\n\n\n\n\n\n\n\n\nTerminology\n\n\n\nSometimes in Python we will encounter expressions of the form\n\n&lt;object type&gt;.&lt;function name&gt;()\n\nIn this syntax, the function &lt;function name&gt; is said to be a method. For example, the function with_columns() is a method for the Table object.\n\n\nThe datascience module contains a plethora of methods we can use to manage tables. For example, the select() method can be used to select columns by name:\n\ntable1.select(\"ID\")\n\n\n\n\nID\n\n\n\n\n12345\n\n\n10394\n\n\n20343\n\n\n\n\n\n\n\n\n\n\n\nSyntax\n\n\n\nMethods are always appended to either a function that creates a blank object type (like Table()) or a variable of the correct type.\n\n\n\n\n\n\n\n\nTask 4\n\n\n\nRead the list of methods for Table objects at http://data8.org/datascience/tables.html, and write down (in a code cell, using comments) at least three different methods, including a short description of what each method does. For example:\n\n# .with_columns(): adds specified columns to a table.\n\n\n\n\n\n\n\n\n\nTask 5\n\n\n\n\nCreate the following table, and assign it to a variable called profs:\n\n\n\n\n\n\nProfessor\nOffice\nCourse\n\n\n\n\nDr. Swenson\nSouth Hall\nPSTAT 130\n\n\nDr. Wainwright\nOld Gym\nPSTAT 120A\n\n\nDr. Mouti\nOld Gym\nPSTAT 126\n\n\n\n\n\nRun a cell containing only the code profs to make sure (visually) that your table looks correct.\n\nSelect the column called Course from profs.\nCreate a new table called profs_new that contains the same rows as the profs table, but with the following additional row:\n\n\n\n\n\n\nProfessor\nOffice\nCourse\n\n\n\n\nDr. Ravat\nSouth Hall\nPSTAT 120B\n\n\n\n\n\nRun a cell containing only the code profs_new to make sure (visually) that the appending was successful. Hint: think about how you can use our discussion on updating variable values from last lab. Also, the method .with_row() may be useful; see the help file at http://data8.org/datascience/tables.html for more information.\n\n\nSuppose we want to select rows of a table that satisfy a given condition. For example, if we wanted to find the information of only people who like Sprite in the table1 table above, we would call\n\ntable1.where(\"Fav_Drink\", \"Sprite\")\n\n\n\n\nName\nID\nFav_Drink\n\n\n\n\nAmy\n20343\nSprite\n\n\n\n\n\nWhat would happen if we tried to select the rows of table1 with Coke in the Fav_Drink column? Well, since there is nobody in table1 that has coke as their favorite drink, we should hope that Python returns an empty table.\n\ntable1.where(\"Fav_Drink\", \"Coke\")\n\n\n\n\nName\nID\nFav_Drink\n\n\n\n\n\n\n\nSure enough, Python has returned an empty table!"
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#arrays",
    "href": "Pages/Labs/Lab02/lab02.html#arrays",
    "title": "Lab02",
    "section": "Arrays",
    "text": "Arrays\nThe final Data Structure we will examine in this class is that of an array. Arrays behave very similarly to Tables, with a few differences. For one, the syntax used to create an array is slightly different:\n\nmake_array(&lt;item 1&gt;, &lt;item 2&gt;, &lt;item 3&gt;, ...)\n\nFor example,\n\nmake_array(\"Spring\", \"Summer\", \"Autumn\", \"Winter\")\n\narray(['Spring', 'Summer', 'Autumn', 'Winter'],\n      dtype='&lt;U6')\n\n\nYou may ask- what’s that dtype='&lt;U6' symbol at the end of the output? For now, don’t worry about it, as we will revisit this later."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#lists-vs.-arrays",
    "href": "Pages/Labs/Lab02/lab02.html#lists-vs.-arrays",
    "title": "Lab02",
    "section": "Lists vs. Arrays",
    "text": "Lists vs. Arrays\nSo, we now know about three different data classes in Python: lists, tables, and arrays. At first glance, lists and arrays may seem somewhat similar. However, there are a few key differences between them:\n\n\n\n\n\n\nTask 6\n\n\n\nMake a list called my_list containing the elements 1, 2, and 3, and make an array called my_array also containing the elements 1, 2, and 3. Run the following commands in separate code cells:\n\nsum(my_list)\nsum(my_array)\nmy_list + 2\nmy_array + 2\n\n\n\nWhat the previous Task illustrates is the fact that arrays lend themselves to element-wise operations, whereas lists do not. One important limitation about arrays, though, is that the elements in an array must all be of the same data type. If you try to make an array consisting of elements that are different data types Python will still run, however it will not run in the way you expect it to!"
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#comparisons",
    "href": "Pages/Labs/Lab02/lab02.html#comparisons",
    "title": "Lab02",
    "section": "Comparisons",
    "text": "Comparisons\nHere’s a question: is 2 less than 3? Well, yes it is! If we wanted to confirm this, we could simply ask Python whether 2 is less than 3 by running\n\n2 &lt; 3\n\nTrue\n\n\nNotice, however, how Python answered this question: it simply returned True. Let’s see what the data type of True is:\n\ntype(True)\n\nbool\n\n\nTrue is of the type bool, which is short for boolean. There are only two boolean quantities in Python: True and False. Let’s see how we can generate a False value:\n\n3 &lt; 2\n\nFalse\n\n\nHere is a list of comparison operators, taken from the Inferential Thinking textbook:\n\n\n\nComparison\nOperator\nTrue Example\nFalse Example\n\n\n\n\nLess than\n&lt;\n2 &lt; 3\n2 &lt; 2\n\n\nGreater than\n&gt;\n3 &lt; 2\n3 &gt; 3\n\n\nLess than or equal\n&lt;=\n2 &lt;= 2\n3 &lt;= 2\n\n\nGreater than or equal\n&gt;=\n3 &gt;= 3\n2 &gt;= 3\n\n\nEqual\n==\n3 == 3\n3 == 2\n\n\nNot equal\n!=\n3 != 2\n2 != 2\n\n\n\nOne nice thing about Python is that it allows for multiple simultaneous comparisons. For example,\n\n2 &lt; 3 &lt; 4\n\nTrue\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn a multiple comparison, Python will only return True when all of the included comparisons are true.\n\n\nFor instance, 2 &lt; 3 &lt; 1 would return False, because even though 2 is less than 3 it is not true that 3 is less than 1.\n\nBelieve it or not, you can compare strings as well! Python compares strings alphabetically; that is, letters at the beginning of the alphabet are considered to have smaller ordinal value than letters at the end of the alphabet. For example:\n\n\"apple\" &lt; \"banana\"\n\nTrue\n\n\n\n\"zebra\" &lt; \"zanzibar\"\n\nFalse\n\n\n\n\"cat\" &lt;= \"catenary\"\n\nTrue\n\n\n\n\n\n\n\n\nTask 7\n\n\n\nCheck how \"statistics\" and \"Statistics\" (note the capitalization!) compare. Use this to answer the question: when Python is comparing strings, does it give precedence to capital letters or not? If so, which (lowercase or capital) is given a “higher” value?\n\n\nFinally, we discuss how comparisons work in the context of lists and arrays. The way Python compares lists is by what is known as lexicographical order. From the official Python help documentation, this means\n\nfirst the first two items are compared, and if they differ this determines the outcome of the comparison; if they are equal, the next two items are compared, and so on, until either sequence is exhausted.\n\nFor instance, [1, 2, 3] &lt; [2, 1, 1] would return True since 1 (the first element of the first list) is less than 2 (the first element of the second list).\n\nThe comparison of arrays is a little more straightforward, except\n\n\n\n\n\n\nImportant\n\n\n\nWhen comparing two arrays, the arrays must be of the same length.\n\n\nTo see exactly how comparison of arrays works, let’s work through a Task:\n\n\n\n\n\n\nTask 8\n\n\n\nMake an array with the elements 1, 2, and 3, and call this x. Make another array with the elements 2, 3, 1, and call this y. Run x &lt; y, and comment on the result.\n\n\nWhat the previous task illustrates is that Python compares arrays element-wise."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#conditionals",
    "href": "Pages/Labs/Lab02/lab02.html#conditionals",
    "title": "Lab02",
    "section": "Conditionals",
    "text": "Conditionals\nNow, we can use comparisons for much more than verifying simple arithmetic relationships. One of the main areas in which comparisons arise is the area of conditional expressions.\n\nSimply put, conditional expressions are how we can convey a set of choices to Python. As an example, let’s consider finding someone’s city based on their zip code. To simplify, let’s assume the only zip codes we consider are 9311, 93120, and 93150. From postal data, we know that:\n\na zip code of 93117 corresponds to Goleta\na zip code of 93120 corresponds to Santa Barbara\na zip code of 93150 corresponds to Montecito\n\nWe can rephrase this information in terms of “if” statements:\n\nIf a person has a zip code of 93117, then they are in Goleta\nOtherwise, if they have a zip code of 93120, then they are in Santa Barbara\nOtherwise, if they have a zip code of 93150, then they are in Montecito\n\nThis is precisely the syntax we would use when translating this experiment into Python syntax:\n\nif zip_code == 93117:\n  location = \"Goleta\"\nelif zip_code == 93120:\n  location = \"Santa Barbara\"\nelif zip_code == 93150:\n  location = \"Montecito\"\n\nBy the way: elif is an abbreviation for else if, which itself can be thought of as equivalent to otherwise, if.\n\nHere’s the general syntax of a conditional expression in Python:\n\nif &lt;condition 1&gt;:\n  &lt;task 1&gt;\nelif &lt;condition 2&gt;:\n  &lt;task 2&gt;\n...\nelse:\n  &lt;final task&gt;\n\nWhen executing the above conditional statement, Python first checks whether &lt;condition 1&gt; returns a value of True or False. If it returns a value of True, then &lt;task 1&gt; is executed and the statement ends. Otherwise, Python checks whether &lt;task 2&gt; is True or False; if it is True then &lt;condition 2&gt; is executed, etc.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the example code above: if &lt;condition 1&gt; is True, then no tasks beyond &lt;task 1&gt; are evaluated. If &lt;condition 2&gt; is True, then no tasks beyond &lt;task 2&gt; are evaluated. And so on and so forth.\n\n\n\n\n\n\n\n\nTask 9\n\n\n\nConsider the code:\n\nx = 2\n\nif x &lt; 2:\n    x = \"hello\"\nelif x &lt; 3:\n    x = \"goodbye\"\nelse:\n    x = \"take care\"\n\nBefore running any code, write down what you think the result of executing x would be. Then, run the loop, execute x, and check whether your answer was correct or not.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIndentation is very important in Python.\n\n\nFor example, if instead of the conditional expression in Task 2 we had instead put\n\nx = 2\n\nif x &lt; 2:\nx = \"hello\"\nelif x &lt; 3:\nx = \"goodbye\"\nelse:\nx = \"take care\"\n\nthen we would have received an error!"
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#functions",
    "href": "Pages/Labs/Lab02/lab02.html#functions",
    "title": "Lab02",
    "section": "Functions",
    "text": "Functions\nFinally, let’s quickly discuss Python functions. We’ve already been using quite a few functions:\n\n\n\n\n\n\nTask 10\n\n\n\nIn a Markdown cell, write down three functions we’ve used in Lab thus far.\n\n\nIf you recall, the general syntax for calling a function is:\n\n&lt;function name&gt;(&lt;arg1&gt;, &lt;arg2&gt;, ... )\n\nwhere &lt;function name&gt; denotes the function name and &lt;arg1&gt;, &lt;arg2&gt;, etc. denote the arguments of the function.\n\nCreating your own function in Python is actually fairly simple! Here is the syntax we use:\n\ndef &lt;function name&gt;(&lt;list out the argument names&gt;):\n  \"\"\"include a 'docstring' here\"\"\"\n  &lt;body of the function&gt;\n  return &lt;what you want the function to output&gt;\n\nFor example,\n\ndef f(x, y):\n  \"\"\"returns x^2 + y^2\"\"\"\n  return x**2 + y**2\n\ncreates a function f that can be called on two arguments, x and y, and returns the sum of squares of the arguments; e.g.\n\nf(3, 4)  # should return 3^2 + 4^2 = 25\n\n25\n\n\nBy the way, the docstring referenced above is a verbal description of what the function does. (Recall from Lab01 that it is just a multi-line comment, since it is enclosed in triple quotation marks!). All functions should include a docstring to convey to the user what the function does.\n\n\n\n\n\n\nImportant\n\n\n\nIf you don’t include a return statement in the definition of a function, then your function will never return anything.\n\n\nFor instance,\n\ndef g(x, y):\n  \"\"\"should return x^2 + y^2\"\"\"\n  x**2 + y**2\n\ng(3, 4)\n\n\n\n\n\n\n\nTask 11\n\n\n\nWrite a function called cent_to_far() which takes in a single temperature c as measured in degrees Centigrade and returns the corresponding temperature in degrees Farenheit. Check that cent_to_far(0) correctly returns 0 and cent_to_far(68) correctly returns 69.7777. As a reminder: \\[ {}^{\\circ}\\mathrm{F} = \\frac{5}{9} ({}^{\\circ}\\mathrm{C}) + 32 \\]\n\n\nFinally, let’s combine some things by way of a concluding Task:\n\n\n\n\n\n\nTask 12\n\n\n\nWrite a function called parity() that returns the parity (i.e. whether a number is even or odd) of an input x. Call your parity() function on 2 and then 3 to make sure your function behaves as expected. Some hints:\n\nRecall that % is the modulus operator in Python. Specifically, x % y returns the remainder of performing y divided by x.\nRecall that even numbers are divisible by 2 (so what does this mean about the remainder of dividing x by 2 if x is even?)"
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html",
    "href": "Pages/Labs/Lab04/lab04.html",
    "title": "Lab04",
    "section": "",
    "text": "If you recall, one of the first things we did in Lab (back in Week 1!) was to use Python as a calculator. At the time, we only used Python to compute relatively simple quantities. Now that we’ve talked a bit about distributions, you can see how Python might be able to simpliy our lives greatly!\nFor instance, take the probability mass function (p.m.f.) of the \\(\\mathrm{Bin}(n, p)\\) distribution: if \\(X \\sim \\mathrm{Bin}(n, p)\\), then \\[ \\mathbb{P}(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1 - p)^{n - k} \\] Can we get Python to compute this for us? Or, remember how when we want to find areas under a normal density curve we have to use tables- can we perhaps compute these areas using Python?\nThe answer to both of these questions is, naturally, “yes”! Specifically, we will make use of the scipy.stats module which contains a plethora of functions relating to the distributions we learned in this class (as well as other distributions we won’t have time to cover).\n\nimport scipy.stats as sps\n\nLet’s tackle the Binomial distribution first. The function sps.binom.pmf() allows us to compute the p.m.f. of the Binomial distribution (with specified parameters) at a particular point.\n\n\n\n\n\n\nTask 1\n\n\n\nLet \\(X \\sim \\mathrm{Bin}(143, 0.153)\\). Compute the following using the sps.binom.pmf() function:\n\n\\(\\mathbb{P}(X = 20)\\)\n\\(\\mathbb{P}(X = 40)\\) [make sure you understand the output of this; feel free to ask your TA if you are confused!]\n\n\n\nNow, let’s talk about areas under the normal curve. If we want to find the following area:\n\nwe would run the following code:\n\nsps.norm.cdf(t, mu, sigma)\n\n\n\n\n\n\n\nTask 2\n\n\n\n\nIf \\(X \\sim \\mathcal{N}(3, 0.5)\\), compute \\(\\mathbb{P}(X \\leq 2)\\).\nIf \\(X \\sim \\mathcal{N}(-2, \\ 1)\\), compute \\(\\mathbb{P}(X \\geq 1)\\).\nIf \\(X \\sim \\mathcal{N}(0, 1)\\), compute \\(\\mathbb{P}(-1 \\leq X \\leq 1)\\).\n\n\n\n\n\n\n\n\n\nTask 3\n\n\n\n\nLook up how to compute the c.d.f. of the Uniform distribution.\nIf \\(X \\sim \\mathrm{Unif}(-1, \\ 1)\\), use Python to compute \\(\\mathbb{P}(X \\leq 0.1532)\\)."
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html#distributions-in-python",
    "href": "Pages/Labs/Lab04/lab04.html#distributions-in-python",
    "title": "Lab04",
    "section": "",
    "text": "If you recall, one of the first things we did in Lab (back in Week 1!) was to use Python as a calculator. At the time, we only used Python to compute relatively simple quantities. Now that we’ve talked a bit about distributions, you can see how Python might be able to simpliy our lives greatly!\nFor instance, take the probability mass function (p.m.f.) of the \\(\\mathrm{Bin}(n, p)\\) distribution: if \\(X \\sim \\mathrm{Bin}(n, p)\\), then \\[ \\mathbb{P}(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1 - p)^{n - k} \\] Can we get Python to compute this for us? Or, remember how when we want to find areas under a normal density curve we have to use tables- can we perhaps compute these areas using Python?\nThe answer to both of these questions is, naturally, “yes”! Specifically, we will make use of the scipy.stats module which contains a plethora of functions relating to the distributions we learned in this class (as well as other distributions we won’t have time to cover).\n\nimport scipy.stats as sps\n\nLet’s tackle the Binomial distribution first. The function sps.binom.pmf() allows us to compute the p.m.f. of the Binomial distribution (with specified parameters) at a particular point.\n\n\n\n\n\n\nTask 1\n\n\n\nLet \\(X \\sim \\mathrm{Bin}(143, 0.153)\\). Compute the following using the sps.binom.pmf() function:\n\n\\(\\mathbb{P}(X = 20)\\)\n\\(\\mathbb{P}(X = 40)\\) [make sure you understand the output of this; feel free to ask your TA if you are confused!]\n\n\n\nNow, let’s talk about areas under the normal curve. If we want to find the following area:\n\nwe would run the following code:\n\nsps.norm.cdf(t, mu, sigma)\n\n\n\n\n\n\n\nTask 2\n\n\n\n\nIf \\(X \\sim \\mathcal{N}(3, 0.5)\\), compute \\(\\mathbb{P}(X \\leq 2)\\).\nIf \\(X \\sim \\mathcal{N}(-2, \\ 1)\\), compute \\(\\mathbb{P}(X \\geq 1)\\).\nIf \\(X \\sim \\mathcal{N}(0, 1)\\), compute \\(\\mathbb{P}(-1 \\leq X \\leq 1)\\).\n\n\n\n\n\n\n\n\n\nTask 3\n\n\n\n\nLook up how to compute the c.d.f. of the Uniform distribution.\nIf \\(X \\sim \\mathrm{Unif}(-1, \\ 1)\\), use Python to compute \\(\\mathbb{P}(X \\leq 0.1532)\\)."
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html#percentiles",
    "href": "Pages/Labs/Lab04/lab04.html#percentiles",
    "title": "Lab04",
    "section": "Percentiles",
    "text": "Percentiles\nAs we have seen in lecture, confidence intervals for a population parameter \\(\\theta\\) take the form \\[ \\widehat{\\theta} \\pm c \\cdot \\mathrm{s.e.} \\] where \\(\\mathrm{s.e.}\\) denotes the standard error (i.e. standard deviation) of the point estimator \\(\\widehat{\\Theta}\\), and \\(c\\) is an appropriately-selected percentile from the distribution of \\(\\widehat{\\Theta}\\).\nUp until now, we have primarily been finding the constant \\(c\\) using the various tables at our disposal. Though being able to read these tables is a useful skill (and a skill that is potentially testable on quizzes and exams…), using computers to compute these percentiles can greatly increase efficiency.\nThe syntax\n\nscipy.stats.norm.ppf(p, m, s)\n\ncomputes the pth percentile of the \\(\\mathcal{N}\\)(m, s) distribution. We will revisit this fairly soon, once we are exposed to another continuous distribution in a few lectures. There are analagous functions that allow us to compute percentiles of other distributions; for example, scipy.stats.t.ppf() can be used to find the percentiles of the \\(t\\) distribution.\n\n\n\n\n\n\nTask 4\n\n\n\n\nUse Python to find the confidence coefficient of a 95% confidence interval for a population proportion.\nUse Python to find the confidence coefficient of an 82% confidence interval for a population proportion."
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html#simulation",
    "href": "Pages/Labs/Lab04/lab04.html#simulation",
    "title": "Lab04",
    "section": "Simulation",
    "text": "Simulation\nNow, let’s tie things together slightly. As data scientists, we obviously love to use data! However, sometimes data can be too time-consuming, costly, or otherwise unfeasible to collect in large quantities. In certain situations, simulations can help address these issues.\nWhen asked to define a “simulation” in the context of data science, ChatGPT returned the following:\n\n[…] a simulation is a computational model or program that is used to replicate real-world scenarios or systems in order to analyze their behavior, predict outcomes, or test hypotheses.\n\nThis is actually a great definition: simulations are designed to simulate (i.e. mimic) real-world situations to generate new observations/outcomes that (we hope) closely resemble the real-world outcomes.\nFor example, suppose we believe that weights of rats in a particular situation are normally distributed with mean 3.8oz and a standard deviation of 0.5oz. Instead of actually going out and collecting the weights of, say, 10 different rats and recording them, we could simulate collecting these weights by generating a series of random numbers that follow the \\(\\mathcal{N}(3.8, \\ 0.5)\\) distribution:\n\n\narray([3.2571847 , 4.29867272, 3.94148925, 3.04685264, 3.51069987,\n       4.62571827, 2.58666038, 3.58554369, 4.43296813, 3.3666298 ])\n\n\nThere are (once again) several modules that contain functions designed to simulate draws from different distributions: for now, we’ll stick with the scipy.stats module.\nTo simulate n draws from a \\(\\mathcal{N}(\\)mu, sigma\\()\\) distribution we use the code\n\nsps.norm.rvs(mu, sigma, n)\n\n(note that, by default, the sample size comes at the end!) To simulate n draws from a \\(\\mathrm{Unif}(\\)a, b\\()\\) distribution we use the code\n\nsps.uniform.rvs(a, b, n)\n\n\n\n\n\n\n\nTask 5\n\n\n\n\nThe time spent waiting in line at Romaine’s is uniformly distributed between 2 mins and 10 mins. Simulate the process of waiting in line at Romaine’s one hundred times; store your result in a variable called x and us the command x[0:11] to display only the first 10 elements of x. (Aside: See if you can understand the syntax used here!)\nThe temperature of a healthy adult is normally distributed with mean 98.2 degrees Fahrenheit and standard deviation 2.4 degrees Fahrenheit. Simulate the process of selecting 150 healthy adults and recording their temperatures (in degrees Fahrenheit); store your result in a variable called y and display only the first 10 elements of y. (Hint: Remember how to index variables!)\n\n\n\nHow might we simulate the experiment of picking \\(k\\) numbers from a specified set? There are several different ways to do this in Python- the way we will do this is the same way we conducted this experiment during the Lecture 11 demo, using the np.random.choice() function.\n\n\n\n\n\n\nTask 6\n\n\n\nImport the numpy.random module with the nickname npr. Simulate the experiment of rolling a fair six-sided die 10 times and recording the outcome of each roll. Think about how you can translate this experiment into an experiment consisting of drawing numbers at random from a set of specified numbers.\n\n\n\nSetting the Seed\nNow, when it comes to simulations, there is a very important concept known as setting a seed.\n\n\n\n\n\n\nTask 7\n\n\n\n\nWrite npr.choice([1, 2, 3], size = 4) in a code cell, and run it three times. In a Markdown cell just below this cell, answer the following question: did you get the same result each time you ran the code cell?\nIn a new code cell write\n\n\nnpr.seed(15)\nnpr.choice([1, 2, 3], size = 4)\n\nRun this new cell three times and again answer the question: did you get the same result each time you ran the code cell?\n\nNow, turn to your neighbor and check whether you both got the same result as each other when completing task (b) above?\n\n\n\nAs you can see, setting a seed, in a sense, removes a certain amount of randomness in Python. After you set a seed, your random number generator will generate the same number (or set of numbers) every time you run it. Though it may seem unclear as to why we would want this, you may be able to imagine that setting the seed is extremely important when it comes to replicability, a concept we will return to later in the course."
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html#introduction-to-loops",
    "href": "Pages/Labs/Lab04/lab04.html#introduction-to-loops",
    "title": "Lab04",
    "section": "Introduction to Loops",
    "text": "Introduction to Loops\nSuppose we have the following outcomes of an experiment:\n\nx = ['success', 'failure', 'failure', 'success', 'failure', 'failure', 'failure', 'success']\n\nHow might we write code to count the number of successes in this string of outcomes? There are several different ways to accomplish this: one involves what is known as a for loop.\n\n\n\nfigure source: https://i.kym-cdn.com/photos/images/newsfeed/001/393/656/da7.jpg\n\n\nHere’s the general idea: we would like to perform an element-wise comparison; that is, we would like to iteratively check whether each element of x is a success or a failure. The “brute-force” way would be to check each element individually, using comparisons:\n\nx[0] == 'success'\n\nTrue\n\n\n\nx[1] == 'success'\n\nFalse\n\n\n\nx[2] == 'success'\n\nFalse\n\n\nAs you can imagine, though, this would get incredibly tedious, especially if x were large! This is where for loops become useful: they allow us to automate this iterative process.\nBefore returning to this success/failure problem, let’s look at an example to see how for loops work.\n\nfor fruit in ['apple', 'banana', 'pear']:\n  print(fruit)\n\napple\nbanana\npear\n\n\nHere are how the different components work:\n\nThe for keyword signifies the beginning of the for loop.\nThe name fruit is the variable.\nThe list following the in keyword contains all of the different values the variable will take during the execution of the for loop.\nThe code after the initial colon : is called the body of the loop. (Note that the body of a for loop must be indented properly!) Here is how the body is executed:\n\nFirst, the variable fruit is assigned the first value in the list of possible values specified in the first line of the loop\nThen, after assigning fruit this value, the code in the body is executed once.\nNext, the variable fruit is assigned the second value of the list of values, and the body is run again.\nThis continues until the list of all possible values is exhausted.\n\n\nSometimes, it may be useful to sketch a diagram/table to keep track of the code at each iteration of the loop:\n\n\n\nFIRST ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nfruit: ‘apple’\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nfruit: ‘apple’\n\n\n\n\n\n\n\nSECOND ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nfruit: ‘banana’\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nfruit: ‘banana’\n\n\n\n\n\n\n\nTHIRD ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nfruit: ‘pear’\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nfruit: ‘pear’\n\n\n\n\n\nIt may seem strange to keep track of the values of the variables at the end of each iteration. The reason we do so is because sometimes the body of the loop will actually change the value of a variable! For example, consider the code\n\nfor n in [1, 2, 3]:\n  n += 2\n  print(n)\n\n3\n4\n5\n\n\nthe associated diagram would look like\n\n\n\nFIRST ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nn: 1\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nn: 3\n\n\n\n\n\n\n\nSECOND ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nn: 2\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nn: 4\n\n\n\n\n\n\n\nTHIRD ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nn: 3\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nn: 5\n\n\n\n\n\nBy the way, notice the shorthand notation += that was used above:\n\n\n\n\n\n\nTip\n\n\n\nThe code x += y is equivalent to x = x + y.\n\n\nFinally, one thing that should be mentioned is that you can call the variable in a loop whatever you like!\n\nfor yummy in ['apple', 'banana', 'pear']:\n  print(yummy)\n\napple\nbanana\npear\n\n\n\n\n\n\n\n\nTask 8\n\n\n\nCopy-paste the code\n\nx = ['success', 'failure', 'failure', 'success', 'failure', 'failure', 'failure', 'success']\n\ninto a cell, and run it. Then, create a for loop that iterates through the elements of x and at each iteration prints True if the corresponding element of x is a 'success' and False if the corresponding element of x is a 'failure'. Your final output should look like:\n\n\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n\n\nBy the way, the set of values a variable will take during a for loop doesn’t have to be a list- it could also be an array! This is particularly useful when there are multiple things we would like to iterate over. For example:\n\nimport datascience as ds\ncredit_scores = ds.make_array(\n  [\"Anne\", 750],\n  [\"Barbara\", 755],\n  [\"Cassandra\", 745]\n)\n\nfor k in credit_scores:\n  print(k[0], \"has a credit score of\", k[1])\n\nAnne has a credit score of 750\nBarbara has a credit score of 755\nCassandra has a credit score of 745\n\n\n\n\n\n\n\n\nTask 9\n\n\n\nMake a table like the one above that keeps track of the variables and their values in the above loop. You do not need to turn this in; do it on a separate sheet of paper and in your .ipynb file simply state “I have done Task 2 on a separate sheet of paper.”\n\n\nNow, we never quite finished our problem of counting the number of successes in the variable x. We were able to iterate through the elements of x to determine which were successes and which were failures, but we never counted the number of successes.\n\nHere is the general idea:\n\nWe initialize a counter variable, which starts off with the value of 0.\nThen, we iterate through the elements of x as we did in Task 1 above. Instead of printing True or False, however, we use a conditional statement to add 1 to count if the corresponding element of x (i.e. the element of x under consideration in the current iteration of the loop).\nFinally, we see what the value of our counter variable is- this will be exactly the number of successes in x!\n\n\n\n\n\n\n\nTask 10\n\n\n\nCombine everything we’ve learned so far to count the number of successes in x. Here is a rough template of how your code should look:\n\ncount = 0     # initialize the counter variable\n\n&lt;for loop code here, containing a conditional and a 'count += 1'&gt;\n\ncount       # display the final value of our counter variable\n\n\n\nThere is another way to iterate through the elements in a list, and this is to use indexing. Before talking about how this works, we should quickly introduce another function: the arange() function from the numpy module. Here is how a general call to numpy.arange() works:\n\nnumpy.arange(a, b, n)\n\nThis code returns the array of evenly spaced integers between a and b - including a but excluding b, where each element is s more than the previous element. That is, the code above is equivalent to array([a, a+s, a+2s, ...]) As a concrete example:\n\nimport numpy as np\nnp.arange(0, 5, 2)\n\narray([0, 2, 4])\n\n\nThe arange() function is particularly useful when we are iterating using indices. For example, given a list x = [1, 2, 3, 4, 5], we can loop through the entries of x using:\n\nfor k in np.arange(0, len(x)):\n  print(x[k])\n\n1\n2\n3\n4\n5\n\n\nNote that this is equivalent to\n\nfor k in x:\n  print(k)\n\n1\n2\n3\n4\n5\n\n\n\n\n\n\n\n\nTask 11\n\n\n\nRewrite your loop from Task 10, except now iterate through the indices of x. Check that your output is the same as in Task 10.\n\n\n\nQuick Aside: arange() vs linspace()\nSome of you may recall that we previously used the numpy.linspace() function to generate a list of numbers between two specified endpoints. The key difference between these two functions is that:\n\narange() allows you to specify the step size\nlinspace() allows you to specify the final number of elements\n\n\n\n\n\n\n\nTask 12\n\n\n\nGenerate the list of numbers [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 2.9, 2] in two ways: one using arange() and the other using linspace()."
  },
  {
    "objectID": "Files/Labs/Lab05_Solns.html",
    "href": "Files/Labs/Lab05_Solns.html",
    "title": "Section 1: Markdown Syntax",
    "section": "",
    "text": "The quick brown fox jumps over the lazy dog.\n\nThat was a really cool sentence!"
  },
  {
    "objectID": "Files/Labs/Lab05_Solns.html#subsection-1.1",
    "href": "Files/Labs/Lab05_Solns.html#subsection-1.1",
    "title": "Section 1: Markdown Syntax",
    "section": "",
    "text": "The quick brown fox jumps over the lazy dog.\n\nThat was a really cool sentence!"
  },
  {
    "objectID": "Files/Labs/Lab05_Solns.html#subsection-1.2-itemized-and-enumerated-lists",
    "href": "Files/Labs/Lab05_Solns.html#subsection-1.2-itemized-and-enumerated-lists",
    "title": "Section 1: Markdown Syntax",
    "section": "Subsection 1.2: Itemized and Enumerated Lists",
    "text": "Subsection 1.2: Itemized and Enumerated Lists\n\nIt is very important to check the Binomial Conditions before using the Binomial Distribution!\n\nFailure to check the necessary conditions can lead to incorrect results.\nIncorrect results are not good!"
  },
  {
    "objectID": "Files/Labs/Lab05_Solns.html#subsection-1.3-typesetting-equations",
    "href": "Files/Labs/Lab05_Solns.html#subsection-1.3-typesetting-equations",
    "title": "Section 1: Markdown Syntax",
    "section": "Subsection 1.3: Typesetting Equations",
    "text": "Subsection 1.3: Typesetting Equations\nThe Pythagorean Theorem states that \\(a^2 + b^2 = c^2\\)\n\\[ f_X(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2} \\]\n\nPythagorean Theorem: \\(a^2 + b^2 = c^2\\)\nEuler’s Identity: \\(e^{i \\pi} + 1 = 0\\)\n\n\\[\nf_X(x) = \\begin{cases}\n    \\frac{1}{b - a} & \\text{if } a \\leq x \\leq b\\\\\n    0 & \\text{otherwise}\n    \\end{cases}\n\\]\n\\[\\begin{align*}\n    \\overline{x} & = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\\\\n    n \\overline{x} & = \\sum_{i=1}^{n} x_i\n\\end{align*}\\]"
  },
  {
    "objectID": "Files/Labs/Lab05_Solns.html#section-1.4-hyperlinks",
    "href": "Files/Labs/Lab05_Solns.html#section-1.4-hyperlinks",
    "title": "Section 1: Markdown Syntax",
    "section": "Section 1.4: Hyperlinks",
    "text": "Section 1.4: Hyperlinks\nPSTAT Department Website"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Welcome to the official course site for PSTAT 5A (titled Understanding Data) at the University of California, Santa Barbara! Please note that this is the site for Summer Session A, 2024 iteration of the course, with Mallory Wang.\n\nAll relevant information for the course can be found on this site, with the (perhaps crucial) exception of quizzes, which will take place on the course Gradescope site.\n\nIf you are looking for a past iteration of this course, please navigate to https://pstat5a-archives.github.io."
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html",
    "href": "Pages/Labs/Lab03/lab03.html",
    "title": "Lab03",
    "section": "",
    "text": "It’s finally time for us to revisit our notions of descriptive statistics (from Week 1 of the course), now in the context of Python!"
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html#modules-revisited",
    "href": "Pages/Labs/Lab03/lab03.html#modules-revisited",
    "title": "Lab03",
    "section": "Modules, Revisited",
    "text": "Modules, Revisited\nBefore we talk about plotting, we will need to quickly talk about modules again. Recall from Lab01 that modules are Python files containing definitions for functions and classes. Up until now, we’ve been importing all functions and classes from a module using the command\n\nfrom &lt;module name&gt; import *\n\nThere is another way to import modules, which is the following:\n\nimport &lt;module name&gt; as &lt;abbreviation&gt;\n\nFor example,\n\nimport numpy np\n\nnot only imports the numpy module but imports it with the abbreviation (i.e. nickname) np so that we can simply write np in place of numpy.\n\nThe reason this is particularly useful is because module names can sometimes be quite long, so being able to refer to the module with a shortened nickname will save a lot of time!\n\nIn general, if we import a module using\n\nimport &lt;module name&gt; as &lt;abbreviation&gt;\n\nwe reference functions from &lt;module name&gt; using the syntax\n\n&lt;abbreviation&gt;.&lt;function name&gt;()\n\nFor example, after having imported the numpy module with the nickname np, we access the sin() function contained in the numpy module by calling\n\nnp.sin()\n\n\n\n\n\n\n\nTask 1\n\n\n\n\nImport the numpy module as np, and check that np.sin(0) returns a value of 0.\nImport the datascience module as ds, and check that\n\n\nds.Table().with_columns(\n  \"Col1\", [1, 2, 3],\n  \"Col2\", [2, 3, 4]\n)\n\ncorrectly displays as\n\n\n\n\n\nCol1\nCol2\n\n\n\n\n1\n2\n\n\n2\n3\n\n\n3\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you import a module with an abbreviation &lt;abbreviation&gt;, you must always use the abbreviation when referencing the module; not the original module name.\n\n\nFor example, after importing numpy as np, running numpy.sin() would return an error."
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html#numerical-summaries",
    "href": "Pages/Labs/Lab03/lab03.html#numerical-summaries",
    "title": "Lab03",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\n\nMeasures of Central Tendency\nRecall that for a list of numbers \\(X = \\{x_i\\}_{i=1}^{n}\\), the mean is defined to be \\[ \\overline{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{n} (x_1 + \\cdots + x_n) \\] Computing the mean of a list or array of numbers in Python is relatively simple, using the np.mean() function [recall that we imported the numpy module with the abbreviation np, meaning np.mean() is a shorthand for numpy.mean()]. Similarly, to compute the median of a list or array we can use np.median().\n\n\n\n\n\n\nTask 2\n\n\n\nLet x_list be a list containing the elements 1, 2, and 3, and let x_array be an array containing the elements 1, 2, and 3. Compute the mean and median of x_list and x_array using the appropriate functions from the numpy module.\n\n\n\n\nMeasures of Spread\nRecall that we also discussed several measures of spread:\n\nStandard deviation\nIQR (Interquartile Range)\nRange\n\nSure enough, the numpy module contains several functions which help us compute these measures. Let’s examine each separately.\n\n\n\n\n\n\nTask 3\n\n\n\n\nLook up the help file on the function np.ptp(), and describe what it does. Also, answer the question: what does ptp actually stand for?\nNow, apply the np.ptp() function on your x_list and x_array variables from Task 1 above and check that it functions like you expect.\n\n\n\nNext, we tackle a slightly peculiar function: np.std(). We expect this to compute the standard deviation of a list/array, but…\n\n\n\n\n\n\nTask 4\n\n\n\n\nCompute the standard deviation of the x_list variable from Task 1 by hand, and write down the answer using a comment or Markdown cell.\nNow, run np.std(x_list). Does this answer agree with what you found in part (a) above?\nNow, recompute the standard deviation of x_list by hand but this time use \\((1/n)\\) instead of \\((1 / n - 1)\\) in the formula. How does this answer compare with the result of np.std(x_list)?\n\n\n\nThe result of the previous Task is the following: given a list x = [x1, x2, ..., xn], running np.std(x) actually computes \\[ \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\overline{x})^2 } \\] as opposed to our usual definition of standard deviation \\[ s_X = \\sqrt{ \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2} \\] We can actually fix this issue by passing in an additional argument to the np.std() function:\n\n\n\n\n\n\nTask 4 (cont’d)\n\n\n\n\nRun np.std(x_list, ddof = 1) and check whether this matches the result of part (a) above.\n\n\n\n\n\n\n\n\n\nResult\n\n\n\nTo compute the standard deviation of a list x, we run np.std(x, ddof = 1).\n\n\nFinally, we turn to the IQR: to compute the IQR of a list/array x, we use (after importing numpy as np)\n\nnp.diff(np.percentile(x, [25,75]))[0]"
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html#visualizations",
    "href": "Pages/Labs/Lab03/lab03.html#visualizations",
    "title": "Lab03",
    "section": "Visualizations",
    "text": "Visualizations\nIt’s finally time to make pretty pictures! The module we will use to generate visualizations in this class is the matplotlib module (though there are quite a few other modules that work for visualizations as well). The official website for matplotlib can be found at https://matplotlib.org/.\n\nBefore we generate any plots, we will need to run the following code once:\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nHere’s what these lines of code are doing:\n\n%matplotlib inline tells Jupyter to actually display our plots in our notebook (if we didn’t include this line, our plots wouldn’t display)\nimport matplotlib imports the matplotlib module\nimport matplotlib.pyplot as plt imports the pyplot submodule (a submodule is just a module contained within another larger module) with the abbreviation plt.\nplt.style.use('seaborn-v0_8-whitegrid') tells Jupyter to use a specific theme (called seaborn-v0_8-whitegrid) when generating plots.\n\nAgain, notice the beauty of the import &lt;module&gt; as &lt;abbreviation&gt; syntax- after running the third line above, we no longer need to write matplotlib.pyplot, just plt! Also, there are lots of other themes you can use when generating your plots: after completing this lab, I encourage you to consult this reference guide for a list of a few other pyplot themes.\n\nBoxplots and Histograms\nNow, let’s proceed on to make some plots. The first two types of plots we will look at are the two we used to describe numerical data: namely, boxplots and histograms. The functions we will use are the plt.boxplot() and plt.his() functions, respectively.\n\n\n\n\n\n\nTask 5\n\n\n\n\nMake a list called y that contains the following elements: [1, 2, 3, 4, 5, 4, 3, 5, 4, 1, 2].\nRun plt.boxplot(y); (be sure to include the semicolon!). With any luck, your plot should look like:\n\n\n\n\n\n\n\n\n\n\n\nLet’s make our boxplot horizontal, as opposed to vertical. Consult the help file on the matplotlib.pyplot.boxplot() function here and figure out how to position your boxplot horizontally. Your new plot should look like:\n\n\n\n\n\n\n\n\n\n\n\nNext, let’s add some color to our plot. Within your call to plt.boxplot(), add the following: patch_artist=True, boxprops = dict(facecolor = \"aquamarine\") (don’t worry too much about what exactly this code is doing). Your boxplot should now look like this:\n\n\n\n\n\n\n\n\n\n\n\nFinally, let’s add a Title! Right below your call to plt.boxplot(), add the following: plt.title(\"My First Python Boxplot\"); (again, note the semicolons). Your final plot should look like this:\n\n\n\n\n\n\n\n\n\n\n\nTime for a review: based on the boxplot we just generated, what is the IQR of y? Write your answer in a Markdown cell. Then, use the syntax discussed in the previous section of this Lab to use Python to compute the IQR of y, and comment on the result.\n\n\n\nOf course, boxplots are not the only way to summarize numerical variables: we also have histograms!\n\n\n\n\n\n\nTask 6\n\n\n\nCall the plt.hist() function on the y list defined in Task 3, and use the help file to add arguments to your call to plt.hist() function to generate the following plot:\n\n\n\n\n\n\n\n\n\nPay attention to the number of bins!\n\n\n\n\nScatterplots\nWe should also quickly discuss how to generate scatterplots in Python.\n\n\n\n\n\n\nTask 7\n\n\n\n\nCopy-paste the following code into a code cell, and then run that cell (don’t worry about what this code is doing- we’ll discuss that in a future lab).\n\n\nnp.random.seed(5)\n\nx1 = np.random.normal(0, 1, 100)\nx2 = x1 + np.random.normal(0, 1, 100)\n\nplt.scatter(x1, x2);\n\nYour plot should look like this:\n\n\n\n\n\n\n\n\n\n\nAdd an x-axis label that says \"x1\" and a y-axis label that says \"x2\", along with the title “My First Python Scatterplot”. Your final plot should look like:\n\n\n\n\n\n\n\n\n\n\n\nDoes there appear to be an association between the variables x1 and x2? If so, is the association positive or negative? Linear or nonlinear? Answer using a comment or a Markdown Cell."
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html#plotting-a-function",
    "href": "Pages/Labs/Lab03/lab03.html#plotting-a-function",
    "title": "Lab03",
    "section": "Plotting a Function",
    "text": "Plotting a Function\nFinally, I’d like to take a quick detour from descriptive statistics and talk about how to plot a function using Python. As a concrete example, let’s try and plot a sine curve from \\(0\\) to \\(2\\pi\\).\nIf you recall, on Lab01 we used the sin() function from the math module- it turns out that the numpy module (which, recall, we have imported as np) also has a sin() function, so let’s use that one today:\n\nnp.sin()\n\nNext, we create a set of finely-spaced points between our two desired endpoints (in this case, \\(0\\) and \\(2\\pi\\), respectively). We will do so using the np.linspace() function, which works as follows:\n\nnp.linspace(start, stop, num)\n\ncreates a set of num evenly-spaced values between start and stop, respectively. For instance:\n\nnp.linspace(0, 1, 10)\n\narray([ 0.        ,  0.11111111,  0.22222222,  0.33333333,  0.44444444,\n        0.55555556,  0.66666667,  0.77777778,  0.88888889,  1.        ])\n\n\nIn the context of plotting, the more points we generate the smoother our plot will seem (you will see what this means in a minute). As such, let’s start with 150 points between 0 and 2 * pi:\n\nx = np.linspace(0, 2 * np.pi, 150)\n\nFinally, we call the plt.plot() function on x and np.sin(x) to generate our plot:\n\nplt.figure(figsize=(4.5, 2.25))\nplt.plot(x, np.sin(x))\n\n\n\n\n\n\n\n\nLet’s see what would have happened if we used fewer values in our np.linspace() call:\n\nxnew = np.linspace(0, 2 * np.pi, 10)\nplt.plot(xnew, np.sin(xnew))\n\n\n\n\n\n\n\n\n\n\nSo, the more points we include in our call to np.linspace(), the smoother our final function will look!\n\nSo, to summarize, here is the general “recipe” to plot a function f() between two values a and b in Python:\n\nLet x = np.linspace(a, b, &lt;some large value&gt;)\nCall plt.plot(x, f(x))\nAdd labels/titles as necessary\n\n\n\n\n\n\n\nTask 8\n\n\n\nGenerate a plot of the function \\(f(x) = x - x^2 \\sin(x)\\) between \\(x = -10\\) and \\(x = 10\\). Experiment around with the number of values generated by np.linspace() to ensure your plot is relatively smooth. Be sure to include axis labels; also, change the color of the graph to red. Your final plot should look something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNOTICE\n\n\n\nYou only need to complete up to here during Lab, but you should complete the following tasks on your own as they are fair game for quizzes and exams (and are also very useful for your own edification!)\n\n\n\nOverlaying Plots\nSometimes it will be useful to overlay two plots on top of each other. Recall that, for a function f() and a variable x that has been assigned a value resulting from a call to numpy.linspace(), we generate a graph of f() using (assuming matplotlib.pyplot has been imported as plt)\n\nplt.plot(x, f(x));\n\nIt stands to reason, then that given another function g() we should be able to superimpose the graph of g() onto the graph of f() by simply adding another call to plt.plot():\n\nplt.plot(x, f(x));\nplt.plot(x, g(x));\n\n\n\n\n\n\n\nTask 9\n\n\n\nGenerate a graph of sin(); on top of this graph, superimpose the graph of cos(). Restrict the x values on your graph to be between \\(-4\\pi\\) and \\(4\\pi\\). Your final graph should look like the following (pay attention to the axis labels and title!):\n\n\n\n\n\n\n\n\n\n\n\nNow, as it stands, it’s a bit difficult to determine which curve corresponds to the sine curve and which corresponds to the cosine curve. As such, we should add some labels!\n\n\n\n\n\n\nTask 10\n\n\n\nCopy your code from Task 9 above into a new code cell, and\n\nadd label = \"sine\" to your call to plt.plot() containing the sine curve\nadd label = \"cosine\" to your call to plt.plot() containing the cosine curve.\n\nDoes this new plot look any different than the plot you generated in Task 3?\n\n\nHm, doesn’t look like anything changed… That’s because we didn’t add a legend to our plot! To add a legend, we simply tack on a call to plt.legend() after our code from above.\n\n\n\n\n\n\nTask 11\n\n\n\nCopy your code from Task 10 above into a new code cell, and add a line underneath it containing a call to plt.legend(). Look up the help file to figure out what arguments you need to pass in to obtain the following graph (note the position of the legend):\n\n\n\n\n\n\n\n\n\n\n\nOkay, we’re almost there! The only issue is that now the legend is covered up by the actual graphs. One way we can fix this is by extending the \\(y-\\)axis further, using the function plt.ylim():\n\n\n\n\n\n\nTask 12\n\n\n\nCopy your code from Task 11 above into a new code cell, and add a line underneath it containing a call to plt.ylim(). Look up the help file to figure out what arguments you need to pass in to obtain a lower \\(y-\\)limit of \\(-1.5\\) and an upper \\(y-\\)limit of \\(2.0\\). Your final graph should look like this:\n\n\n\n\n\n\n\n\n\n\n\nFinally, it is sometimes considered bad form to rely too heavily on colors in plots. This is because doing so alienates readers who are colorblind. One way around this is to rely on different line types; e.g. used dashed lines for one graph and dotted lines for another.\n\n\n\n\n\n\nTask 13\n\n\n\nCopy your code from Task 12 above into a new code cell. Read the following help file and figure out how to pass in a value to the linestyle argument to your two calls to plt.plot() to generate the following plot:\n\n\n\n\n\n\n\n\n\nNote that the sine curve is now dotted, and the cosine curve is now dashed."
  },
  {
    "objectID": "Pages/Labs/Lab05/lab05.html",
    "href": "Pages/Labs/Lab05/lab05.html",
    "title": "Lab05",
    "section": "",
    "text": "Let’s start off this week’s lab by taking a quick break from Python and instead learn some tools for making our Jupyter Notebooks look a little nicer (and help incorporate some mathematical equations into them!).\nAlso, the format of this lab is a little different in that there aren’t any numbered “Tasks”; instead, there are “Action Items” which you need to perform in a .ipynb file. As such, you don’t need to label your tasks; we will only be looking at your final product (i.e. your final PDF)."
  },
  {
    "objectID": "Pages/Labs/Lab05/lab05.html#introduction-to-markdown-syntax",
    "href": "Pages/Labs/Lab05/lab05.html#introduction-to-markdown-syntax",
    "title": "Lab05",
    "section": "Introduction to Markdown Syntax",
    "text": "Introduction to Markdown Syntax\nRemember way back in Week 1, when we started adding “Markdown Cells” to our Jupyter Notebooks? Unbeknownst to you, you’ve actually been using a different computing language than Python! Wikipedia defines Markdown as:\n\n[…] a lightweight markup language for creating formatted text using a plain-text editor.\n\nWhat this means is that Markdown is a language that is specifically designed to communicate with your computer and facilitate the formatting of text and equations."
  },
  {
    "objectID": "Pages/Labs/Lab05/lab05.html#markdown-syntax",
    "href": "Pages/Labs/Lab05/lab05.html#markdown-syntax",
    "title": "Lab05",
    "section": "Markdown Syntax",
    "text": "Markdown Syntax\nAs with Python, Markdown has some specific syntax as well. We will take a few moments to explore some of this syntax.\n\n1. Headers\n\n\n\n\n\n\nImportant Information\n\n\n\nThere are several different “levels” of headers:\n\nA main (first-level) header\nA second-level header\nA third-level header\nAnd so on and so forth\n\nIn Markdown, the level of the header is specified by how many hashtags (#’s) we include before the header text.\n\n\nFor example: all of the headers used in this explanation document are either second- or third-level headers.\n\n\n\n\n\n\nAction Item\n\n\n\nAdd a first-level (main) header that says Section 1: Markdown Syntax. Run this cell, and ensure it displays properly.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nAfter your first-level (main) Section 1: Markdown Syntax header, add a second-level header that says Subsection 1.1. Again, run this cell and ensure that this header appears smaller than the first-level (main) header you created in the action item above.\n\n\n\n\n2. Text Formatting\nAs we have previously seen, text written in a Markdown cell will display exactly as it is written in your final PDF. We can, however, also spice up the formatting of our text!\n\n\n\n\n\n\nNote\n\n\n\nThere are several different text formatting options available to us in Markdown:\n\nBoldface Text: **Boldface Text**\nItalicized Text: _Italicized Text_ or *Italicized Text*\nFixed-Width Text: `Fixed-Width Text`\n\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nUnderneath your second-level header \"Subsection 1.1\", write the following sentence including all formatting as it appears here:\n\nThe quick brown fox jumps over the lazy dog\n\n\n\nBy the way, notice how we managed to display the sentence above as a block-quote? The syntax to do that in Markdown is using a single &gt; symbol, followed by a space. For example,\n\nThis text\n\nwas generated using the code &gt; This text.\n\n\n\n\n\n\nCaution\n\n\n\nBe mindful about line breaks and spaces when using Markdown syntax.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nUnderneath your “quick brown fox” text, add a block-quote that says\n\nThat was a really cool sentence!\n\nRun your Markdown cell, and ensure the block-quote is displaying properly. (Again: line breaks and indentation will be important!)\n\n\n\n\n3. Itemized and Enumerated Lists\n\n\n\n\n\n\nImportant Information\n\n\n\nThere are two main types of lists: itemized and enumerated lists. Itemized lists have no explicit ordering: these are sometimes called “bulleted” lists, and look like this:\n\nItem 1\nItem 2\nItem 3\n\nEnumerated lists do have an explicit ordering:\n\nItem 1\nItem 2\nItem 3\n\nIn Markdown, itemized lists are created by navigating to a new line, writing a dash (-) or an asterisk (*), followed by a space, followed by the text you wish to have displayed in the corresponding item. For instance: the itemized list above was generated using the code\n\n- Item 1\n- Item 2\n- Item 3\n\nIn Markdown, enumerated lists are created by navigating to a new line, writing the number of the item (e.g. 1, 2, etc.) either either a close-parenthesis ()), or a period (.) For instance: both of the following codes will generate an enumerated list:\n\n1) Item 1\n2) Item 2\n3) Item 3\n\n\n1. Item 1\n2. Item 2\n3. Item 3\n\nTo add sublevel items, navitage to a new line, hit the space bar 4 times, and then add the corresponding item symbol (i.e. - or * for itemized lists; 1) or a) or 1. or a. for enumerated lists; etc.)\n\nAs an example, this is a main-level item\n\nFollowed by a sublevel item!\n\n\nThis was generated using\n\n- As an example, this is a main-level item\n    - Followed by a sublevel item!\n\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nAdd a new second-level header to your document that says Subsection 1.2: Itemized and Enumerated Lists. Underneath this, include the following list (making sure it displays properly when you run the cell!)\n\nIt is very important to check the Binomial Conditions before using the Binomial Distribution!\n\nFailure to check the necessary conditions can lead to incorrect results.\nIncorrect results are not good!\n\n\n\n\n\n\n4. Equations in Markdown\nOne of the very nice things about Markdown is that it is able to (very nicely) combine both plain-text as well as equations!\n\nThe language used to specify equations in Markdown is called LaTeX (pronounced “lay-TECH”, or “lah-TECH”). LaTeX equations have two main styles: inline, like \\(x + y\\), and “display-style”, like \\[ \\sum_{i=1}^{k} x_i \\]\n\n\n\n\n\n\nImportant Information\n\n\n\nIn LaTeX, inline equations are specified by single dollar signs and display-style equations are specified by double-dollar signs. For instance:\n\n$x + y$ will display as \\(x + y\\)\n$$x + y$$ will display as \\[x + y\\]\n\n\n\nInside either an inline or display-style equation environment, we can use LaTeX-specific syntax to generate equations.\n\n\n\n\n\n\nImportant Information\n\n\n\n\nStandard mathematical symbols display in LaTeX as they would in text: this includes addition (+), subtraction (-), division (/), and multiplication (*).\nThe symbol for “less than or equal to” (≤) is typeset as \\leq; the symbol for “greater than or equal to” (≥) is typeset as \\geq.\nExponents are generated using the caret (^): for example, $x^2$ displays as \\(x^2\\)\nSubscripts are generated using the underscore (_): for example, $x_2$ displays as \\(x_2\\)\n\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nAdd a second-level (main) header that says \"Subsection 1.3: Typesetting Equations\", and underneath it write the following sentence (including all LaTeX formatting!):\n\nThe Pythagorean Theorem states that \\(a^2 + b^2 = c^2\\).\n\nRun this cell, and ensure it displays properly.\n\n\nWe can also typeset more advanced formulas and symbols:\n\n\n\n\n\n\nImportant Information\n\n\n\n\nSums (using sigma-notation) are generated using the \\sum command: for example, $\\sum_{x = 0}^{1} x$ displays as \\(\\sum_{x = 0}^{1} x\\)\nIntegrals are generated using the \\int command: for example, $\\int_{a}^{b} f(x) \\ dx$ displays as \\(\\int_{a}^{b} f(x) \\ dx\\)\nFractions are generated using the \\frac command: for example, $\\frac{1}{2}$ displays as \\(\\frac{1}{2}\\)\nSquare-roots are generated using the \\sqrt command: for example, $\\sqrt{2}$ displays as \\(\\sqrt{2}\\)\nThe symbol \\(\\pi\\) is generated using $\\pi$\nAdding a bar on top of a variable can be done using \\overline{}; e.g. $\\overline{y}$ typesets as \\(\\overline{y}\\)\nThe probability symbol can be typeset using $\\mathbb{P}$; e.g. \\(\\mathbb{P}\\).\nThe symbol for Expected Value can be typeset using $\\mathbb{E}$; e.g. \\(\\mathbb{E}\\).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn LaTeX, curly braces ({, }) are used to delineate “chunks” of code/text. For example, if you just write $x^-2$, this displays as \\(x^-2\\). If we want to display \\(x^{-2}\\), we need to use $x^{-2}$.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen writing fractions inside parentheses, it is important to use \\left( and \\right) to ensure the size of the parentheses scale with the fractions. For example, \\[ (\\frac{1}{2}) \\] which was generated using $$ (\\frac{1}{2}) $$, looks a bit worse than \\[ \\left(\\frac{1}{2}\\right) \\] which was generated using $$ \\left(\\frac{1}{2}\\right) $$.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nAdd the following equation to your document: \\[ f_X(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2}  \\]\n\n\nThere is nothing stopping us from including equations in itemized and enumerated lists!\n\n\n\n\n\n\nAction Item\n\n\n\nAdd the following list to your document, ensuring that it displays properly when the cell is run (pay attention to the formatting of the text as well!):\n\nPythagorean Theorem: \\(a^2 + b^2 = c^2\\)\nEuler’s Identity: \\(e^{i \\pi} + 1 = 0\\)\n\n\n\nIt may also be useful to learn how to make piecewise-defined equations using LaTeX. To typeset piecewise-defined functions, we use the cases environment: \\[ \\begin{cases}\n    a   & \\text{if condition 1}   \\\\\n    b   & \\text{if condition 2}   \\\\\n    c   & \\text{if condition 3}   \\\\\n\\end{cases} \\] was generated using the code\n$$ \\begin{cases} \n    a   & \\text{if condition 1}   \\\\\n    b   & \\text{if condition 2}   \\\\\n    c   & \\text{if condition 3}   \\\\\n\\end{cases} $$\n\n\n\n\n\n\nAction Item\n\n\n\nAdd the probability density function \\(f_X(x)\\) of the \\(\\mathrm{Unif}(a, \\ b)\\) distribution to your Notebook: \\[ f_X(x) = \\begin{cases} \\frac{1}{b - a} & \\text{if } a \\leq x \\leq b \\\\ 0 & \\text{otherwise} \\\\ \\end{cases} \\]\n\n\nFinally, we should talk about how to align equations. The easiest way to align display-style equations is using an align* environment. For example: \\[\\begin{align*}\n  a + b       & = c + d + e   \\\\\n  a + b + 0   & = c + d\n\\end{align*}\\] was typeset using\n\\begin{align*}\n  a + b       & = c + d + e   \\\\\n  a + b + 0   & = c + d + 0\n\\end{align*}\n\n\n\n\n\n\nAction Item\n\n\n\nAdd the following set of equations into your Markdown document, taking care to use appropriate alignment.\n\\[\\begin{align*}\n  \\overline{x}            & = \\frac{1}{n} \\sum_{i=1}^{n} x_i    \\\\\n  n \\overline{x}    & = \\sum_{i=1}^{n} x_i\n\\end{align*}\\]\n\n\nThis is only just the surface of what LaTeX can do! If you are planning on going into any STEM-related field, we highly encourage you to delve deeper into the world of LaTeX as it will be a very valuable skill to have.\n\n\nHyperlinks\nSometimes it will be necessary to include hyperlinks into our reports. With Markdown, this is fairly straightforward!\n\n\n\n\n\n\nImportant Information\n\n\n\nThe syntax of including a hyperlink in a Markdown cell is as follows:\n\n[text](link)\n\nFor example, Click Here was generated using [Click Here](https://google.com).\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nMake a new second-level header in your document called Section 1.4: Hyperlinks. Underneath this header, create clickable text that says “PSTAT Department Website” and, when clicked, navigates the user to the main website of the Department of Statistics and Applied Probability here at UCSB (https://pstat.ucsb.edu). Your final result should look and function like PSTAT Department Website."
  },
  {
    "objectID": "Pages/Labs/Lab05/lab05.html#importing-and-manipulating-data",
    "href": "Pages/Labs/Lab05/lab05.html#importing-and-manipulating-data",
    "title": "Lab05",
    "section": "Importing and Manipulating Data",
    "text": "Importing and Manipulating Data\nFinally, let’s play with some real-world data!\n\nIntroduction to the Dataset\nThe dataset we will explore is called air22.csv, and can be accessed at https://pstat5a.github.io/Files/Datasets/air22.csv. It contains observations from the Bureau of Transportation Statistics https://www.transtats.bts.gov/ on flights taking place during 2022.\nHere is an abridged version of the data dictionary:\n\nyear: the year in which the data was collected\nmonth: the month in which the data was collected\ncarrier: the carrier abbreviation (e.g. AS for Alaska, AA for American Airlines, etc.)\ncarrier_name: the full name of the carrier (e.g. Alaska, American, etc.)\nairport: the airport IATA code (e.g. SEA for SeaTac International Airport, etc.)\nairport_name: the full name of the airport\narr_flights: the number of flights that arrived\narr_del15: the number of flights that were delayed for more than 15 minutes\ncarrier_ct: the number of flights delayed due to carrier-related issues (e.g. no crew, delayed captain, etc.)\nweather_ct: the number of flights delayed due to weather-related issues\nnas_ct: the number of flights delayed due to National Air Security (NAS)-related issues (e.g. heavy traffic)\nsecurity_ct: the number of flights cancelled due to security-related issues\nlate_aircraft_ct: the number of flights delayed due to the arriving aircraft being delayed\narr_cancelled: the number of cancelled flights\narr_diverted: the number of flights that were diverted\narr_delay: the total time (in minutes) of delayed flight\ncarrier_delay: the total time (in minutes) of delay due to air carrier-related issues\nweather_delay: the total time (in minutes) of delay due to air weather-related issues\nnas_delay: the total time (in minutes) of delay due to air NAS-related issues\nsecurity_delay: the total time (in minutes) of delay due to air security-related issues\nlate_aircraft_delay: the total time (in minutes) of delay due to the arriving aircraft being delayed\n\n\n\nPart 1: Importing\nWe should start by importing the dataset into our JupyterHub environment! In this class, we will almost exclusively import datasets to datascience tables (remember those, from Lab 1?) The syntax we use to import a dataset and store it in a table called table_name is:\n\ntable_name = Table.read_table(&lt;location of dataset&gt;)\n\nBy default, the read_table method will use the first line of data as labels for each of the columns. If, however, your data does not include labels as its first line you will need to manually specify the labels:\n\ntable_name = Table.read_table(&lt;location of dataset&gt;, names = [labl1, ...])\n\n\n\n\n\n\n\nAction Item\n\n\n\nAdd a first-level header that says Section 2: Importing and Manipulating Data.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nImport the air22.csv dataset, and save it as a table called air. Hint: make sure you import any necessary modules first!\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe list of Table()-related methods at http://data8.org/datascience/tables.html may prove very useful for this lab!\n\n\n\n\nPart 2: Getting a Feel for the Dataset\nLet’s start of easy by answering a few questions. You should use code to determine the answers to these questions, but should write your answers using descriptive sentences in a markdown cell.\n\n\n\n\n\n\nAction Item\n\n\n\nHow many rows are in the dataset? (Recall: in the language introduced in Week 1, this is the number of observational units in the data matrix.)\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nHow many columns are in the dataset? (Recall: in the language introduced in Week 1, this is the number of variables in the data matrix.)\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nReturn a list of the column names in the dataset. Hint: Recall that the column names, in the context of the datascience module, are sometimes called the labels of the table. Again, consult the help file if you need help!\n\n\n\n\nPart 3: Accessing Specific Elements of the Dataset\nThere are a few methods we can use to access specific parts of a Table:\n\ntable_name.column(&lt;index or label&gt;): returns an array containing the data in the specified column\ntable_name.row(&lt;index or label&gt;): returns an array containing the data in the specified column\ntable_name.column(i).item(j): returns the value in column i + 1, row j + 1 (note the plus ones!)\n\n\n\n\n\n\n\nAction Item\n\n\n\nDisplay only the arr_del15 column of the dataset.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nWhat years are included in the dataset?\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nCreate a histogram of the delay times caused by weather-related issues. Use 100 bins, and include axis labels as well as a plot title.\n\n\nWe can also get creative, and use comparisons to subset various parts of our dataset. For example,\n\nair.column(\"carrier\") == \"AS\"\n\nwill return a Boolean vector (i.e. a vector of True and False elements), with True elements corresponding to values in the carrier column that have value \"AS\". In words: this would give us the indices of rows corresponding to data on Alaska Airlines.\n\n\n\n\n\n\nAction Item\n\n\n\nHow many observational units were recorded from Alaska Airlines? Hint: Think about how a sum could help us here.\n\n\n\n\nPart 4: A Mini-Project\nAlright, let’s close off this lab with a bit of a mini-project. Our goal is to examine the number of arrivals over time. Specifically, we would ultimately like to plot the average number of arrivals per month vs. month.\n\n\n\n\n\n\nAction Item\n\n\n\nExplain, in words, what the following line of code is doing:\n\nair.row(air.column(1) == 1)\n\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nExplain, in words, what the following line of code is doing:\n\nair.row(air.column(1) == 2)[6]\n\nHint: arr_flights is the 7th column in the dataset.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nBased on your answers to the previous two Action Items, complete the following code to compute the average number of flights per month:\n\nmeans = []\nfor k in _________________:\n  means.______(np.nanmean(air.row(air.column(___) == ___)[___]))\n\n(By the way, the nanmeans() function is a variant of the mean() function from the numpy module that ignores missing values when computing the mean.)\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nPlot your means list against the list containing the integers 1, 2, 3, …, 12 (this list functions like a list of month names.)"
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html",
    "href": "Pages/Labs/Lab01/lab01.html",
    "title": "Lab01",
    "section": "",
    "text": "Welcome to the first PSTAT 5A Computing Lab! As we will soon learn, computers play an integral part in effectively and efficiently performing statistical analyses. The primary goal of these Computing Labs is to develop the skills to be able to communicate with computers, and also learn the basic principles and language of programming."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#structure-of-labs",
    "href": "Pages/Labs/Lab01/lab01.html#structure-of-labs",
    "title": "Lab01",
    "section": "Structure of Labs",
    "text": "Structure of Labs\nEvery week we (the course staff) will publish a lab document, which is intended to be completed during your Lab Section (i.e. your first Section) of the week. Each lab document will consist of a combination of text, tips, and the occasional task for you to complete based on the text provided. Your TA will cover exactly what you need to turn in at the end of each lab in order to receive credit, but you should read all lab material carefully and thoroughly as content from labs will appear on quizzes and exams."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#what-is-programming",
    "href": "Pages/Labs/Lab01/lab01.html#what-is-programming",
    "title": "Lab01",
    "section": "What Is Programming?",
    "text": "What Is Programming?\nComputers, though incredibly useful, are fairly complex machines. To communicate with them, we need to use a specific language, known as a Programming Language. There are a number of programming languages currently in use, with names such as R, Julia, MatLab, and - the language we will use for this course - Python.\n\nPython programs can be written in a number of different environments, such as a text editor (e.g. Notepad, VS Code, etc.) or a Terminal window. For this class, we will use Jupyter Notebook (where Jupyter is pronounced like the planet), an interactive environment that has the added benefit of being hosted online meaning you do not have to download anything onto your personal machines in order to run Python code!"
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#getting-started",
    "href": "Pages/Labs/Lab01/lab01.html#getting-started",
    "title": "Lab01",
    "section": "Getting Started",
    "text": "Getting Started\n\nNavigate to https://pstat5a.lsit.ucsb.edu\nClick the “Sign in with your UCSB NetID” button, and sign in.\nUnder “Notebook”, click “Python 3 (ipykernel)” (see below).\n\n\nCongratulations- you have just made your first Jupyter notebook! Now, it’s time for our first task:\n\n\n\n\n\n\nTask 1\n\n\n\nChange the name of your notebook to “Lab01” using the following steps:\n\nIn the lefthand menu bar, find the notebook you just created (by default this will be something like “Untitled” or “Untitled1”), and right-click and click “Rename” (see picture below)\n\n\n\nRename your file to “Lab01”, and then hit the return (enter) key on your keyboard. You should see the filename in the menubar update:\n\n\n\n\n\nJupyterHub Environment\nLet’s take a minute to familiarize ourselves with the JupyterHub environment. Every Jupyter notebook is comprised of what are known as cells; these are the shaded grey rectangles that appear in a Jupyter notebook.\n\nIf your cell has a grey background (like in the image above), it is inactive. To activate a cell, place your cursor inside it, and click:\n\nmeans it is selected and active, and ready to be populated with text and/or code.\n\n\n\n\n\n\nImportant\n\n\n\nWhen you run code using the “Run” button at the top of your environment, only the active cell will be executed.\n\n\n\n\nCells\nThere are two main types of cells we will be using in this class: Markdown cells (which include text/descriptions, but no code) and code cells (which contain code that needs to be run). We’ll be talking a bit more about Markdown cells in a few weeks.\n\n\n\n\n\n\nTask 2\n\n\n\n\nIf you haven’t already, click into the code cell that was automatically created when you created your document to activate it.\nClick on the dropdown menu that currently says “code” (near the center of the top of your interface), and select “Markdown”\n\n\n\nClick back into the cell, copy-paste the text [including the hashtag!] # Task 2, and then run the cell by clicking on the button that looks like a “play” symbol at the top of your window:\n\n\n\nNote that after running your cell from step 3 above, Jupyter automatically created a new code cell. Click into this code cell and run the code 2 + 2.\n\nWhen you are done, your notebook should look something like this:\n\n\n\nNotice that after running a cell, Jupyter automatically adds a new cell right after it!\n\n\n\n\n\n\nTip\n\n\n\nTo run a cell and automatically create a new cell underneath it, use the keyboard shortcut SHIFT + ENTER.\n\n\nBy the way, do you notice the little In [1]: at the left of our first cell? This is Jupyter’s way of letting us know the order in which the code cells have been executed. The 1 in our cell from Task 2 above corresponds to the fact that this was the 1st code cell we executed in our document."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#coding-with-python",
    "href": "Pages/Labs/Lab01/lab01.html#coding-with-python",
    "title": "Lab01",
    "section": "Coding with Python",
    "text": "Coding with Python\nThere is a reason we use the word “language” to describe programming languages- that is because they function quite like a human language. This means that they each have their own syntax (i.e. set of grammar rules). It is precisely the syntax of the Python language that we will be learning over the course of these Computing Labs!\n\nPrograms are made up of expressions, like 2 + 2. We evaluate expressions by running (or executing) them in a programming language. Expressions are like the sentences of programming- they contain complex pieces of information that are conveyed between the user and the computer.\n\nMuch like sentences in other languages, expressions must obey a rigid syntax. For example, when we want to perform addition in Python we must use the + symbol; we can’t, for example, say 2 plus 2.\n\nWhat happens when we violate a syntax rule? Well…\n\n\n\n\n\n\nTask 3\n\n\n\n\nCreate a mardown cell and write # Task 3\nCreate a code cell, and run 2 plus 2. (You should get an error!)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor this class, we expect you to precede each code cell from a particular task with a markdown cell that says # Task X (where X is the number of the task).\n\nWe will stop explicitly writing this step in the tasks below, but you are still expected to include a labeling cell!\n\n\nWell, what is this error saying? Let’s examine it more closely.\n\n  File \"&lt;ipython-input-2-5196071441ec&gt;\", line 1\n    2 plus 2\n      ^\nSyntaxError: invalid syntax\n\nIndeed, Python is telling us exactly what went wrong- the SyntaxError part of the error message tells us that we violated one of the syntax rules of Python, and the ^ pointing to the p in plus is telling us that the exact syntax error occurred when we tried to use the word plus.\n\n\n\n\n\n\nTip\n\n\n\nAlways read error messages!\n\n\nThe messages that Python displays when we get an error are Python’s way of trying to communicate with us what is going wrong!"
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#python-as-a-calculator",
    "href": "Pages/Labs/Lab01/lab01.html#python-as-a-calculator",
    "title": "Lab01",
    "section": "Python as a Calculator",
    "text": "Python as a Calculator\nAlright, let’s get our hands dirty with some real programming! One of the many uses of Python is to help us compute arithmetic quantities very quickly. As a rule-of-thumb, Python adheres to the order of operations:\n\nParentheses\nExponents\nMultiplication\nDivision\nAddition\nSubtraction\n\nHere is a list of mathematical operators and their corresponding Python syntax:\n\n\n\nOperation\nPython Operator\nExample\nResult\n\n\n\n\nAddition\n+\n2 + 2\n4\n\n\nSubtraction\n-\n2 - 2\n0\n\n\nMultiplication\n*\n2 * 2\n4\n\n\nDivision\n/\n2 / 2\n1\n\n\nExponentiation\n**\n2 ** 2\n4\n\n\n\n\n\n\n\n\n\nTask 4\n\n\n\nCompute the following:\n\n\\(\\displaystyle \\frac{2 + 3}{4 + 5^6}\\)\n\\(\\displaystyle (1 - 3 \\cdot 4^5)^{6}\\)\n\n\n\nNaturally, Python is capable of much more than just basic arithmetic!\n\n\n\n\n\n\nTask 5\n\n\n\nCreate a code chunk and run sin(1) to compute the sine of 1.\n\n\nUh-oh- looks like we’ve encountered another error! Indeed, even the most experience coder will often run up against errors like this, and need to subsequently enter the stage of debugging their code.\n\nWe’re now getting a new error: this time, it’s a NameError. As the name suggests, this is Python’s way of telling us that it doesn’t recognize the name of something we’ve written. In fact, it’s explicitly saying:\n\nNameError: name 'sin' is not defined,\n\nSpecifically, Python is telling us that it (somehow) doesn’t know what sin means. Why? Well, to answer that, we need to take a bit of a detour into the world of modules."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#python-modules",
    "href": "Pages/Labs/Lab01/lab01.html#python-modules",
    "title": "Lab01",
    "section": "Python Modules",
    "text": "Python Modules\nIt is important to note that all Python objects take up space in the form of memory (i.e. storage space on your computer). Nowadays, with recent innovations in computers and computer memory, this is not so much of an issue but historically, when many of these programming languages were first being created, optimizing space was of the utmost concern. (Even today, efficiency is a guiding tenet of most programmers!)\n\nThink of it this way- if you are doing work on code that doesn’t involve much trigonometry, there isn’t a whole lot of need to have the sin function readily available. The idea programmers had was to compartmentalize, and store certain functions in what are known as modules.\n\nModules are Python files containing definitions for functions and classes (we’ll talk about data classes a little later). While data types and built-in functions in the Python standard library are available for immediate use, modules need to be imported first.\n\nThe syntax for importing all functions from a module is:\n\nfrom &lt;module name&gt; import *\n\nSometimes, we may not want to import the entirety of a module and instead import only a couple of functions from that module. In that case, we would use the syntax:\n\nfrom &lt;module name&gt; import &lt;function name&gt;\n\nWe’ll talk a bit more about modules in a future lab. For now, let’s return to our task of computing \\(\\sin(1)\\).\n\n\n\n\n\n\nTask 5 (cont’d)\n\n\n\nIt turns out that the sin() function is located in the math module Load all functions from the math module, and then try re-running sin(1)."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#functions",
    "href": "Pages/Labs/Lab01/lab01.html#functions",
    "title": "Lab01",
    "section": "Functions",
    "text": "Functions\nWe will talk extensively about Python functions in a few weeks. For now, suffice it to say that Python functions work just like mathematical functions: for example, note how we used the sin() function in the previous task. One piece of terminology that is somewhat specific to programming is the notion of calling- when we say we call a function on an argument, we mean that we’re passing that argument through the function. So, for example, in Task 5 we called the sin() function on the argument 1."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#variable-assignment",
    "href": "Pages/Labs/Lab01/lab01.html#variable-assignment",
    "title": "Lab01",
    "section": "Variable Assignment",
    "text": "Variable Assignment\nLet’s talk a bit about variables. Just like in math, variables in a programming language refer to a placeholder name for a particular piece of information (be it a function, value, etc.) The act of storing information in a variable is called assignment, and in Python variable assignment is performed using the = symbol.\n\n&lt;variable name&gt; = &lt;what you want to associate with the variable&gt;\n\nFor example, after running\n\nx = 2\n\nthe quantity x will always be synonymous with the quantity 2, and running x + 2 will return a value of 4 (as 2 + 2 = 4).\n\nPython affords a lot of flexibility when it comes to variable names- that is, we can pick almost anything we want to be a variable name! There are, however, some exceptions:\n\nVariable names cannot start with a number\nVariable names cannot include a space\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good programming practice to give your variables names that are descriptive, but not overly long.\n\n\nIf we want to view the value stored in a variable, we have two options: we could simply type the name of the variable, and run the cell:\n\nx\n\n2\n\n\nor we could pass the variable name into a call to the print() function:\n\nprint(x)\n\n2\n\n\n\n\n\n\n\n\nTask 6\n\n\n\n(a) Define a variable called my_variable, and assign it the value 5.\n(b) Now, run the command print(My_variable) (note the capitalization!)\n\n\n\n\n\n\n\n\nTip\n\n\n\nPython is case-sensitive.\n\n\nSometimes it will be necessary to update or re-assign a new value to an existing variable. For example, let’s examine the structure of the following code:\n\nx = 2\nx = x + 3\n\nWhat do you think running x will return? If you said 5, you’d be correct! The key point of this is:\n\n\n\n\n\n\nImportant\n\n\n\nIn variable assignment, Python starts by executing the righthand side of the equality before executing the lefthand side.\n\n\nSo, in code example above, Python first executed x + 3 (which is equivalent to 2 + 3; i.e. 5), and then re-assigned x the value 5."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#comments",
    "href": "Pages/Labs/Lab01/lab01.html#comments",
    "title": "Lab01",
    "section": "Comments",
    "text": "Comments\nWhen writing large pieces of code, programmers will often utilize comments to annotate their work and help readers understand what their code is doing. In Python there are two types of comments: inline comments and multiline comments. Inline comments are created using the hashtag (#) and multiline comments must be enclosed in three quotation marks (\"\"\"). As an example of both, consider the following snippet of code:\n\nx = 1             # define x\ny = 2             # define y\nz = (x + y) ** 2  # define z\ny = z / 3         # redefine y\n\n\"\"\"This code is defines 3 variables,\ncalled 'x', 'y', and 'z'.\"\"\"\n\n\n\n\n\n\n\nTask 7\n\n\n\nGo back and add some descriptive comments to some of your previous code cells. (You don’t need a separate markdown cell indicating you have done so.)"
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#data-types",
    "href": "Pages/Labs/Lab01/lab01.html#data-types",
    "title": "Lab01",
    "section": "Data Types",
    "text": "Data Types\nBefore closing out this lab, we should talk a bit about the quantities we assign to variables- i.e. the different data types in Python.\n\nThe term data type loosely refers to the actual type of a particular quantity (e.g. numerical, character, etc.) The main data types we will discuss in this lab are:\n\nfloat: refers to numerical (real-valued) quantities\nint: short for integer; refers to numerical quantities that are integers\nstr: short for string; refers to character- or text-type data (and will always be enclosed in either single quotation marks or double quotation marks)\n\n\n\n\n\n\n\nTask 8\n\n\n\nRun each of the following:\n\ntype(1)\ntype(1.1)\ntype(\"hello\")\n\n\n\nLet’s combine our knowledge of variable assignment with our newfound knowledge of data types!\n\n\n\n\n\n\nTask 9\n\n\n\n(a) Perform the following variable assignments:\n\ncourse = \"PSTAT 5A\"\nnum_sections = 4\nsection_capacity = 25\n\n(c) A new section has been added! Update the variable num_sections to be one more than when you initially defined it above. (Don’t just use num_sections = 5- think about our discussion on updating variables above!)\n(b) Using comments, write down what you think the output of each of the following expressions will be:\n\ntype(course)\ntype(num_sections)\nnum_sections * section_capacity\n\nThen, run each expression in a separate code chunk and comment on the results.\n(c) Create a new variable called course_capacity and assign it the value of the maximum capacity of the course. (Hint: there are only 5 sections, and each section has a maximum capacity of 25. Try to use your already-defined variables as much as possible!)\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe type() function can be used to identify the data type of a particular quantity."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#final-formatting",
    "href": "Pages/Labs/Lab01/lab01.html#final-formatting",
    "title": "Lab01",
    "section": "Final Formatting",
    "text": "Final Formatting\nIt’s time to start adding the finishing touches to our first lab!\n\n\n\n\n\n\nTask 10\n\n\n\n\nClick on the gear-shaped icon in the top-right of your console:\n\n\n\nScroll down until you see the Notebook Metadata:\n\n\n\nRight after the second-to-last brace (}), add a comma , and then the following code:\n\n\n\"authors\": [\n        {\n            \"name\": \"&lt;YOUR NAME&gt;\"\n        },\n        {\n            \"name\": \"&lt;YOUR NETID&gt;\"\n        }\n    ]\n\nwhere you replace &lt;YOUR NAME&gt; and &lt;YOUR NETID&gt; with your name and NetID, respectively. For example, after performing the above steps, my Notebook Metadata would look like:\n\nIMPORTANT: If your NetID contains an underscore, e.g. my_netid, then you need to replace the underscore with the text \\\\textunderscore (with the two backslashes at the beginning) followed by a space. Otherwise, your Lab will not convert to a PDF properly. So, for example, if your NetID is my_netid then you should write your NetID as my\\\\textunderscore netid. Your TA will go over how to update your Notebook Metadata in the last few minutes of Lab."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#what-to-turn-in",
    "href": "Pages/Labs/Lab01/lab01.html#what-to-turn-in",
    "title": "Lab01",
    "section": "What to Turn In",
    "text": "What to Turn In\nCongrats on finishing the first PSTAT 5A Computing Lab! Here’s what you need to submit:\n\nYour downloaded .ipynb file\nYour downloaded .PDF file\n\n(Please consult the video on Canvas showing you how to upload your work to Gradescope). Toward the end of Lab, your TA will show you how to download the above files. You will have until 40 minutes after the end of your Section (e.g. if your Section ends at 1:20pm, then you have until 2:00pm to submit) to turn in your work in order to get credit for the lab. Also, please remember that you need to upload both a .ipynb file and a .pdf. We will be grading labs based on effort, so just turn in what you are able to!"
  },
  {
    "objectID": "Pages/course_staff.html",
    "href": "Pages/course_staff.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Instructor: Ethan P. Marzban\n\n\n\n\n\n\n\n\n\n\n\n\nEthan P. Marzban\n\n\nHello! I currently just finished the 3rd year of my PhD program here in the PSTAT department, having joined back in 2020 (after having completed my undergraduate degree in Statistics as well). Outside of school I enjoy playing the piano, drinking boba, and talking about cats!\n\n\n\n\nEmail:\n\n\nepmarzban@pstat.ucsb.edu (Please use only in case of emergency)\n\n\n\n\nOH:\n\n\nTuesdays 4:30 - 5:30pm, over Zoom  Thursdays 2:30 - 3:30pm, in SH 5607F (the Sobel Seminar Hall)",
    "crumbs": [
      "Course Info",
      "Course Staff / OH"
    ]
  },
  {
    "objectID": "Pages/course_staff.html#instructor",
    "href": "Pages/course_staff.html#instructor",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Instructor: Ethan P. Marzban\n\n\n\n\n\n\n\n\n\n\n\n\nEthan P. Marzban\n\n\nHello! I currently just finished the 3rd year of my PhD program here in the PSTAT department, having joined back in 2020 (after having completed my undergraduate degree in Statistics as well). Outside of school I enjoy playing the piano, drinking boba, and talking about cats!\n\n\n\n\nEmail:\n\n\nepmarzban@pstat.ucsb.edu (Please use only in case of emergency)\n\n\n\n\nOH:\n\n\nTuesdays 4:30 - 5:30pm, over Zoom  Thursdays 2:30 - 3:30pm, in SH 5607F (the Sobel Seminar Hall)",
    "crumbs": [
      "Course Info",
      "Course Staff / OH"
    ]
  },
  {
    "objectID": "Pages/course_staff.html#teaching-assistants-tas",
    "href": "Pages/course_staff.html#teaching-assistants-tas",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Teaching Assistants (TAs)",
    "text": "Teaching Assistants (TAs)\n\n\n\n\n\n\n\nTA: Mengrui Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nMengrui Zhang\n\n\nMengrui is a PhD Student in the PSTAT Department.\n\n\n\n\nEmail:\n\n\nmengrui@ucsb.edu\n\n\n\n\nOH:\n\n\nMW, 4 - 6pm (SH 5431 W)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTA: Olivier Mulkin\n\n\n\n\n\n\n\n\n\n\n\n\nOlivier Mulkin\n\n\nOlivier is a PhD Student in the PSTAT Department.\n\n\n\n\nEmail:\n\n\nomulkin@ucsb.edu\n\n\n\n\nOH:\n\n\nT, 1 - 3pm (Zoom)",
    "crumbs": [
      "Course Info",
      "Course Staff / OH"
    ]
  },
  {
    "objectID": "Pages/syllabus.html",
    "href": "Pages/syllabus.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "WELCOME TO PSTAT 5A! I am very excited to introduce to you the wonderful fields of Statistics and Data Science. As our world becomes ever more saturated with data, the need for data literacy becomes increasingly important. By the end of this course, I hope you will be able to think critically about statistical studies and results, understand how data can be used to simultaneously inform and manipulate, and begin applying your new skills to your future endeavors. I am very much looking forward to a great quarter with all of you!\n— Mallory",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#lecture-information",
    "href": "Pages/syllabus.html#lecture-information",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Lecture Information",
    "text": "Lecture Information\n\n\n\n\n\n\nLecture Times and Locations\n\n\n\nM, T, W, Th: 8am - 9:20pm in HSSB 1173",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#course-staff",
    "href": "Pages/syllabus.html#course-staff",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Course Staff",
    "text": "Course Staff\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructor:\nMallory Wang\n\n\n\n\nEmail:\nmallorywang@ucsb.edu\n\n\nHelp Hours:\n\nTuesdays: 9:30am - 10:30am (TBD)\nThursdays: 9:30am - 10:30am (TBD)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTAs\n\n\n\nDaniel Silva  | (dcsilva@ucsb.edu) |\n\n\n\nHezhong Zhang  | (hzhang586@umail.ucsb.edu)",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#schedule-of-sections",
    "href": "Pages/syllabus.html#schedule-of-sections",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Schedule of Sections",
    "text": "Schedule of Sections\n\n\n\nTimes\nTA\nLocation\n\n\n\n\nMW 10:00 - 10:50pm\nDaniel Silva\nPHELPS 1513\n\n\nMW 11:00 - 11:50pm\nHezhong Zhang\nPHELPS 1513\n\n\nMW 12:00 - 12:50pm\nHezhong Zhang\nPHELPS 1513",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#course-description",
    "href": "Pages/syllabus.html#course-description",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Course Description",
    "text": "Course Description\nThe official description of this course, from the Course Catalog, is:\n\nIntroduction to data science. Concepts of statistical thinking. Topics include random variables, sampling distributions, hypothesis testing, correlation and regression. Visualizing, analyzing and interpreting real world data using Python. Computing labs required.",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#textbooks",
    "href": "Pages/syllabus.html#textbooks",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Textbook(s)",
    "text": "Textbook(s)\nThis quarter, we do not have a required textbook- the lecture slides and lab activities are designed to be self-sufficient. However, the following textbooks are highly recommended:\n\nOpenIntro: Statistics. David Diez, Mine Çetinkaya-Rundel, and Christopher D Barr. (free version, courtesy of the authors, available at https://leanpub.com/os)\nComputational and Inferential Thinking: The Foundations of Data Science. Ani Adhikari and John DeNero. (available at: https://www.inferentialthinking.com)\nStatClass (2nd Edition, Revised). Dawn E. Holmes and Lubella A. Lenaburg",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#course-components",
    "href": "Pages/syllabus.html#course-components",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Course Components",
    "text": "Course Components\nThe following are the assignments and metrics that will be used to compute your final grade in this course:\n\nLabs\nEvery Monday you will work through a Lab worksheet that will be posted to the website by the previous day. You will then have until 40 minutes after the end of your scheduled section to submit your work through Gradescope. Your TA will explain more about the structure of lab during your first Section meeting of the quarter.\n\n\nQuizzes\nQuizzes will be administered asynchronously on Fridays, through Gradescope. Specifically, the quiz will remain open from 8am until 11:59pm, and you must find 40 consecutive minutes to take the quiz. (Consecutive means you cannot start the quiz, and then come back to it later- once you start, you will have 40 minutes to both complete the quiz as well as upload your work). I encourage you to spend 25 minutes working on the quiz, and 15 minutes uploading. No quiz scores will be dropped. There are no quizzes in Exam Week.\n\n\nExam\nThere is a final exam for this class. You are required to take the final exam, which will take place during the last lecture.\n\n\nHomework:\nThere will be weekly homework due on Tuesday before lecture. Detail TBD.\n\n\nSchedule of Due Dates\nTBD",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#grading-scheme",
    "href": "Pages/syllabus.html#grading-scheme",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Grading Scheme",
    "text": "Grading Scheme\nYour final grade will be computed using your choice of the following weights:\n\n\n\nHomework:\n25%\n\n\nLabs:\n25%\n\n\nQuizzes:\n25%\n\n\nFinal Examination:\n25%\n\n\n\nor\n\n\n\nLabs:\n25%\n\n\nQuizzes:\n35%\n\n\nFinal Examination:\n40%\n\n\n\nPlease note that late submissions for any of the above will not be accepted. Additionally, make-up exams cannot be accommodated.\nYour final letter grade will be issued according to the following scheme (cutoffs between plusses and minuses will be calculated at the end of the quarter):\n\nA– – A+: 90 – 100%\nB– – B+: 80 – 89.99%\nC– – C+: 70 – 79.99%\nD– – D+ : 60 – 69.99%\nF: 0 – 59.99%\n\nI have elected to adopt an uncurved grading scheme to eliminate any sense of “competition” among students; I highly encourage you all to collaborate with and uplift each other. Having said that, I will certainly consider adjusting the cutoffs (naturally, in everyone’s favor) at the end of the quarter if necessary.",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#academic-integrity",
    "href": "Pages/syllabus.html#academic-integrity",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAs a member of the UCSB community, it is expected that you will act with academic integrity. This means, among other things, that the work you submit should be entirely your own and not copied from any external sources. Collaboration on non-quiz and non-exam assignments is perfectly acceptable (and even encouraged), but the work you submit should still be your own; you can’t have someone else write up solutions for you.\nAnyone found guilty of academic misconduct will be reported to the Academic Senate, and will receive at minimum a failing grade on the assignment in question; further actions may also include failing the course, and marks being made on permanent records. Depending on the severity of the infraction, expulsion is also a possibility.\nBasically, don’t cheat- please! If you’re ever struggling with course material, please come talk to me or the TA’s. We are truly here for you, and want only the best for you.",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#disabled-students-program-dsp",
    "href": "Pages/syllabus.html#disabled-students-program-dsp",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Disabled Students Program (DSP)",
    "text": "Disabled Students Program (DSP)\nIf you have a disability, or otherwise require accommodations for the exams and/or quizzes please reach out to the Disabled Students Program (DSP) ASAP to ensure your request(s) for accommodation can be processed. We ask that all requests be logged at least a week in advance, to ensure the system enough time to process. Please note that we cannot grant any requests for accommodations unless they come to us from DSP directly.",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#technology-needs",
    "href": "Pages/syllabus.html#technology-needs",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Technology Needs",
    "text": "Technology Needs\nAs a part of this course, you will be required to program in Python. Though the Lab Sections take place in specially designed classrooms that come equipped with computers, your homework and quizzes may cover Python-related questions, which means we expect you to have access to a laptop capable of connecting to the internet. If you do not currently possess such a laptop, please check out UCSB’s Basic Needs Resource page on Technology Resources to try and acquire one.",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#section-switching",
    "href": "Pages/syllabus.html#section-switching",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Section Switching",
    "text": "Section Switching\nAs mentioned above, Sections (both Discussion and Lab) take place in special “Collaborate Classrooms” which are equipped with laptops. There are a fixed number of seats and laptops in these classrooms, meaning we cannot under any circumstance over-enroll sections. Therefore, if you want to switch section unofficially (we do not have the ability to switch your official enrollment through GOLD), please follow the steps at this link. Any requests to switch sections that do not adhere to the guidelines posted at that link will be ignored.",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#email-policy",
    "href": "Pages/syllabus.html#email-policy",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Email Policy",
    "text": "Email Policy\nI will try my best to answer all emails within 48 hours. If you have not gotten a response from me in that time frame, please send me a followup as I probably missed it.",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/syllabus.html#disclaimer",
    "href": "Pages/syllabus.html#disclaimer",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Disclaimer",
    "text": "Disclaimer\nThis syllabus and much of the content for this course has been graciously borrowed from the original work of Ethan P. Marzban with some modifications. Thank you, Ethan!",
    "crumbs": [
      "Course Info",
      "Syllabus"
    ]
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#setup",
    "href": "Pages/Lectures/FinRev/FinRev.html#setup",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Setup",
    "text": "Setup\n\nConsider a population, governed by some parameter \\(\\theta\\) (e.g. a mean \\(\\mu\\), a variance \\(\\sigma^2\\), a proportion \\(p\\), etc.)\nSuppose we have a null hypothesis that \\(\\theta = \\theta_0\\) (for some specified and fixed value \\(\\theta_0\\)), along with an alternative hypothesis.\nThe goal of hypothesis testing is to use data (in the form of a representative sample taken from the population), and determine whether or not this data leads credence to the null in favor of the alternative.\n\nRecall that there are four main types of alternatives we could adopt: two-sided, lower-tailed, upper-tailed, and simple-vs-simple."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#testing-a-mean",
    "href": "Pages/Lectures/FinRev/FinRev.html#testing-a-mean",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Testing a Mean",
    "text": "Testing a Mean\n\nBefore MT2, we discussed the framework of hypothesis testing a population proportion p.\nAfter MT2, we discussed how to perform hypothesis testing on a population mean \\(\\mu\\).\nLet’s, for the moment, consider a two-sided test: \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\mu = \\mu_0   \\\\\nH_A:    & \\mu \\neq \\mu_0\n\\end{array} \\right. \\]\nSince we know that \\(\\overline{X}\\), the sample mean, is a relatively good point estimator of a population mean \\(\\mu\\), we know that our test statistic should involve \\(\\overline{X}\\) in some way."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#testing-a-mean-1",
    "href": "Pages/Lectures/FinRev/FinRev.html#testing-a-mean-1",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Testing a Mean",
    "text": "Testing a Mean\n\nSpecifically, we know that our test statistics are usually standardized versions of point estimators. As such, it is tempting to adopt \\[ \\mathrm{TS} = \\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}} \\] as, under certain conditions, this follows a standard normal distribution under the null (i.e. when assuming the true population mean \\(\\mu\\) is in fact \\(\\mu_0\\)): \\[ \\mathrm{TS} = \\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}} \\stackrel{H_0}{\\sim} \\mathcal{N}(0, \\ 1) \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#testing-a-mean-2",
    "href": "Pages/Lectures/FinRev/FinRev.html#testing-a-mean-2",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Testing a Mean",
    "text": "Testing a Mean\n\nBut, we won’t always have access to the true population standard deviation \\(\\sigma\\)! Rather, sometimes we only have access to \\(s_X\\), the sample standard deviation.\nThis leads to the following test statistic: \\[ \\mathrm{TS} = \\frac{\\overline{X} - \\mu_0}{s_X / \\sqrt{n}} \\] which now no longer follows the standard normal distribution under the null, but rather a t-distribution with \\(n - 1\\) degrees of freedom: \\[ \\mathrm{TS} = \\frac{\\overline{X} - \\mu_0}{s_X / \\sqrt{n}} \\stackrel{H_0}{\\sim} t_{n - 1} \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#sampling-distribution-of-overlinex",
    "href": "Pages/Lectures/FinRev/FinRev.html#sampling-distribution-of-overlinex",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Sampling Distribution of \\(\\overline{X}\\)",
    "text": "Sampling Distribution of \\(\\overline{X}\\)\n\n\n\n\n\n\ngraph TB\n  A[Is the population Normal?  . ] --&gt; |Yes| B{{Use Normal .}}\n  A --&gt; |No| C[Is n &gt;= 30?  .]\n  C --&gt; |Yes| D[sigma or s?  .]\n  C --&gt; |No| E{{cannot proceed   .}}\n  D --&gt; |sigma| F{{Use Normal .}}\n  D --&gt; |s| G{{Use t }}"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#test-statistic",
    "href": "Pages/Lectures/FinRev/FinRev.html#test-statistic",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nSo, to summarize, our test statistic is: \\[ \\mathrm{TS} = \\begin{cases}\n\\displaystyle \\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}}    & \\text{if } \\quad  \\begin{array}{rl} \\bullet & \\text{pop. is normal, OR} \\\\ \\bullet & \\text{$n \\geq 30$ AND $\\sigma$ is known} \\end{array} \\quad \\stackrel{H_0}{\\sim} \\mathcal{N}(0, \\ 1) \\\\[5mm]\n\\displaystyle \\frac{\\overline{X} - \\mu_0}{s / \\sqrt{n}}         & \\text{if } \\quad  \\begin{array}{rl} \\bullet & \\text{$n \\geq 30$ AND $\\sigma$ is not known} \\end{array} \\quad \\stackrel{H_0}{\\sim} t_{n - 1}\n\\end{cases} \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#test",
    "href": "Pages/Lectures/FinRev/FinRev.html#test",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Test",
    "text": "Test\n\nRecall our null and alternative hypotheses: \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\mu = \\mu_0   \\\\\nH_A:    & \\mu \\neq \\mu_0\n\\end{array} \\right. \\]\nIf an observed instance of \\(\\overline{X}\\) is much larger than \\(\\mu_0\\), we are more inclined to believe the alternative over the null.\n\nIn other words, we would reject \\(H_0\\) for large positive values of \\(\\mathrm{TS}\\).\n\nHowever, we would also be more inclined to believe the alternative over the null if an observed instance of \\(\\overline{X}\\) was much smaller than \\(\\mu_0\\).\n\nIn other words, we would reject \\(H_0\\) for large negative values of \\(\\mathrm{TS}\\) as well."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#test-1",
    "href": "Pages/Lectures/FinRev/FinRev.html#test-1",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Test",
    "text": "Test\n\nWe combine these two cases using absolute values: \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| &gt; c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] for some critical value \\(c\\).\n\nThe critical value will depend not only on the confidence level, but also the sampling distribution of \\(\\overline{X}\\).\nSpecifically, as we have previously seen, it will be the appropriate percentile (“appropriate” as dictated by the confidence level) of either the \\(\\mathcal{N}(0, \\ 1)\\) distribution or the \\(t_{n - 1}\\) distribution."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#critical-value",
    "href": "Pages/Lectures/FinRev/FinRev.html#critical-value",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Critical Value",
    "text": "Critical Value\n\n\nThe critical value is the positive value along the x-axis that makes the blue shaded region equal to \\(\\alpha\\)."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#p-values",
    "href": "Pages/Lectures/FinRev/FinRev.html#p-values",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "p-Values",
    "text": "p-Values\n\nWe also saw how, instead of looking at critical values, we can also look at p-values.\nThe p-value is the probability of observing something as or more extreme (in the directino of the alternative) than what we currently observe.\nAs such, p-values that are smaller than the level of significance lead credence to the alternative over the null; i.e. we reject whenever \\(p &lt; \\alpha\\).\n\nNote this means that the way we compute p-values depends on the type of test (i.e. two-sided, lower-tailed, or upper-tailed) that we are conducting."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#p-value-lower-tailed-test",
    "href": "Pages/Lectures/FinRev/FinRev.html#p-value-lower-tailed-test",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "p-value; Lower-Tailed Test",
    "text": "p-value; Lower-Tailed Test"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#p-value-upper-tailed-test",
    "href": "Pages/Lectures/FinRev/FinRev.html#p-value-upper-tailed-test",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "p-value; Upper-Tailed Test",
    "text": "p-value; Upper-Tailed Test"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#p-value-two-sided-test",
    "href": "Pages/Lectures/FinRev/FinRev.html#p-value-two-sided-test",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "p-value; Two-Sided Test",
    "text": "p-value; Two-Sided Test"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#worked-out-example",
    "href": "Pages/Lectures/FinRev/FinRev.html#worked-out-example",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nA city official claims that the average monthly rent of a 1 bedroom apartment in GauchoVille is $1.1k. To test this claim, a representative sample of 37 1 bedroom apartments is taken; the average monthly rent of these 37 apartments is found to be $1.21k and the standard deviation of these 37 apartments is found to be 0.34. Assume we are conducting a two-sided test with a 5% level of significance.\n\nDefine the parameter of interest.\nState the null and alternative hypotheses.\nCompute the value of the test statistic.\nAssuming the null is correct, what is the distribution of the test statistic?\nWhat is the critical value of the test?\nConduct the test, and phrase your conclusions in the context of the problem.\nWhat code would we use to compute the p-value? Would we expect this value to be less than or greater than 5%?"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#solutions",
    "href": "Pages/Lectures/FinRev/FinRev.html#solutions",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Solutions",
    "text": "Solutions\n\n\\(\\mu =\\) average monthly cost of a 1 bedroom apartment in GauchoVille.\n\n\\[\\left[ \\begin{array}{rr}\n  H_0:    & \\mu = 1.1   \\\\\n  H_A:    & \\mu \\neq 1.1\n\\end{array} \\right. \\]\n\nSince we do not have access to the population standard deviation, we use \\[ \\mathrm{TS} = \\frac{\\overline{X} - \\mu_0}{s / \\sqrt{n}} = \\frac{1.21 - 1.1}{0.34 / \\sqrt{37}} = \\boxed{ 1.97 } \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#solutions-1",
    "href": "Pages/Lectures/FinRev/FinRev.html#solutions-1",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Solutions",
    "text": "Solutions\n\nWe ask:\n\nIs the population normally distributed (i.e. are housing prices in GauchoVille normally distributed)? No.\nIs our sample size large enough? Yes; \\(n = 37 \\geq 30\\).\nDo we have \\(\\sigma\\) or \\(s\\)? We have \\(s\\).\n\n\n\nTherefore, we use a t-distribution with \\(n - 1 = 37 - 1 = 36\\) degrees of freedom: \\[ \\boxed{\\mathrm{TS} \\stackrel{H_0}{\\sim} t_{36} } \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#solutions-2",
    "href": "Pages/Lectures/FinRev/FinRev.html#solutions-2",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Solutions",
    "text": "Solutions\n\nFrom the t-table provided on the website (which will also be provided to you during the exam), the critical value is \\(\\boxed{2.03}\\).\n\nSince \\(|\\mathrm{TS}| = |1.97| = 1.97 &lt; 2.03\\), we fail to reject the null:\n\n\n\nAt a 5% level of significance, there was insufficient evidence to reject the null hypothesis that the true monthly cost of a 1-bedroom apartment in GauchoVille is $1.1k in favor of the alternative that the true cost is not $1.1k.\n\n\n\nThe code we would use, after importing scipy.stats, is 2 * scipy.stats.t.cdf(-1.97, 36), which we would expect to be larger than 5% as we failed to reject based on the critical value, and we only reject when p is less than \\(\\alpha\\) (which is 5% for this problem)."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#two-samples",
    "href": "Pages/Lectures/FinRev/FinRev.html#two-samples",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Two Samples",
    "text": "Two Samples\n\nThe above discussion was in regards to a single sample, taken from a single population.\nWhat happens if we have two populations, goverend by parameters \\(\\theta_1\\) and \\(\\theta_2\\).\nFor example, suppose we want to compare the average air pollution in Santa Barbara to that in Los Angeles.\nThat is, given two populations (Population 1 and Population 2) with population means \\(\\mu_1\\) and \\(\\mu_2\\), we would like to test some claim involving both \\(\\mu_1\\) and \\(\\mu_2\\)."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#two-samples-1",
    "href": "Pages/Lectures/FinRev/FinRev.html#two-samples-1",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Two Samples",
    "text": "Two Samples\n\nFor this class, we only ever consider a null of the form \\(H_0: \\mu_1 = \\mu_2\\); i.e. that the two populations have the same average.\nWe do still have three alternative hypotheses available to us:\n\n\\(H_A: \\ \\mu_1 \\neq \\mu_2\\)\n\\(H_A: \\ \\mu_1 &lt; \\mu_2\\)\n\\(H_A: \\ \\mu_1 &gt; \\mu_2\\)\n\nRemember that the trick is to reparameterize everything to be in terms of a difference of parameters, thereby reducing the two-parameter problem into a one-parameter problem."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#two-sided",
    "href": "Pages/Lectures/FinRev/FinRev.html#two-sided",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Two-Sided",
    "text": "Two-Sided\n\nFor example, suppose we are testing the following hypotheses: \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\mu_1 = \\mu_2   \\\\\nH_A:    & \\mu_1 \\neq \\mu_2\n\\end{array} \\right. \\]\nWe can define \\(\\delta = \\mu_2 - \\mu_1\\), and equivalently re-express our hypotheses as \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\delta = 0   \\\\\nH_A:    & \\delta \\neq 0\n\\end{array} \\right. \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#test-statistic-1",
    "href": "Pages/Lectures/FinRev/FinRev.html#test-statistic-1",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nNow, we need some sort of test statistic.\nSuppose we have a (representative) sample \\(X = \\{x_i\\}_{i=1}^{n_1}\\) from Population 1 and a (representative) sample \\(Y = \\{y_i\\}_{i=1}^{n_2}\\) from Population 2 (note the potentially different sample sizes!)\nWe have an inkling that a decent point estimator for \\(\\delta = \\mu_2 - \\mu_1\\) is \\(\\widehat{\\delta} = \\overline{Y} - \\overline{X}\\).\nOur test statistic will be some standardized form of \\(\\widehat{\\delta}\\), meaning we need to find \\(\\mathbb{E}[\\widehat{\\delta}]\\) and \\(\\mathrm{SD}(\\widehat{\\delta})\\)."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#linear-combinations-of-random-variables",
    "href": "Pages/Lectures/FinRev/FinRev.html#linear-combinations-of-random-variables",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Linear Combinations of Random Variables",
    "text": "Linear Combinations of Random Variables\n\nOur two main results are:\n\n\\(\\mathbb{E}[aX + bY + c] = a \\cdot \\mathbb{E}[X] + b \\cdot \\mathbb{E}[Y] + c\\)\n\\(\\mathrm{Var}(aX + bY + c) = a^2 \\cdot \\mathrm{Var}(X) + b^2 \\cdot \\mathrm{Var}(Y)\\), for independent random variables \\(X\\) and \\(Y\\).\n\nSince \\(\\mathbb{E}[\\overline{Y}] = \\mu_2\\) and \\(\\mathbb{E}[\\overline{X}] = \\mu_1\\), we have that \\[\\begin{align*}\n\\mathbb{E}[\\widehat{\\delta}]    & = \\mathbb{E}[\\overline{Y} - \\overline{X}]    \\\\\n  & = \\mathbb{E}[\\overline{Y}] - \\mathbb{E}[\\overline{X}]   \\\\\n  & = \\mu_2 - \\mu_1 = \\delta\n\\end{align*}\\] which effectively shows that \\(\\widehat{\\delta}\\) is a “good” point estimator of \\(\\delta\\)."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#linear-combinations-of-random-variables-1",
    "href": "Pages/Lectures/FinRev/FinRev.html#linear-combinations-of-random-variables-1",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Linear Combinations of Random Variables",
    "text": "Linear Combinations of Random Variables\n\nAdditionally, since \\[ \\mathrm{Var}(\\overline{X}) = \\frac{\\sigma_1^2}{n_1}; \\qquad \\mathrm{Var}(\\overline{Y}) = \\frac{\\sigma_2^2}{n_2} \\] we have \\[\\begin{align*}\n\\mathrm{Var}(\\widehat{\\delta})    & = \\mathrm{Var}(\\overline{Y} - \\overline{X})   \\\\\n  & = \\mathrm{Var}(\\overline{Y}) + \\mathrm{Var}(\\overline{X})   \\\\\n  & = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#linear-combinations-of-random-variables-2",
    "href": "Pages/Lectures/FinRev/FinRev.html#linear-combinations-of-random-variables-2",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Linear Combinations of Random Variables",
    "text": "Linear Combinations of Random Variables\n\nAlso remember how linear combinations of normally-distributed random variables work: if \\(X \\sim \\mathcal{N}(\\mu_X, \\ \\sigma_X)\\) and \\(Y \\sim \\mathcal{N}(\\mu_Y, \\ \\sigma_Y)\\) with \\(X \\perp Y\\) then \\[ (aX + bY + c) \\sim \\mathcal{N}\\left( a \\mu_X + b \\mu_Y + c, \\ \\sqrt{a^2 \\sigma_X^2 + b^2 \\sigma_Y^2} \\right) \\]\nSee, for example, Problem 3 from the practice problem set."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#test-statistic-2",
    "href": "Pages/Lectures/FinRev/FinRev.html#test-statistic-2",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nThis led us to consider the following test statistic: \\[ \\mathrm{TS}_1 = \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\] which, under the null, would follow a standard normal distribution if \\(\\overline{X}\\) and \\(\\overline{Y}\\) both followed a normal distribution.\nHowever, in many situations, we won’t have access to the population variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\). Rather, we will only have access to the sample variances \\(s_X^2\\) and \\(s_Y^2\\). Hence, we modify our test statistic to be of the form \\[ \\mathrm{TS} = \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{\\frac{s_X^2}{n_1} + \\frac{s_Y^2}{n_2}}} \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#distribution-of-the-test-statistic",
    "href": "Pages/Lectures/FinRev/FinRev.html#distribution-of-the-test-statistic",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Distribution of the Test Statistic",
    "text": "Distribution of the Test Statistic\n\nThis statistic is no longer normally distributed under the null.\nIt approximately follows a t distribution with degrees of freedom given by the Satterthwaite Approximation: \\[ \\mathrm{df} = \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{s_X^2}{n_1} \\right) + \\left( \\frac{s_Y^2}{n_2} \\right) \\right]^2 }{ \\frac{\\left( \\frac{s_X^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_Y^2}{n_2} \\right)^2}{n_2 - 1} } \\right\\} \\]\nThat is; \\[ \\mathrm{TS} \\stackrel{H_0}{\\sim} t_{\\mathrm{df}}; \\quad \\text{df given by above}\\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#test-2",
    "href": "Pages/Lectures/FinRev/FinRev.html#test-2",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Test",
    "text": "Test\n\nIf we are conducting a two-sided hypothesis test, then both large positive values and large negative values of our test statistic would lead credence to the null over the alternative.\n\nAs such, our test would reject for large values of \\(|\\mathrm{TS}|\\)\n\nIf instead our alternative took the form \\(\\mu_1 &lt; \\mu_2\\); i.e. that \\(\\delta = \\mu_2 - \\mu_1 &gt; 0\\), our test would reject for large positive values of \\(\\mathrm{TS}\\).\nIf instead our alternative took the form \\(\\mu_1 &gt; \\mu_2\\); i.e. that \\(\\delta = \\mu_2 - \\mu_1 &lt; 0\\), our test would reject for large negative values of \\(\\mathrm{TS}\\).\nAgain, the key is to note that after reparameterizing the problem to be in terms of the difference \\(\\delta = \\mu_2 - \\mu_1\\), the problem becomes a familiar one-parameter problem."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#worked-out-example-2",
    "href": "Pages/Lectures/FinRev/FinRev.html#worked-out-example-2",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nA renter wants to know which city is cheaper to live in: GauchoVille or Bruin City. Specifically, she would like to test the null hypothesis that the two cities have the same average monthly rent against the alternative that GauchoVille has a higher average monthly rent.\n\nAs such, she takes a representative sample of 32 houses from GauchoVille (which she calls Population 1) and 32 houses from Bruin City (which she calls Population 2), and records the following information about her samples (all values are reported in thousands of dollars):\n\\[\\begin{array}{r|cc}\n                    & \\text{Sample Average}     & \\text{Sample Standard Deviation}    \\\\\n  \\hline\n  \\textbf{GauchoVille}  &     3.2                    & 0.50        \\\\\n  \\textbf{Bruin City}   &     3.5                    & 0.60\n\\end{array}\\]\n\nWrite down the null and alternative hypotheses, taking care to define any relevant parameter(s).\nCompute the value of the test statistic.\nAssuming the null is correct, what is the distribution of the test statistic? Be sure to include any/all relevant parameter(s)! (Assume all independence and normality conditions are met.)\nWhat is the critical value of the test, if we are to use a 5% level of significance?\nConduct the relevant test at a 5% level of significance, and report your conclusions in the context of the problem."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#solutions-3",
    "href": "Pages/Lectures/FinRev/FinRev.html#solutions-3",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Solutions",
    "text": "Solutions\n\nLet \\(\\mu_1\\) denote the true average monthly rent in Population 1 (GauchoVille) and let \\(\\mu_2\\) denote the true average monthly rent in Population 2 (Bruin City). Then, the null and alternative hypotheses can be phrased as: \\[ \\left[ \\begin{array}{rr}\n  H_0:    & \\mu_1 = \\mu_2   \\\\\n  H_A:    & \\mu_1 &lt; \\mu_2\n\\end{array} \\right. \\] which, phrased in terms of the difference \\(\\mu_2 - \\mu_1\\), is equivalent to \\[ \\left[ \\begin{array}{rr}\n  H_0:    & \\mu_2 - \\mu_1 = 0   \\\\\n  H_A:    & \\mu_2 - \\mu_1 &gt; 0\n\\end{array} \\right. \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#solutions-4",
    "href": "Pages/Lectures/FinRev/FinRev.html#solutions-4",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Solutions",
    "text": "Solutions\n\nWe compute\n\n\n\\[\\begin{align*}\n  \\mathrm{TS}   & = \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{\\frac{s_X^2}{n_1} + \\frac{s_Y^2}{n_2}}} \\\\\n    & = \\frac{3.5 - 3.2}{\\sqrt{\\frac{0.5^2}{32} + \\frac{0.6^2}{32}  }} \\approx \\boxed{2.173}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#solutions-5",
    "href": "Pages/Lectures/FinRev/FinRev.html#solutions-5",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Solutions",
    "text": "Solutions\n\nWe know that, under the null, the test statistic (assuming all independence and normality conditions hold) follows a t-distribution with degrees of freedom given by the Satterthwaite Approximation. As such, we should first compute the degrees of freedom:\n\n\n\\[\\begin{align*}\n  \\mathrm{df}   & = \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{s_X^2}{n_1} \\right) + \\left( \\frac{s_Y^2}{n_2} \\right) \\right]^2 }{ \\frac{\\left( \\frac{s_X^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_Y^2}{n_2} \\right)^2}{n_2 - 1} } \\right\\} \\\\\n    & = \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{0.5^2}{32} \\right) + \\left( \\frac{0.6^2}{32} \\right) \\right]^2 }{ \\frac{\\left( \\frac{0.5^2}{32} \\right)^2}{32 - 1} + \\frac{\\left( \\frac{0.6^2}{32} \\right)^2}{32 - 1} } \\right\\} \\\\\n    & = \\mathrm{round}\\{60.04737\\} = 60\n\\end{align*}\\]\n\n\nTherefore, \\(\\mathrm{TS} \\stackrel{H_0}{\\sim} t_{60}\\)"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#solutions-6",
    "href": "Pages/Lectures/FinRev/FinRev.html#solutions-6",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Solutions",
    "text": "Solutions\n\nRecall that we have an upper-tailed alternative. As such, the critical value will be the \\((1 - 0.05) \\times 100 = 95\\)th percentile of the \\(t_{60}\\) distribution. From our table, we see that this is \\(\\boxed{1.67}\\).\nWe reject when our test statistic is larger than the critical value (again, since we are using an upper-tailed alternative). Since \\(\\mathrm{TS} = 2.173 &gt; 1.67\\), we reject the null:\n\n\n\nAt a 5% level of significance, there was sufficient evidence to reject the null that the average monthly rent in the two cities is the same against the alternative that the average monthly rent in Bruin City is higher than that in GauchoVille.\n\n\n\nQuick aside: do you think it was a valid assumption to make that the “normality conditions” hold?"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#multiple-populations",
    "href": "Pages/Lectures/FinRev/FinRev.html#multiple-populations",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Multiple Populations",
    "text": "Multiple Populations\n\nSuppose, instead of comparing two population means, we compare k population means \\(\\mu_1, \\cdots, \\mu_k\\).\nThis is one framework in which ANOVA (Analysis of Variance) is useful.\nGiven \\(k\\) populations, each assumed to be normally distributed, with means \\(\\mu_1, \\cdots, \\mu_k\\), ANOVA tests the following hypotheses: \\[ \\left[ \\begin{array}{rl}\nH_0:    & \\mu_1 = \\mu_2 = \\cdots = \\mu_k    \\\\\nH_A:    & \\text{at least one of the $\\mu_i$'s is different from the others}\n\\end{array} \\right. \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#anova",
    "href": "Pages/Lectures/FinRev/FinRev.html#anova",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "ANOVA",
    "text": "ANOVA\n\nSpecifically, ANOVA utilizes the so-called F-statistic \\[ \\mathrm{F} = \\frac{\\mathrm{MS}_{\\mathrm{G}}}{\\mathrm{MS}_{E}} \\] where \\(\\mathrm{MS}_{\\mathrm{G}}\\), the mean square between groups, can be thought of as a measure of variability between group means, and \\(\\mathrm{MS}_{\\mathrm{E}}\\), the mean squared error, can be thought of as a measure of variability within groups/variability due to chance.\nIf \\(\\mathrm{MS}_{\\mathrm{G}}\\) is much larger than \\(\\mathrm{MS}_{\\mathrm{E}}\\) - i.e. if the variability between groups is much more than what we would expect due to chance alone - we would likely reject the null that all group means were the same.\n\nAs such, ANOVA rejects for values of the F-statistic that are large (i.e. much greater than 1)."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#anova-1",
    "href": "Pages/Lectures/FinRev/FinRev.html#anova-1",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "ANOVA",
    "text": "ANOVA\n\nAssuming the \\(k\\) populations follow independent normal distributions, the F-statistic follows an F-distribution under the null.\n\nSpecifically, \\(F \\sim F_{k-1, \\ n - k}\\) where \\(n\\) is the total number of observations across all groups.\n\nSince we reject \\(H_0\\) (in favor of \\(H_A\\)) whenever \\(F\\) is large, we always compute p-values in ANOVA using right-tail probabilities:"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#anova-2",
    "href": "Pages/Lectures/FinRev/FinRev.html#anova-2",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "ANOVA",
    "text": "ANOVA\n\nThe results of an ANOVA are typically displayed by way of an ANOVA Table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDF\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nBetween Groups\n\\(k - 1\\)\n\\(\\mathrm{SS}_{\\mathrm{G}}\\)\n\\(\\mathrm{MS}_{\\mathrm{G}}\\)\nF\np-value\n\n\nResiduals\n\\(n - k\\)\n\\(\\mathrm{SS}_{\\mathrm{E}}\\)\n\\(\\mathrm{MS}_{\\mathrm{E}}\\)\n\n\n\n\n\n\n\n\n\nYou should familiarize yourself with how these quantities relate to each other; specifically, that \\[ \\mathrm{MS}_{\\mathrm{G}} = \\frac{\\mathrm{SS}_{\\mathrm{G}}}{k - 1} ; \\quad \\mathrm{MS}_{\\mathrm{E}} = \\frac{\\mathrm{SS}_{\\mathrm{E}}}{n - k}; \\quad F = \\frac{\\mathrm{MS}_{\\mathrm{G}}}{\\mathrm{MS}_{\\mathrm{E}}} \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#associations-and-correlations",
    "href": "Pages/Lectures/FinRev/FinRev.html#associations-and-correlations",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Associations and Correlations",
    "text": "Associations and Correlations\n\nRecall, from Week 1, that a scatterplot is a good way to visualize the relationship between two numerical variables x and y.\nTwo variables can have either a positive or a negative relationship/association, along with a linear or nonlinear one.\n\n“Positive” means a one-unit increase in x translates to an increase in y\n“Negative” means a one-unit increase in x translates to an degrease in y\n“Linear” means the rate of change is fixed (i.e. constant)\n“Nonlinear” means the rate of change depends on x"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#no-relationship",
    "href": "Pages/Lectures/FinRev/FinRev.html#no-relationship",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "No Relationship",
    "text": "No Relationship\n\nSometimes, two variables will have no relationship at all:"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#strength-of-a-relationship",
    "href": "Pages/Lectures/FinRev/FinRev.html#strength-of-a-relationship",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Strength of a Relationship",
    "text": "Strength of a Relationship"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#pearsons-r",
    "href": "Pages/Lectures/FinRev/FinRev.html#pearsons-r",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Pearson’s r",
    "text": "Pearson’s r\n\nPearson’s r (or just the correlation coefficient) is a metric used to quantify the strength and direction of a linear relationship between two variables.\nGiven variables x and y (whose elements are denoted using the familiar notation we’ve been using throughout this course), we compute r using \\[ r = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\overline{x}}{s_X} \\right) \\left( \\frac{y_i - \\overline{y}}{s_Y} \\right)  \\]\nRecall that \\(-1 \\leq r \\leq 1\\) for any two variables x and y.\n\nFurthermore, r will only ever be \\(-1\\) or \\(1\\) exactly when the points in the scatterplot fall perfectly on a line."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#statistical-modeling",
    "href": "Pages/Lectures/FinRev/FinRev.html#statistical-modeling",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Statistical Modeling",
    "text": "Statistical Modeling\n\nWe may also want to model the relationship between x and y.\nSpecifically, given a response variable y and an explanatory variable x, a statistical model asserts that x and y are related according to \\[ \\texttt{y} = f(\\texttt{x}) + \\texttt{noise} \\] where \\(f()\\) is called the signal function\n\nBy the way: on a scatterplot, the response variable will always appear on the vertical axis and the explanatory variable will appear on the horizontal axis.\n\nIf the response variable is numerical, we call the model a regression model. If the response variable is categorical, we call the model a classification model."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#simple-linear-regression",
    "href": "Pages/Lectures/FinRev/FinRev.html#simple-linear-regression",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nSimple Lineaer Regression refers to a situation in which:\n\nWe have a numerical response y\nWe have a single explanatory variable x\nWe assume a linear form for the signal function; i.e. \\(f(x) = \\beta_0 + \\beta_1 x\\)\n\nThis leads to the model \\[ \\texttt{y} = \\beta_0 + \\beta_1 \\cdot \\texttt{x} + \\texttt{noise} \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#simple-linear-regression-1",
    "href": "Pages/Lectures/FinRev/FinRev.html#simple-linear-regression-1",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nNow, the noise part of our model makes it impossible to know the true values of \\(\\beta_0\\) and \\(\\beta_1\\).\n\nIn this way, we can think of them as population parameters.\n\nAs such, we seek to find point estimators \\(\\widehat{\\beta_0}\\) and \\(\\widehat{\\beta_1}\\) that best estimate \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\nTo quantify what we mean by “best”, we employed the condition of minimizing the residual sum of squares.\n\nEffectively, this means finding the line \\(\\widehat{\\beta_0} + \\widehat{\\beta_1} \\cdot \\texttt{x}\\) that minimizes the average distance between the points in the dataset and the line."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#regression",
    "href": "Pages/Lectures/FinRev/FinRev.html#regression",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Regression",
    "text": "Regression"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#regression-1",
    "href": "Pages/Lectures/FinRev/FinRev.html#regression-1",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Regression",
    "text": "Regression\n\nSuch estimators (i.e. those that minimize the RSS) are said to be ordinary least squares (OLS) estimates.\n\nThe resulting line \\(\\widehat{\\beta_0} + \\widehat{\\beta_1} \\cdot \\texttt{x}\\) is thus called the OLS Regression Line\n\nIt turns out that the OLS estimates of \\(\\beta_0\\) and \\(\\beta_1\\) are: \\[\\begin{align*}\n\\widehat{\\beta_1}   & = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^{n} (x - \\overline{x})^2} = \\frac{s_Y}{s_X} \\cdot r \\\\\n\\widehat{\\beta_0}   & = \\overline{y} - \\widehat{\\beta_1} \\cdot \\overline{x}\n\\end{align*}\\] where r denotes Pearson’s Correlation Coefficient \\[ r = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\overline{x}}{s_X} \\right) \\left( \\frac{y_i - \\overline{y}}{s_Y} \\right)  \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#regression-2",
    "href": "Pages/Lectures/FinRev/FinRev.html#regression-2",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Regression",
    "text": "Regression\n\nThe values along the OLS regression line corresponding to x values observed in the dataset are called fitted values:\n\n\n\n\n\nIn a sense, the fitted values represent guess/estimate of the de-noised value of y"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#regression-3",
    "href": "Pages/Lectures/FinRev/FinRev.html#regression-3",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Regression",
    "text": "Regression\n\nWe can use the OLS regression line to perform prediction; i.e. to infer response values associated with explanatory values that were not included in the original dataset."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#example",
    "href": "Pages/Lectures/FinRev/FinRev.html#example",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\nExercise 1\n\n\n\n\nAn airline is interested in determining the relationship between flight duration (in minutes) and the net amount of soda consumed (in oz.). Letting x denote flight duration (the explanatory variable) and y denote amount of soda consumed (the response variable), a sample yielded the following results: \\[ \\begin{array}{cc}\n  \\displaystyle \\sum_{i=1}^{102} x_i  = 20,\\!190.55;   & \\displaystyle \\sum_{i=1}^{102} (x_i - \\overline{x})^2 =  101,\\!865    \\\\\n  \\displaystyle \\sum_{i=1}^{102} y_i  = 166,\\!907.8   & \\displaystyle \\sum_{i=1}^{102} (y_i - \\overline{y})^2 =  120,\\!794.2   \\\\\n\\displaystyle \\sum_{i=1}^{102} (x_i - \\overline{x})(y_i - \\overline{y}) = 80,\\!184.62 \\\\\n\\end{array} \\]\n\nFind the equation of the OLS Regression line.\nIf a particular flight has a duration of 130 minutes, how many ounces of soda would we expect to be consumed on the flight? (Suppose the \\(x-\\)observations ranged from around \\(114\\) to around \\(271\\))"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#solutions-7",
    "href": "Pages/Lectures/FinRev/FinRev.html#solutions-7",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Solutions",
    "text": "Solutions\n\n\\[\\begin{align*}\n  \\widehat{\\beta_1}   & = \\frac{\\sum_{i=1}^{n}(x_i - \\overline{x})(y_i - \\overline{y})} {\\sum_{i=1}^{n} (x_i - \\overline{x})^2} = \\frac{80,\\!184.62}{101,\\!865} \\approx \\boxed{0.7872}   \\\\\n  \\widehat{\\beta_0}   & = \\overline{y} - \\widehat{\\beta_1} \\cdot \\overline{x} = \\boxed{1510.138}\n\\end{align*}\\] Therefore, \\[ \\widehat{(\\texttt{amt. of soda})} = 1510.138 + (0.7872) \\cdot (\\texttt{flight duration}) \\]\n\n\n\n\n\\(\\widehat{y}^{(130)} = 1510.138 + (0.7872) \\cdot (130)  = \\boxed{1612.474 \\text{ oz.}}\\)"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#extrapolation",
    "href": "Pages/Lectures/FinRev/FinRev.html#extrapolation",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nRemember that it is dangerous to try and use the OLS regression line to predict response values for explanatory variables that are far outside of the scope of the original data.\nFor example, since the dataset in the previous example only included flights between 114 minutes and 271 minutes, it would be dangerous to try to predict the amount of soda that would be consumed on a 13-hr flight (780 mins) using the OLS regression line, as we cannot be certain that the relationship between amt. of soda and flight duration remains linear for larger values of flight duration.\nRecall that this relates to extrapolation."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#inference-on-the-slope",
    "href": "Pages/Lectures/FinRev/FinRev.html#inference-on-the-slope",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Inference on the Slope",
    "text": "Inference on the Slope\n\nWe also talked about how we can perform inference on the slope \\(\\beta_1\\) of the OLS regression line.\nSpecifically, we may want to test \\[ \\left[ \\begin{array}{rl}\nH_0:    & \\beta_1 = 0   \\\\\nH_A:    & \\beta_1 \\neq 0\n\\end{array} \\right. \\]\n\nThe reason we want to test this is that, if we have reason to believe that \\(\\beta_1\\) could be zero, then there might not be a linear relationship between y and x at all!\n\nUnder normality conditions, \\[ \\frac{\\widehat{\\beta_1} - \\beta_1}{\\mathrm{SD}(\\widehat{\\beta_1})} \\stackrel{H_0}{\\sim} t_{n - 2} \\]"
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#example-1",
    "href": "Pages/Lectures/FinRev/FinRev.html#example-1",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nThe results of regressing a variable y onto another variable x are shown below:\n\n\n\n\nEstimate\nStd. Error\nt-value\nPr(&gt;|t|)\n\n\n\n\nIntercept\n-0.05185\n0.24779\n-0.209\n0.836\n\n\nSlope\n0.08783\n0.07869\n1.116\n0.272\n\n\n\nIs it possible that there exists no linear relationship between y and x? (Use a 5% level of significance wherever necessary.) Explain.\n\n\n\n\n\n\nSince the p-value of testing \\(H_0: \\beta_1 = 0\\) vs \\(H_A: \\beta_1 \\neq 0\\) is \\(0.272\\), which is greater than a significance level of 5%, we would fail to reject the null; that is, it is possible that there exists no linear relationship between y and x."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#sampling-procedures",
    "href": "Pages/Lectures/FinRev/FinRev.html#sampling-procedures",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\n\nFinally, last lecture, we returned to the basics- data!\nSpecifically, we discussed different ways data can be collected; i.e. the different sampling procedures that are available to us.\nIn a simple random sample, every individual in the population has an equal chance of being included in the sample.\n\nThis can sometimes be costly, or even lead to biased samples.\n\nIn a stratified sampling scheme, the population is first divided into several strata (groups), and an SRS is taken from each stratum.\n\nThis has the benefit of creating a potentially more representative sample, though can still be quite costly. Results are also heavily dependent on the strata that were created.\n\nA cluster sampling scheme again divides the population into groups (now called clusters), takes an SRS of clusters, and then takes an SRS from the selected clusters.\n\nThis has the benefit of being (potentially) cheaper, but can again lead to biased samples and is also heavily dependent on the clusters that were created."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#sampling-procedures-1",
    "href": "Pages/Lectures/FinRev/FinRev.html#sampling-procedures-1",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\n\nA convenience sample is one in which individuals are included (or excluded) from the sample based on convenience; e.g. people who are nearby (geographically) are included whereas people who are farther away are not.\n\nConvenience Samples are cheap and, well, convenient, but can lead to very skewed or biased results.\n\nSpeaking of bias, there was another form of bias we discussed: non-response bias.\n\nThis occurs when certain individuals (or potentially even demographics, genders, etc.) do not participate in a survey, despite having been included in the sample of surveyed individuals."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#other-distinctions",
    "href": "Pages/Lectures/FinRev/FinRev.html#other-distinctions",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Other Distinctions",
    "text": "Other Distinctions\n\nIn an observational study, treatment is neither administered nor withheld from subjects.\nIn an experiment, treatment is administered (or possibly withheld) from subjects.\nIn a longitudinal study, subjects are tracked over a period of time. (Observations are therefore correlated)\nIn a cross-sectional study, there is no tracking of subjects over time."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#example-2",
    "href": "Pages/Lectures/FinRev/FinRev.html#example-2",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\nExample (1.20 from OpenIntro)\n\n\n\n\nOn a large college campus first-year students and sophomores live in dorms located on the eastern part of the campus and juniors and seniors live in dorms located on the western part of the campus. Suppose you want to collect student opinions on a new housing structure the college administration is proposing and you want to make sure your survey equally represents opinions from students from all years.\n\nWhat type of study is this?\nSuggest a sampling strategy for carrying out this study."
  },
  {
    "objectID": "Pages/Lectures/FinRev/FinRev.html#solutions-8",
    "href": "Pages/Lectures/FinRev/FinRev.html#solutions-8",
    "title": "PSTAT 5A: Final Exam Review",
    "section": "Solutions",
    "text": "Solutions\n\nTreatment has neither been administered nor withheld, meaning this is an observational study.\n\n\n\n\nStratified sampling seems like the way to go, with western campus and eastern campus being the two strata.\n\nSpecifically, we should take an SRS from both western campus and eastern campus students, to ensure that students across all years are (somewhat) equally represented."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#structure-of-data",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#structure-of-data",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Structure of Data",
    "text": "Structure of Data\n\nWe started by talking about the structure of data.\nWe were exposed to the notion of a data matrix, which is comprised of a series of observational units (i.e. rows) on a series of variables (i.e. columns)\nFor instance, the palmerpenguins data matrix is:\n\n\n\n\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#structure-of-data-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#structure-of-data-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Structure of Data",
    "text": "Structure of Data\n\n\nOf course, the reader is not expected to a priori know what the variables in a dataset represent; as such, most datasets come equipped with a data dictionary that lists out the variables included in the dataset along with a brief description of each.\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nspecies\nThe species of penguin (either Adelie, Chinstrap, or Gentoo)\n\n\nisland\nThe island on which the penguin was found (either Biscoe, Dream, or Torgersen)\n\n\nbill_length_mm\nThe length (millimeters) of the penguin’s bill\n\n\nbill_depth_mm\nThe depth (in millimeters) of the penguin’s bill\n\n\nflipper_length_mm\nThe length (in millimeters) of the penguin’s flipper\n\n\nbody_mass_g\nThe mass (in grams) of the penguin\n\n\nsex\nThe sex of the penguin (either Male or Female)\n\n\nyear\nThe year in which the penguin was observed"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#classification-of-variables",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#classification-of-variables",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Classification of Variables",
    "text": "Classification of Variables\n\nWe also saw that variables fall into two main types: numerical and categorical.\n\nRemember that it is not enough to simply check whether our data is comprised of numbers, as categorical data can be encoded using numbers (e.g. months in a year).\nRather, we should check whether it makes interpretable sense to add two elements in our variable (e.g. 1 + 2 is 3, whereas Jan + Feb is not March)."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#classification-of-variables-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#classification-of-variables-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Classification of Variables",
    "text": "Classification of Variables\n\nWithin numerical data, we have a further subdivision into discrete and continuous variables.\n\nThe set of possible values of a discrete variable has jumps, whereas the set possible values of a continuous variable has no jumps.\n\nWithin categorical data, we have a further subdivision into ordinal and nominal variables.\n\nOrdinal variables have a natural ordering (e.g. letter grades, months of the year, etc.) whereas nominal variables do not (e.g. favorite color)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#full-classification-scheme",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#full-classification-scheme",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Full Classification Scheme",
    "text": "Full Classification Scheme\n\n\n\n\n\n\n\ndata_classification\n\n\ncluster_main\n\n\n\ncluster_0\n\n\n\ncluster_1\n\n\n\ncluster_2\n\n\n\ncluster_3\n\n\n\n\nData\n\nVariable\n\n\n\nnumerical\n\nNumerical\n\n\n\nData-&gt;numerical\n\n\n\n\n\ncategorical\n\nCategorical\n\n\n\nData-&gt;categorical\n\n\n\n\n\ncontinuous\n\nContinuous\n\n\n\nnumerical-&gt;continuous\n\n\n\n\n\ndiscrete\n\nDiscrete\n\n\n\nnumerical-&gt;discrete\n\n\n\n\n\nnominal\n\nNominal\n\n\n\ncategorical-&gt;nominal\n\n\n\n\n\nordinal\n\nOrdinal\n\n\n\ncategorical-&gt;ordinal"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#visualization",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#visualization",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Visualization",
    "text": "Visualization\n\nOnce we have classified a variable as being either numerical or categorical, we can ask ourselves: how can we best visualize this variable?\nFor categorical data, we use a bargraph and for numerical data we use either a histogram or a boxplot."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#bargraph",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#bargraph",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Bargraph",
    "text": "Bargraph"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#histogram",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#histogram",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Histogram",
    "text": "Histogram\n\n\nRemember the importance of binwidth: demo"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#boxplot",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#boxplot",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Boxplot",
    "text": "Boxplot\n\n\nRemember that the whiskers are never allowed to extend beyond 1.5 times the IQR (and recall that the IQR is just the width of the box)."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#numerical-summaries",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#numerical-summaries",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\n\nWe can also produce numerical summaries of numerical variables.\nMeasures of Central Tendency are different quantities that summarize the “center” of a variable\n\nThere are two main measures of central tendency we discussed: the mean and the median."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#the-mean",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#the-mean",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "The Mean",
    "text": "The Mean\n\nThe mean (or arithmetic mean) is a sort of “balancing point”:\n\n\n\n\n\n\n\n\n\n\\[ \\overline{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\]\n\n\nAlso recall our discussion on data aggregation, and how the incorporation of new data changes the mean."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#spread",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#spread",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Spread",
    "text": "Spread\n\nAnother way we could summarize a numerical dataset (i.e. a dataset containing only one variable, one that is numerical) is to describe how “spread out” the values are.\nThe variance is a sort of “average distance of points to the mean”:\n\n\n\n\n\n\n\n\n\n\\[ s_x^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2 \\]\n\n\nThe standard deviation is just the square root of the variance"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#spread-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#spread-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Spread",
    "text": "Spread\n\nThe interquartile range (IQR) is another measure of spread: \\[ \\mathrm{IQR} = Q_3 - Q_1 \\] where \\(Q_1\\) and \\(Q_3\\) denote the first and third quartiles, respectively.\n\nRecall that the \\(p\\)th percentile of a dataset \\(X\\) is the value \\(\\pi_{x, \\ 0.5}\\) such that p% of observations lie to the left of (i.e. are less than) \\(\\pi_{x, \\ 0.5}\\).\n\\(Q_1\\) is the 25th percentile and \\(Q_3\\) is the 75th percentile\n\nThe third measure of spread we discussed is the range: \\[ \\mathrm{range}(X) = \\max\\{x_1, \\cdots, x_n\\} - \\min\\{x_1, \\ \\cdots, \\ x_n\\} \\]"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#number-summary",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#number-summary",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "5-Number Summary",
    "text": "5-Number Summary\n\nRecall the five number summary, which contains:\n\nThe minimum\nThe first quartile\nThe median\nThe third quartile\nThe maximum\n\nAlso recall how all of these quantities appear on a boxplot!"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#comparisons-of-variables",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#comparisons-of-variables",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Comparisons of Variables",
    "text": "Comparisons of Variables\n\nIf we want to compare two variables, there are three cases to consider:\n\nNumerical vs. Numerical\nNumerical vs. Categorical\nCategorical vs. Categorical\n\nWhen comparing two numerical variables, we use a scatterplot\nWhen comparing a numerical variable to a categorical variable, we use a side-by-side boxplot\nWhen comparing two categorical variables, we construct a contingency table"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-probability",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-probability",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Basics of Probability",
    "text": "Basics of Probability\n\nProbability is, in many ways, the language of uncertainty.\nAn experiment is any procedure we can repeat an infinite number of times, where each time we repeat the procedure the same fixed set of “things” can occur\n\nThese “things” are called outcomes\nThe outcome space, denoted \\(\\Omega\\), is the set containing all outcomes associated with a particular experiment.\nEvents are just subset of the outcome space.\n\nWe can express outcome spaces using tables or trees."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#probability-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#probability-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Probability",
    "text": "Probability\n\nProbability is a function that acts on events\n\nNotationally: \\(\\mathbb{P}(E)\\)\n\nThere are two main approaches to computing probabilities:\n\nThe Classical Aproach: if outcomes are equally likely, then for any event \\(E\\) \\[ \\mathbb{P}(E) = \\frac{\\#(E)}{\\#(\\Omega)} \\]\nThe long-run [relative] frequency approach: repeat the experiment an infinite number of times and define \\(\\mathbb{P}(E)\\) to be the proportion of times \\(E\\) occurs"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#long-run-frequencies-example",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#long-run-frequencies-example",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Long-Run Frequencies Example",
    "text": "Long-Run Frequencies Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nToss\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nOutcome\nH\nT\nT\nH\nT\nH\nH\nH\nT\nT\n\n\nRaw freq. of H\n1\n1\n1\n2\n2\n3\n4\n5\n5\n5\n\n\nRel. freq of H\n1/1\n1/2\n1/3\n2/4\n2/5\n3/6\n4/7\n5/8\n5/9\n5/10"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#set-operations",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#set-operations",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Set Operations",
    "text": "Set Operations\n\nGiven two events \\(E\\) and \\(F\\), there are several operations we can perform:\n\nComplement: \\(E^\\complement\\); denotes “not \\(E\\)”\nUnion: \\(E \\cup F\\); denotes \\(E\\) or \\(F\\) (or both)\nIntersection: \\(E \\cap F\\); denotes \\(E\\) and \\(F\\)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#venn-diagrams",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#venn-diagrams",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Venn Diagrams",
    "text": "Venn Diagrams\n\n\n\n\n\n \\(A^\\complement\\)  (complement)\n\n\n\n\n\n\n\n\n \\(A \\cap B\\)  (intersection)\n\n\n\n\n\n\n\n\n \\(A \\cup B\\)  (union)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#axioms-of-probability",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#axioms-of-probability",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Axioms of Probability",
    "text": "Axioms of Probability\n\n\\(\\mathbb{P}(E) \\geq 0\\) for any event \\(E\\)\n\\(\\mathbb{P}(\\Omega) = 1\\)\nFor disjoint events \\(E\\) and \\(F\\) (i.e. for \\(E \\cap F = \\varnothing\\)), \\(\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F)\\)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#probability-rules",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#probability-rules",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Probability Rules",
    "text": "Probability Rules\n\nProbability of the Empty Set: \\(\\mathbb{P}(\\varnothing) = 0\\)\nComplement Rule: \\(\\mathbb{P}(E^\\complement) = 1 - \\mathbb{P}(E)\\)\nAddition Rule: \\(\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F) - \\mathbb{P}(E \\cap F)\\)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#conditional-probabilities",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#conditional-probabilities",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Conditional Probabilities",
    "text": "Conditional Probabilities\n\n\\(\\mathbb{P}(E \\mid F)\\) denotes an “updating” of our beliefs on \\(E\\) in the presence of \\(F\\))\n\nDefinition: \\(\\displaystyle \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}\\), provided \\(\\mathbb{P}(F) \\neq 0\\)\n\nMultiplication Rule: \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)\\)\nBayes’ Rule: \\(\\displaystyle \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)}{\\mathbb{P}(F)}\\)\nLaw of Total Probability: \\(\\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) + \\mathbb{P}(F \\mid E^\\complement) \\cdot \\mathbb{P}(E^\\complement)\\)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#independence",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#independence",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Independence",
    "text": "Independence\n\nIndependence asserts that \\(\\mathbb{P}(E \\mid F) = \\mathbb{P}(E)\\), which in turn implies \\(\\mathbb{P}(F \\mid E) = \\mathbb{P}(F)\\) and \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\cdot \\mathbb{P}(F)\\)\n\nNote that \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\cdot \\mathbb{P}(F)\\) only when \\(E\\) and \\(F\\) are independent! Otherwise, you have to compute \\(\\mathbb{P}(E \\cap F)\\) using the multiplication rule.\nThe interpretation of independence is that the two events “do not affect each other”"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-programming",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-programming",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Basics of Programming",
    "text": "Basics of Programming\n\nRecall that, in this class, we use Python as our main computing language\n\nWe run Python in Jupyter Notebooks\n\nThere are a few data types we have encountered thus far:\n\nstr, for “string”\nint, for “integer”\nfloat, aka “real number with decimals”\nbool, for “boolean”"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-programming-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-programming-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Basics of Programming",
    "text": "Basics of Programming\n\nThere are also certain data classes we have encountered, which are effectively various python objects aggregated into a larger structure. These include:\n\nTables\nLists\nArrays\n\nWe can use indexing to extract certain pieces of data classes; e.g. for a list x, x[i] returns the (i + 1)th element of x.\n\nRemember that Python starts indexing at zero!"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#comparisons-and-conditionals",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#comparisons-and-conditionals",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Comparisons and Conditionals",
    "text": "Comparisons and Conditionals\n\nTake a look at lab 2."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#functions",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#functions",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Functions",
    "text": "Functions\n\nI’d also like to take a minute to talk about functions in Python.\nPython functions behave much like mathematical functions: they take in some number of arguments (i.e. inputs) and can output a variety of things.\n\nFor example, the type() function returns either the data type or data structure of a single input.\nThere is some language we use when dealing with functions in programming: when we pass the argument x into the function f(), we say that we have “called” the function f() on the argument/input x. The object f(x) is called a function call."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#functions-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#functions-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Functions",
    "text": "Functions\n\nTo create a user-defined function function_name(), we use the following structure:\n\n\n\ndef function_name(&lt;arg1&gt;, ...., &lt;argn&gt;):\n  \"\"\"\n  include a docstring here\n  \"\"\"\n  &lt;body of the function here&gt;\n\n\n\nBy the way, when writing skeleton code like the above (i.e. a chunk of code that mimics the structure/format of an actual piece of code, but isn’t fully filled in), programmers often use the symbols &lt; and &gt; to denote text. These symbols should not be included in your actual code.\n\nFor example, we would not actually enclose our arguments in &lt; and &gt;’s; we would simply write out the arguments. We’ll see an example of this in a minute"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#exercise",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#exercise",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\n\n\nExercise\n\n\n\nWrite a function is_mult_of_three() that takes in a single input x and outputs True if x is a multiple of three and False if not. Additionally, the function should return \"Error: input must be numeric\" if the argument x that is provided is not numerical.\n\n\n\n\n\nTake a moment to open up our JupyterHub server instance, and try writing out the function on your own. Then we’ll work through it together (please note that solutions to this won’t be provided, so make sure you take notes!)"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#last-time",
    "href": "Pages/Lectures/Lecture02/Lec02.html#last-time",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Last Time",
    "text": "Last Time\n\nLast time we started discussing how to produce and interpret visual summaries for datasets consisting of only one variable.\n\nWe learned that histograms and boxplots are good visualizers for numerical variables and barplots are good visualizers for categorical data.\n\nBut, as we also saw (in the palmerpenguins dataset), data is usually comprised of several variables.\nA natural question therefore arises: how might we visualize the relationship between multiple variables?"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#multiple-variables",
    "href": "Pages/Lectures/Lecture02/Lec02.html#multiple-variables",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Multiple Variables",
    "text": "Multiple Variables\n\nPerhaps unsurprisingly, visualizing the relationship between 3 or more variables can be a bit tricky.\nAs such, we will restrict ourselves to comparing only two variables.\nEven if we compare only two variables, three cases arise:\n\nComparing two numerical variables\nComparing one numerical and one categorical variable\nComparing two categorical variables\n\nWe will examine the first two cases above, and save the third for later."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#two-numerical-variables",
    "href": "Pages/Lectures/Lecture02/Lec02.html#two-numerical-variables",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Two Numerical Variables",
    "text": "Two Numerical Variables\n\nLet’s say we have two variables, and we want to visualize their relationship.\nAs an example, let’s return to the palmerpenguins dataset and compare the bill_length_mm and bill_depth_mm variables. Let’s also restrict ourselves to Gentoo penguins.\n\n\n\n\n   bill_length_mm bill_depth_mm\n 1           46.1          13.2\n 2           50            16.3\n 3           48.7          14.1\n 4           50            15.2\n 5           47.6          14.5\n 6           46.5          13.5\n 7           45.4          14.6\n 8           46.7          15.3\n 9           43.3          13.4\n10           46.8          15.4\n# ℹ 114 more rows"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#scatterplot",
    "href": "Pages/Lectures/Lecture02/Lec02.html#scatterplot",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nThis type of visualization is called a scatterplot.\nSpecifically, when comparing two numerical variables of the same length, we generate a scatterplot by plotting each observational unit on a Cartesian coordinate system where the axes are prescribed by the variables in question.\n\n\n\n\n\n\n\n\nResult\n\n\n\nWhen comparing two numerical variables (of the same length), a scatterplot is the best visulization tool."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#interpreting-scatterplots",
    "href": "Pages/Lectures/Lecture02/Lec02.html#interpreting-scatterplots",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Interpreting Scatterplots",
    "text": "Interpreting Scatterplots\n\n\n\n\nLet’s return to the scatterplot we generated before:\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice how as the values of bill_length_mm increase, the corresponding values of bill_depth_mm also increase on average?\n\nThis makes intutive sense: longer bills are probably deeper!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#trend",
    "href": "Pages/Lectures/Lecture02/Lec02.html#trend",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Trend",
    "text": "Trend\n\nThis is an example of what we call a trend; specifically, a positive linear trend.\n\nA trend is, loosely speaking, any relationship we observe between the two variables in a scatterplot.\nA trend is said to be linear if a one-unit change in one variable corresponds to a fixed amount of change in the other (we’ll talk about nonlinear trends in a bit)\nA trend is said to be positive (or increasing) if a one-unit increase in one variable corresponds to a one-unit increase in the other."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#nonlinear-trends",
    "href": "Pages/Lectures/Lecture02/Lec02.html#nonlinear-trends",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Nonlinear Trends",
    "text": "Nonlinear Trends\n\nSo, what concretely makes a trend nonlinear?\nWell, let’s first quickly talk about what makes a function nonlinear.\nConsider two functions, \\(f\\) and \\(g\\), whose graphs are depicted below:"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#linearity",
    "href": "Pages/Lectures/Lecture02/Lec02.html#linearity",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Linearity",
    "text": "Linearity\n\nThe key to determining linearity is to check the rate of change.\nFor a linear function: a one-unit change in x corresponds to the same amount of change in y, regardless of where the change in x occurs.\n\nI.e. as x increases from 0 to 1, y should increase/decrease by the same amount as if x had increased from, say, 2 to 3."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#linearity-1",
    "href": "Pages/Lectures/Lecture02/Lec02.html#linearity-1",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Linearity",
    "text": "Linearity\n\nFor the function f above, this is the case:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx: \\(0 \\to 1\\)\ny: \\(0 \\to -1.5\\)\n\\(\\Delta\\)y: \\((-1.5 - 0) = -1.5\\)\n\n\nx: \\(2 \\to 3\\)\ny: \\(-3 \\to -4.5\\)\n\\(\\Delta\\)y: \\((-4.5 + 3.5) = -1.5\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#linearity-2",
    "href": "Pages/Lectures/Lecture02/Lec02.html#linearity-2",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Linearity",
    "text": "Linearity\n\nFor the function g above, this is not the case:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx: \\(0 \\to 1\\)\ny: \\(0.12 \\to 0.09\\)\n\\(\\Delta\\)y: \\((0.09 - 0.12) = -0.03\\)\n\n\nx: \\(2 \\to 3\\)\ny: \\(0.08 \\to 0.073\\)\n\\(\\Delta\\)y: \\((0.073 - 0.08) = -0.007\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#linearity-3",
    "href": "Pages/Lectures/Lecture02/Lec02.html#linearity-3",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Linearity",
    "text": "Linearity\n\nThe notion of in the context of trends is pretty much the same: if a one-unit change in x corresponds to the same amount of change in y everywhere, then we say that the trend is linear. Otherwise, we say the trend is nonlinear.\nBy the way- another way to talk about trends is to phrase things in terms of the variables being compared.\n\nFor example, if the scatterplot of two variables displays a positive linear trend, we might say that the two variables have a positive linear association.\nAs a concrete example: bill length and bill depth appear to have a positive linear association, as seen in the scatterplot from a few slides ago."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#a-numerical-and-a-categorical-variable",
    "href": "Pages/Lectures/Lecture02/Lec02.html#a-numerical-and-a-categorical-variable",
    "title": "PSTAT 5A: Lecture 02",
    "section": "A Numerical and a Categorical Variable",
    "text": "A Numerical and a Categorical Variable\n\nThe final case we will consider today is comparing a numerical variable to a categorical one.\nAs a concrete example, here is a (mock) dataset comprised of the following variables:\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\n\nstdy_hrs\naverage amount of time (in hrs) a student spent studying for a particular class each week\n\n\nltr_grd\nthe final letter grade (A+, A, A-, etc.) the student received in the class"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#side-by-side-boxplots",
    "href": "Pages/Lectures/Lecture02/Lec02.html#side-by-side-boxplots",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Side-by-Side Boxplots",
    "text": "Side-by-Side Boxplots\n\n\n\n\n\n\n\nResult\n\n\n\nWhen comparing one numerical and one categorical variable, it is best to visualize their relationship using a side-by-side boxplot.\n\n\n\n\n\n\nThough the notion of trend is slightly different in the context of a side-by-side boxplot, we can still use them to determine relationships.\nFor example, from the plot on the previous slide, we can see that, on average, students who received lower grades tended to study less than those students who received higher grades."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#causality",
    "href": "Pages/Lectures/Lecture02/Lec02.html#causality",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Causality",
    "text": "Causality\n\nI should make a very important point: identifying trends is not the same thing as identifying causal relationships.\nFor example, the side-by-side boxplot from a few slides ago does not tell us that “studying less causes your grade to decrease”\n\nThere are a lot of other confounding variables that could contribute to the decrease in grade.\n\nWe won’t talk too much about causality in this course, but it is an important thing to be aware of: association is not the same thing as causation!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#quantifying-data",
    "href": "Pages/Lectures/Lecture02/Lec02.html#quantifying-data",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Quantifying Data",
    "text": "Quantifying Data\n\nLet’s once again consider a single numerical variable.\nAs a concrete example, we can consider the exam scores variable from the previous slides:\n\n\n\n\n  [1] 88.236 77.348 81.050 74.431 75.083 79.569 74.998 80.099 74.264 83.850\n [11] 89.857 81.427 79.439 84.260 78.565 77.570 78.224 73.780 88.085 79.341\n [21] 80.554 77.317 81.155 83.842 87.051 78.362 81.528 72.148 74.131 78.927\n [31] 75.446 79.791 78.199 90.769 85.640 78.420 83.484 79.045 97.909 86.736\n [41] 73.723 76.973 81.320 79.238 85.803 86.621 85.781 81.844 82.896 80.478\n [51] 75.903 84.565 76.302 83.432 85.448 69.695 81.049 85.575 84.791 82.525\n [61] 78.361 77.803 86.542 84.171 86.103 72.772 78.730 76.189 75.187 79.194\n [71] 77.159 82.048 82.661 84.021 76.008 79.474 79.015 86.992 72.524 76.094\n [81] 78.765 80.623 82.497 75.776 70.614 79.677 81.182 77.943 76.863 85.561\n [91] 89.569 96.695 73.680 77.770 81.584 81.965 78.373 76.295 73.212 79.229\n[101] 87.273 87.364 82.706 83.843 75.864 82.791 82.637 78.685 72.626 69.302\n[111] 93.408 73.189 83.764 77.832 82.803 80.278 94.962 79.616 85.667 82.710\n[121] 86.823 76.656 74.623 71.508 91.131 78.318 81.058 86.239 76.585 85.652\n[131] 77.122 86.036 83.127 83.234 80.746 83.878 75.544 73.780 81.106 85.523"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#summarizing-data",
    "href": "Pages/Lectures/Lecture02/Lec02.html#summarizing-data",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Summarizing Data",
    "text": "Summarizing Data\n\nThe remainder of today’s lecture will be devoted to finding numerical summaries of a dataset \\(X = \\{x_i\\}_{i=1}^{n}\\).\nThis will lead us to several different summary statistics, which are mathematical quantities that seek to describe different aspects of our data."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#quantifying-center",
    "href": "Pages/Lectures/Lecture02/Lec02.html#quantifying-center",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Quantifying “Center”",
    "text": "Quantifying “Center”\n\nHere is a very broad question: what is the center of a dataset \\(X = \\{x_i\\}_{i=1}^{n}\\)?\nPerhaps the scores dataset from earlier is a bit too complicated- let’s simplify things and look at the dataset \\[ X = \\{1, 1, 1, 2, 3, 3, 4, 5, 5, 5, 6, 6\\} \\]\nAs a starting point, I can “plot” these points on a number line (to produce what is known as a dotplot):"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#quantifying-center-1",
    "href": "Pages/Lectures/Lecture02/Lec02.html#quantifying-center-1",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Quantifying “Center”",
    "text": "Quantifying “Center”\n\n\nBack to our question: what is the center of this dataset?\n\n\n\nPerhaps we can think of center as a balancing point. In other words: where should I place a fulcrum to ensure this number line remains balanced?\n\n\n\n\n\n\n\n\n\nWe call this balancing point the arithmetic mean (or just mean, or average, for short), and denote it \\(\\overline{x}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#the-mean",
    "href": "Pages/Lectures/Lecture02/Lec02.html#the-mean",
    "title": "PSTAT 5A: Lecture 02",
    "section": "The Mean",
    "text": "The Mean\n\n\n\n\n\n\nFormula: The Mean\n\n\n\nGiven a set of data \\(x = \\{x_i\\}_{i=1}^{n}\\), we compute its mean, denoted \\(\\overline{x}\\), using the formula \\[ \\overline{x} = \\frac{1}{n} (x_1 + \\cdots + x_n) \\] which can be equivalently written as \\[ \\overline{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\]\n\n\n\n\n\nIn words: we compute the mean by adding up all of the points included in our dataset, and dividing by the total number of points."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#a-note-on-notation",
    "href": "Pages/Lectures/Lecture02/Lec02.html#a-note-on-notation",
    "title": "PSTAT 5A: Lecture 02",
    "section": "A Note on Notation",
    "text": "A Note on Notation\n\nPerhaps you haven’t seen the notation \\(\\sum_{i=1}^{n} x_i\\) before. Don’t get scared by it! It’s just a shorthand notation for the sum of the points \\(\\{x_1, x_2, \\cdots, x_n\\}\\).\nSo, if it’s easier for you, you can always think of \\((x_1 + \\cdots x_n)\\) in place of \\(\\sum_{i=1}^{n} x_i\\).\nHaving said that, I will often use this notation (called sigma notation, as the symbol \\(\\sum\\) is the capital Greek letter “sigma”) as it saves quite a bit of time in the long run.\n\nI also urge you to familiarze yourself with \\(\\Sigma\\) notation, as I’m sure you will encounter it even beyond this course!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise",
    "href": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 1\n\n\n\nCompute the mean of the set \\(B = \\{-1, 0, 1, 1, 2, 4\\}\\). Discuss with your neighbors!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#another-one",
    "href": "Pages/Lectures/Lecture02/Lec02.html#another-one",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Another One!",
    "text": "Another One!\n\n\n\n\n\n\nExercise 2\n\n\n\nA collection of \\(n = 42\\) exam scores have an average 50%. Two additional scores of 100% are reported. How does the average of scores change with the addition of these two new scores?"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#range",
    "href": "Pages/Lectures/Lecture02/Lec02.html#range",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Range",
    "text": "Range\n\nAnother way we can summarize a dataset \\(X = \\{x_i\\}_{i=1}^{n}\\) is to describe how spread out it is.\nOne idea on how we can capture the spread is to say: how far apart is the smallest value from the largest value?\nIndeed, this statistic has a name: the range.\n\n\n\n\n\n\n\n\nFormula: Range\n\n\n\nGiven a set of numbers \\(X = \\{x_1, x_2, \\cdots, x_n\\}\\), we compute the range of \\(X\\) as: \\[ \\mathrm{range}(X) = \\max\\{x_1, \\cdots, x_n\\} - \\min\\{x_1, \\ \\cdots, \\ x_n\\} \\] i.e. the largest value minus the smallest value."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#leadup",
    "href": "Pages/Lectures/Lecture02/Lec02.html#leadup",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Leadup",
    "text": "Leadup\n\nNow, there is another way to think about spread: suppose we look at the average distance of points from their mean.\nMore specifically: define \\(d_i := x_i - \\overline{x}\\) to be the deviation of the \\(i\\)th point from the mean:"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#the-variance",
    "href": "Pages/Lectures/Lecture02/Lec02.html#the-variance",
    "title": "PSTAT 5A: Lecture 02",
    "section": "The Variance",
    "text": "The Variance\n\n\nThis is what we call the variance of the set \\(X\\), denoted by \\(s_x^2\\).\n\n\n\n\n\n\n\n\nFormula: Variance, and Standard Deviation\n\n\n\nGiven a set of data \\(X = \\{x_i\\}_{i=1}^{n}\\), we compute the variance of \\(X\\) by \\[ s_x^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2 \\] We define the standard deviation, denoted by \\(s_x\\), to be \\(\\sqrt{s_X^2}\\); i.e. \\[ s_x := \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise-1",
    "href": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise-1",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 3\n\n\n\nFor the set \\(X = \\{1, 2, 3, 4, 5\\}\\), compute \\(s_x\\). Discuss with your neighbor!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#iqr",
    "href": "Pages/Lectures/Lecture02/Lec02.html#iqr",
    "title": "PSTAT 5A: Lecture 02",
    "section": "IQR",
    "text": "IQR\n\nThere is yet another way to quantify the spread of a dataset, and that is what is known as the Interquartile Range (IQR, for short).\n\n\n\n\n\n\n\n\nFormula: The IQR\n\n\n\nGiven a set of data \\(x = \\{x_i\\}_{i=1}^{n}\\), we compute its interquartile range using the formula \\[ \\mathrm{IQR} = Q_3 - Q_1 \\] where \\(Q_1\\) and \\(Q_3\\) denote the first and third quartiles, respectively.\n\n\n\n\n\n\nIn other words, the IQR is just the width of the box in a boxplot!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#five-number-summary",
    "href": "Pages/Lectures/Lecture02/Lec02.html#five-number-summary",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Five Number Summary",
    "text": "Five Number Summary\n\nSpeaking of boxplots, there is a set of numbers that occurs frequently when summarizing numerical data, collectively called the five number summary. The elements of the five number summary are:\n\nThe minimum\nThe first quartile\nThe median\nThe third quartile\nThe maximum"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise-2",
    "href": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise-2",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 4\n\n\n\nConsider a dataset \\(X\\) that has boxplot given by:\n\n\n\n\n\n\n\n\n\nProvide the five number summary, along with the IQR. Discuss with your Neighbors!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#leadup-1",
    "href": "Pages/Lectures/Lecture02/Lec02.html#leadup-1",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Leadup",
    "text": "Leadup\n\nSuppose \\(F = \\{f_i\\}_{i=1}^{n}\\) denotes a set of temperature measurements, as recorded in Fahrenheit.\nNow, let’s say I want to convert the measurements to be in terms of Centigrade, and will store the resulting values in a set called \\(C = \\{c_i\\}_{i=1}^{n}\\) (for “Centigrade”).\nThe \\(c_i\\) values aren’t new data- they are still linked with our original \\(f_i\\) Fahrenheit measurements! Specifically, they are linked by way of the following formula: \\[ c_i = \\frac{5}{9} (f_i - 32) \\] (as this is the conversion from Fahrenheit to Centigrade)."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#transformations",
    "href": "Pages/Lectures/Lecture02/Lec02.html#transformations",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Transformations",
    "text": "Transformations\n\n\nThis is a specific example of a more general concept known as data transformation.\n\n\n\nIn general, given a set of data \\(X = \\{x_i\\}_{i=1}^{n}\\), we can consider the set of points obtained by applying some function (a.k.a. a transformation) \\(g\\) to the points \\(x_i\\), to obtain a new set \\(\\{g(x_i)\\}_{i=1}^{n}\\)\n\nFor instance, we used \\(g(x) = (5/9) \\cdot (x - 32)\\) in our Fahrenheit-to-Centigrade example on the previous slide.\nThere are many other functions we could consider! For instance, in certain cases we may need to transform our data using a logarithm function; i.e. \\(g(x) = \\ln(x)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#linear-transformations",
    "href": "Pages/Lectures/Lecture02/Lec02.html#linear-transformations",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Linear Transformations",
    "text": "Linear Transformations\n\nFor now, let’s restrict ourselves to considering linear transformations of our data.\nRecall that a function \\(g(x)\\) is said to be linear if it is of the form \\(g(x) = ax + b\\).\n\nSo, we are restricting ourselves to functions \\(g\\) of this form.\n\nOne question we may ask is: what is the mean of our transformed data?\n\nIn other words, given a set \\(X = \\{x_i\\}_{i=1}^{n}\\) and a new set \\(Y := \\{ax_i + b\\}_{i=1}^{n}\\) for fixed constants \\(a\\) and \\(b\\), what can we say about \\(\\overline{Y}\\)?"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#more-complicated-transformations",
    "href": "Pages/Lectures/Lecture02/Lec02.html#more-complicated-transformations",
    "title": "PSTAT 5A: Lecture 02",
    "section": "More Complicated Transformations",
    "text": "More Complicated Transformations\n\nFor more general transformations \\(g\\), things don’t work out as nicely.\nFor instance, if we take the logarithm of each datapoint, the average of these new points is not necessarily going to be the logarithm of the average.\n\nTo see that, we can consider a specific example: Let \\(X = \\{1, 2, 3\\}\\) and take \\(Y = \\{\\ln(1), \\ \\ln(2), \\ \\ln(3)\\}\\). Then \\(\\overline{x} = 2\\) and \\(\\overline{y} \\approx 0.597\\), whereas \\(\\ln(\\overline{x}) = \\ln(2) \\approx 0.693\\).\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf \\(X = \\{x_i\\}_{i=1}^{n}\\) and \\(Y = \\{g(x_i)\\}_{i=1}^{n}\\) where \\(g\\) is not a linear transformation, then it is not the case that \\(\\overline{y} = g(\\overline{x})\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#summary",
    "href": "Pages/Lectures/Lecture02/Lec02.html#summary",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Summary",
    "text": "Summary\n\nWe started off by finishing our discussion of data visualizing, identifying ways to visualize the relationship between two variables.\n\nSuch visualizations included: scatterplots and side-by-side boxplots.\nWe also discussed notions of trend.\n\nNext, we discussed various numerical summaries of data.\n\nThese included measures of central tendency (like the mean or the median), along with measures of spread (like the variance, standard deviation, or IQR).\nWe were also introduced to the five number summary, which is closely related to boxplots.\n\nNext time we’ll begin our discussion on Probability, which, as we will see, provides a rigorous way of quantifying uncertainty."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#multiple-populations",
    "href": "Pages/Lectures/Lecture18/Lec18.html#multiple-populations",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Multiple Populations",
    "text": "Multiple Populations\n\nSo far, we’ve talked about constructing confidence intervals and performing hypothesis tests for both population proportions and population means.\nOne crucial thing to note is that everything we’ve done has been in the context of a single population\nSometimes, as Data Scientists, we may want to test claims about the differences between two populations\n\nE.g. Is the average monthly income in Santa Barbara different from the average monthly income in San Francisco?\nE.g. Is the proportion of people who test positive for a disease in one country different than the proportion that test positive in a second country?"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#two-populations-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#two-populations-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Two Populations",
    "text": "Two Populations\n\nStatistically: we are imagining two populations, Population 1 and Population 2, governed by parameters \\(\\theta_1\\) and \\(\\theta_2\\), respectively, and trying to test claims about the relationship between \\(\\theta_1\\) and \\(\\theta_2\\).\n\nFor example, we could consider two populations with means \\(\\mu_1\\) and \\(\\mu_2\\), respectively, and try to make claims about whether or not \\(\\mu_1\\) and \\(\\mu_2\\) are equal.\n\nThe trick Statisticians use is to think in terms of the difference \\(\\theta_2 - \\theta_1.\\)\n\nFor example, if our null hypothesis is that \\(\\theta_1 = \\theta_2\\), this can be rephrased as \\(H_0: \\ \\theta_2 - \\theta_1 = 0\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#two-populations-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#two-populations-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Two Populations",
    "text": "Two Populations\n\nThe reason we do this is because we have now effectively reduced our two-parameter problem into a one-parameter problem, involving only the parameter \\(\\delta := \\theta_2 - \\theta_1\\).\nNow, we will need a point estimator of \\(\\delta\\).\nIf \\(\\widehat{\\theta}_1\\) and \\(\\widehat{\\theta}_2\\) are point estimators of \\(\\theta_1\\) and \\(\\theta_2\\), respectively, then a natural point estimator of \\(\\delta\\) is \\(\\widehat{\\delta} = \\widehat{\\theta}_2 - \\widehat{\\theta}_1\\).\n\nFor example, a natural point estimator for the difference \\(\\mu_2 - \\mu_1\\) of population means is \\(\\overline{X}_2 - \\overline{X}_1\\), the difference in sample means."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#two-populations-3",
    "href": "Pages/Lectures/Lecture18/Lec18.html#two-populations-3",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Two Populations",
    "text": "Two Populations\n\nWe will ultimately need access to the sampling distribution of \\(\\widehat{\\delta}\\).\nBefore delving into that, however, we will need a little more probability knowledge; specifically, knowledge on how linear combinations of random variables work."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#linear-combinations-of-random-variables-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#linear-combinations-of-random-variables-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Linear Combinations of Random Variables",
    "text": "Linear Combinations of Random Variables\n\nRecall, from many weeks ago, that a random variable \\(X\\) is simply some numerical variable that tracks a random outcome of an experiment.\n\nE.g. number of heads in 10 tosses of a fair coin; number of people in a population that test positive for a disease; etc.\n\nA random variable \\(X\\), whether it be discrete or continuous, has an expected value \\(\\mathbb{E}[X]\\) and a variance \\(\\mathrm{Var}(X)\\).\nNow, suppose we have two random variables \\(X\\) and \\(Y\\), and three constants \\(a\\), \\(b\\), and \\(c\\).\nOur task for now is to say as much as we can about the quantity \\(aX + bY + c\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#linear-combinations-of-random-variables-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#linear-combinations-of-random-variables-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Linear Combinations of Random Variables",
    "text": "Linear Combinations of Random Variables\n\n\n\n\n\n\n\nTheorem\n\n\n\n\nGiven two random variables \\(X\\) and \\(Y\\), and constants \\(a, \\ b,\\) and \\(c\\), \\[ \\mathbb{E}[aX + bY + c] = a \\cdot \\mathbb{E}[X] + b \\cdot \\mathbb{E}[Y] + c \\]\n\n\n\n\n\n\n\nAs an example: if \\(\\mathbb{E}[X] = 2\\) and \\(\\mathbb{E}[Y] = -1\\), then \\[\\mathbb{E}[2X + 3Y + 1] = 2(2) + 3(-1) + 1 = 2 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#linear-combinations-of-random-variables-3",
    "href": "Pages/Lectures/Lecture18/Lec18.html#linear-combinations-of-random-variables-3",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Linear Combinations of Random Variables",
    "text": "Linear Combinations of Random Variables\n\n\n\n\n\n\n\nTheorem\n\n\n\n\nGiven two independent random variables \\(X\\) and \\(Y\\), and constants \\(a, \\ b,\\) and \\(c\\), \\[ \\mathrm{Var}(aX + bY + c) = a^2 \\mathrm{Var}(X) + b^2 \\mathrm{Var}(Y) \\]\n\n\n\n\n\n\n\nYou will not be responsible for the proof of this fact.\nAlso, we haven’t explicitly talked about what independence means in the context of random variables; for now, suffice it to say that it works analogously to the concept of independence of events. That is, if the random variables \\(X\\) and \\(Y\\) come from two experiments that don’t have any relation to each other, then \\(X\\) and \\(Y\\) will be independent."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#linear-combinations-of-normally-distributed-random-variables",
    "href": "Pages/Lectures/Lecture18/Lec18.html#linear-combinations-of-normally-distributed-random-variables",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Linear Combinations of Normally-Distributed Random Variables",
    "text": "Linear Combinations of Normally-Distributed Random Variables\n\nSomething interesting happens when we consider taking linear combinations of normally-distributed random variables.\nSay we have \\(X \\sim \\mathcal{N}(\\mu_x, \\ \\sigma_x)\\) and \\(Y \\sim \\mathcal{N}(\\mu_y, \\ \\sigma_y)\\) with \\(X \\perp Y\\).\nThen, \\[ (aX + bY + c) \\sim \\mathcal{N}\\left( a\\mu_x + b \\mu_y + c, \\ \\sqrt{a^2 \\sigma_x^2 + b^2 \\sigma_y^2} \\right) \\]\n\nNote that the expectation and standard deviation simply follow from the previous fact. What is unique/impotant about this result is that it tells us linear combinations of normally-distributed random variables are also normally-distributed!\nThis is not true of all distributions; for example, linear combinations of uniformly-distributed random variables are not uniformly distributed."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#worked-out-example",
    "href": "Pages/Lectures/Lecture18/Lec18.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSuppose that wait times at the Goleta and Santa Barbara DMVs are normally distributed and independent. Specifically, if \\(X\\) denotes the wait time (in minutes) of a randomly-selected customer from the Goleta location and \\(Y\\) denotes the wait time (in minutes) of a randomly-selected customer from the Santa Barbara location, then \\[ X \\sim \\mathcal{N}(5, \\ 1.5)   \\quad Y \\sim \\mathcal{N}(7, 1.75)  \\] What is the probability that a randomly-selected person from the Goleta location and a randomly-selected person from the Santa Barbara location are served within 2 minutes of each other?"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#solutions",
    "href": "Pages/Lectures/Lecture18/Lec18.html#solutions",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Solutions",
    "text": "Solutions\n\nFirst, let \\(X\\) denote the wait time of the person from the Goleta location and let \\(Y\\) denote the wait time of the person from the Santa Barbara location. Then \\[ X \\sim \\mathcal{N}(5, \\ 1.5)   \\quad Y \\sim \\mathcal{N}(7, 1.75)  \\] with \\(X \\perp Y\\).\nThe quantity we seek is \\(\\mathbb{P}(|X - Y| &lt; 2)\\), which we can first write as \\[ \\mathbb{P}(-2 \\leq X - Y \\leq 2) \\]\nNow, by the result above we know what the distribution of \\((X - Y)\\) is: \\[ (X - Y) \\sim \\mathcal{N}\\left( 5 - 7 , \\ \\sqrt{1.5^2 + 1.75^2} \\right) \\sim \\mathcal{N}\\left(-2, \\  \\sqrt{1.5^2 + 1.75^2} \\right)\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#solutions-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#solutions-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Solutions",
    "text": "Solutions\n\nFor ease of notation, let \\(D = X - Y\\). Then what we just showed is that \\[ D \\sim \\mathcal{N}(-2, \\  \\sqrt{1.5^2 + 1.75^2}) \\]\nFurthermore, the quantity we seek is \\[ \\mathbb{P}(-2 \\leq D \\leq 2) \\]\nHence, we are now in business!\n\nNamely, we can just use the same procedure we have been using for normal distribution problems all along in this class."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#solutions-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#solutions-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Solutions",
    "text": "Solutions\n\\[\\begin{align*}\n  \\mathbb{P}(-2 \\leq D \\leq 2)   & = \\mathbb{P}(D \\leq 2) - \\mathbb{P}(D \\leq -2)    \\\\\n    & = \\mathbb{P}\\left( \\frac{D + 2}{\\sqrt{1.5^2 + 1.75^2}} \\leq \\frac{2 + 2}{\\sqrt{1.5^2 + 1.75^2}} \\right)     \\\\\n      & \\hspace{20mm} - \\mathbb{P}\\left( \\frac{D + 2}{\\sqrt{1.5^2 + 1.75^2}} \\leq \\frac{-2 + 2}{\\sqrt{1.5^2 + 1.75^2}} \\right)   \\\\\n    & = \\mathbb{P}\\left( \\frac{D + 2}{\\sqrt{1.5^2 + 1.75^2}} \\leq 1.74 \\right)     \\\\\n      & \\hspace{20mm} - \\mathbb{P}\\left( \\frac{D + 2}{\\sqrt{1.5^2 + 1.75^2}} \\leq 0 \\right)    \\\\\n    & = 0.9591 - 0.5000 = \\boxed{0.4591}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#two-populations-4",
    "href": "Pages/Lectures/Lecture18/Lec18.html#two-populations-4",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Two Populations",
    "text": "Two Populations\n\nAlright, so what does this mean in the context of our two-proportion problem?\nWell, for one thing, we can easily construct a confidence interval for \\((\\theta_2 - \\theta_1)\\) using: \\[ (\\widehat{\\theta}_2 - \\widehat{\\theta}_1) \\pm c \\cdot \\sqrt{\\mathrm{Var}(\\widehat{\\theta}_1) + \\mathrm{Var}(\\widehat{\\theta}_2)} \\] where \\(c\\) is a constant that is determined by both the sampling distribution of \\(\\widehat{\\theta}_2 - \\widehat{\\theta}_1\\) as well as our confidence level.\nBy the way, can anyone tell me why the variances are added, and not subtracted?"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#two-means",
    "href": "Pages/Lectures/Lecture18/Lec18.html#two-means",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Two Means",
    "text": "Two Means\n\nTo make things more specific, let’s consider comparing two population means.\nSpecifically: imagine we have two populations (which we will call Population 1 and Population 2), governed by population means \\(\\mu_1\\) and \\(\\mu_2\\), respectively.\nFor now, let’s focus a two-sided test, where our hypotheses are \\[\\left[ \\begin{array}{rr}\nH_0:    & \\mu_1 = \\mu_2   \\\\\nH_A:    & \\mu_1 \\neq \\mu_2\n\\end{array} \\right.\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#two-means-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#two-means-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Two Means",
    "text": "Two Means\n\nAgain, it’s customary to rephrase things to be in terms of differences: \\[\\left[ \\begin{array}{rr}\nH_0:    & \\mu_2 - \\mu_1 = 0   \\\\\nH_A:    & \\mu_2 - \\mu_1 \\neq 0\n\\end{array} \\right.\\]\nNow, we need data!\nSuppose we have a sample \\(X = \\{X_i\\}_{i=1}^{n_1}\\) taken from Population 1 and a sample \\(Y = \\{Y_i\\}_{i=1}^{n_2}\\) taken from Population 2.\n\nNote that we are allowing for different sample sizes, \\(n_1\\) and \\(n_2\\)!\n\nLet’s also assume that, in addition to being representative samples, the two samples are both independent within themselves and independent from each other (i.e. assume the \\(X_i\\)’s and \\(Y_i\\)’s are independent, and that the \\(X\\)’s are independent from the \\(Y\\)’s)"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#two-means-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#two-means-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Two Means",
    "text": "Two Means\n\nAgain, we are interested in finding a point estimator for \\(\\mu_2 - \\mu_1\\).\nHere’s a question: do we have a natural point estimator for \\(\\mu_2\\)? What about for \\(\\mu_1\\)?\nSo, it seems that a natural point estimator for \\(\\delta = \\mu_2 - \\mu_1\\) is \\[ \\widehat{\\delta} = \\overline{Y} - \\overline{X} \\]\nWhat is the sampling distribution of \\(\\widehat{\\delta}\\)?\nWell, there are a few cases to consider."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#sampling-distribution-of-widehatdelta",
    "href": "Pages/Lectures/Lecture18/Lec18.html#sampling-distribution-of-widehatdelta",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Sampling Distribution of \\(\\widehat{\\delta}\\)",
    "text": "Sampling Distribution of \\(\\widehat{\\delta}\\)\n\nSuppose that our two populations had known variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\), respectively.\nThen, if both \\(\\overline{X}\\) and \\(\\overline{Y}\\) were normally distributed, we could one of the facts we previously saw in this lecture to conclude that \\[ \\widehat{\\delta} \\sim \\mathcal{N}\\left( \\delta, \\ \\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} } \\right) \\]\n\nSee the whiteboard for more details"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#the-test-statistc",
    "href": "Pages/Lectures/Lecture18/Lec18.html#the-test-statistc",
    "title": "PSTAT 5A: Lecture 18",
    "section": "The Test Statistc",
    "text": "The Test Statistc\n\nIn this case, a natural candidate for our test statistic would be \\[ \\frac{\\widehat{\\delta}}{\\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} =  \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\] as, under the null, this would follow a standard normal distribution.\nHowever, there are a few problems with this.\nFor one, it requires both \\(\\overline{X}\\) and \\(\\overline{Y}\\) to be normally distributed, which we know is not always the case.\nAlright, that’s fine though- so long as our sample sizes are large enough, the Central Limit Theorem kicks in and we can be reasonably certain that \\(\\overline{X}\\) and \\(\\overline{Y}\\) will be pretty close to normally distributed."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#the-test-statistic",
    "href": "Pages/Lectures/Lecture18/Lec18.html#the-test-statistic",
    "title": "PSTAT 5A: Lecture 18",
    "section": "The Test Statistic",
    "text": "The Test Statistic\n\nHowever, the main problem in using this test statistic is that it requires access to the population variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)!\nAny ideas on how to remedy this?\n\nRight; let’s just replace the population variances with their sample analogues: \\[ \\mathrm{TS} = \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{ \\frac{s_X^2}{n_1} + \\frac{s_Y^2}{n_2}}}\\] where \\[\\begin{align*}\ns_X^2   & = \\frac{1}{n_1 - 1} \\sum_{i=1}^{n_1} (X_i - \\overline{X})^2   \\\\\ns_Y^2   & = \\frac{1}{n_2 - 1} \\sum_{i=1}^{n_2} (Y_i - \\overline{Y})^2\n  \\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#the-test-statistic-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#the-test-statistic-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "The Test Statistic",
    "text": "The Test Statistic\n\nAny guesses on what distribution this follows under the null?\nIf you said t….. you’d be wrong! (But pretty close.)\nIt turns out that, under the null (i.e. assuming that \\(\\mu_1 = \\mu_2\\), or, equivalently, that \\(\\delta = \\mu_2 - \\mu_1 = 0\\)), this test statistic approximately follows a t-distribution.\nWhat degrees of freedom?\nThat’s right: \\[ \\mathrm{df} = \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{s_X^2}{n_1} \\right) + \\left( \\frac{s_Y^2}{n_2} \\right) \\right]^2 }{ \\frac{\\left( \\frac{s_X^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_Y^2}{n_2} \\right)^2}{n_2 - 1} } \\right\\} \\]\n\nThis is related to what is known as the Satterthwaite Approximation, sometimes called the Welch-Satterthwaite Equation"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#the-test",
    "href": "Pages/Lectures/Lecture18/Lec18.html#the-test",
    "title": "PSTAT 5A: Lecture 18",
    "section": "The Test",
    "text": "The Test\n\nAlright, so we finally have a test statistic: \\[ \\mathrm{TS} = \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\] and its (approximate) distribution under the null: \\[ \\mathrm{TS} \\stackrel{H_0}{\\sim} t_{\\nu} \\] where \\(\\nu\\) is given by the Satterthwaite Approximation.\nRecall our hypotheses: \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\mu_2 - \\mu_1 = 0   \\\\\nH_A:    & \\mu_2 - \\mu_1 \\neq 0\n\\end{array} \\right. \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#the-test-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#the-test-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "The Test",
    "text": "The Test\n\nWe can see that large values of \\(|\\mathrm{TS}|\\) lead credence to the alternative over the null; as such, our decision will take the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| &gt; c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where \\(c\\) is the appropriately-selected quantile of the appropriate t-distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nGaucho Gourmande has two locations: one in Goleta and one in Santa Barbara. The owner would like to determine whether the average revenue generated by the two locations are equal or not. To that end, he computes the net revenue generated by the Goleta location over 30 days and also computes the net revenue generated by the Santa Barbara location over 35 days (assume all of the necessary independence conditions hold), and produced the following information:\n\\[\\begin{array}{r|cc}\n                    & \\text{Sample Average}     & \\text{Sample Standard Deviation}    \\\\\n  \\hline\n  \\textbf{Goleta}   &     \\$13                    & \\$3.45        \\\\\n  \\textbf{Santa Barbara}   &     \\$15                    & \\$4.23\n\\end{array}\\]\nTest the owner’s claims at an \\(\\alpha = 0.05\\) level of significance, using a two-sided alternative."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#solutions-3",
    "href": "Pages/Lectures/Lecture18/Lec18.html#solutions-3",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Solutions",
    "text": "Solutions\n\nOur first step should be to figure out what “Population 1” and “Population 2” are in the context of the problem.\nLet “Goleta Location” be Population 1 and “Santa Barbara Location” be Population 2.\n\nIt is perfectly acceptable to swap these two, but just be sure you remain consistent throughout the problem!\nAlso, I will expect you to explicitly write out your definitions of the populations (like above), even if the problem doesn’t explicitly ask you to do so.\n\nIn this way, \\[ \\overline{X} = 13; \\quad s_X = 3.45; \\quad \\overline{Y} = 15; \\quad s_Y = 4.23 \\]\nAdditionally, \\(n_1 = 30\\) and \\(n_2 = 35\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#solutions-4",
    "href": "Pages/Lectures/Lecture18/Lec18.html#solutions-4",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Solutions",
    "text": "Solutions\n\nNow, let’s compute the value of the test statistic. \\[ \\mathrm{TS} =   \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{ \\frac{s_X^2}{n_1} + \\frac{s_Y^2}{n_2}}} = \\frac{15 - 13}{\\sqrt{\\frac{3.45^2}{30}  + \\frac{4.23^2}{35} }} = 2.10 \\]\nWe should next figure out the degrees of freedom: \\[\\begin{align*}\n\\mathrm{df}   &  = \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{s_X^2}{n_1} \\right) + \\left( \\frac{s_Y^2}{n_2} \\right) \\right]^2 }{ \\frac{\\left( \\frac{s_X^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_Y^2}{n_2} \\right)^2}{n_2 - 1} } \\right\\}     \\\\\n  & =  \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{3.45^2}{30} \\right) + \\left( \\frac{4.23^2}{35} \\right) \\right]^2 }{ \\frac{\\left( \\frac{3.45^2}{30} \\right)^2}{30 - 1} + \\frac{\\left( \\frac{4.23^2}{35} \\right)^2}{35 - 1} } \\right\\} = 63\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#solutions-5",
    "href": "Pages/Lectures/Lecture18/Lec18.html#solutions-5",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Solutions",
    "text": "Solutions\n\nAt this point, we could either proceed using critical values or using p-values.\nLet’s use p-values, for practice.\nOur p-value is computed as\n\n\n\nimport scipy.stats as sps\n2*sps.t.cdf(-2.10, 63)\n\n0.03973904581390475\n\n\n\n\nThis is below our level of significance \\(\\alpha = 0.05\\) meaning we would reject the null.\nIf we wanted to instead use critical values:\n\n\n\n-sps.t.ppf(0.05, 63)\n\n1.6694022215079614\n\n\n\n\nThis means our critical value is 1.67; since \\(|\\mathrm{TS}| = |2.10| = 2.10 &gt; 1.67\\), we would again reject at an \\(\\alpha = 0.05\\) level of significance."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#solutions-6",
    "href": "Pages/Lectures/Lecture18/Lec18.html#solutions-6",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Solutions",
    "text": "Solutions\n\nAt a 5% level of significance, there was sufficient evidence to reject the owner’s claims that the revenue generated by the two locations are equal, in favor of the alternative that the revenue generated by the two locations are not equal."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#extensions",
    "href": "Pages/Lectures/Lecture18/Lec18.html#extensions",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Extensions",
    "text": "Extensions\n\nUnsurprisingly, we can adapt the above procedure to account for one-sided alternatives as well.\nFor instance, suppose we wish to test \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\mu_1 = \\mu_2   \\\\\nH_A:    & \\mu_1 &lt; \\mu_2\n\\end{array} \\right.\\]\nAgain, we rephrase things as: \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\mu_2 - \\mu_1 = 0   \\\\\nH_A:    & \\mu_2 - \\mu_1 &gt; 0\n\\end{array} \\right.\\] which is now a familiar upper-tailed test on \\(\\delta = \\mu_2 - \\mu_1\\) and \\(\\mu_0 = 0.\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#extensions-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#extensions-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Extensions",
    "text": "Extensions\n\nSpecifically, we would take the same test statistic (which would still follow the same distribution under the null) and use the decision rule \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\mathrm{TS} &gt; c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where \\(c\\) is the appropriate quantile of the approximate t distribution (with degrees of freedom given by the Satterthwaite Approximation).\nA similar result holds for the lower-tailed test- I encourage you to work it out on your own."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#leadup",
    "href": "Pages/Lectures/Lecture11/Lec11.html#leadup",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s, for the moment, don our Sociology hats and say we’re interested in estimating the average monthly income of US Citizens.\nSurveying every single US Citizen and recording their income is not feasible- doing so would be far too expensive (both in terms of monetary cost as well as temporal cost)\nInstead, a natural idea is to take a sample of some subset of US Citizens, and record the average monthly income of these sampled individuals.\nNow, here’s a question: given two separate samples of, say, 125 US Citizens- do we expect the average income of these two samples to be exactly the same, or slightly different?\n\nProbably slightly different!"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#leadup-1",
    "href": "Pages/Lectures/Lecture11/Lec11.html#leadup-1",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Leadup",
    "text": "Leadup\n\nSo, it seems we have two things to keep track of:\n\nThe true average monthly income of US Citizens\nThe average monthly income of a sample of US Citizens.\n\nBy the last point on the previous slide, we can see that the second quantity above is a random variable.\n\nThis is an example of a sample statistic, which is basically any quantity that is computed from our data.\n\nThe true average monthly income of US Citizens is a fixed number, which we call a population parameter.\n\nIn general, a population parameter is just a parameter that relates to the population (e.g. mean, median, variance, etc.)"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#inferential-statistics",
    "href": "Pages/Lectures/Lecture11/Lec11.html#inferential-statistics",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\n\nWe consider the population to be some large group we are interested in studying.\n\nThe population is governed by some set of parameters (e.g. mean, median, variance, etc.)\n\nFrom the population we draw a sample (which is random!), and compute sample statistics to try and make inference about the population parameters.\nThis is the structure of inferential statistics."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#parameter-vs.-statistic",
    "href": "Pages/Lectures/Lecture11/Lec11.html#parameter-vs.-statistic",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Parameter vs. Statistic",
    "text": "Parameter vs. Statistic\n\nIt is extremely important to be able to distinguish a population parameter from a sample statistic.\nThere are a couple of ways to do this.\nThe first is to consider the general structure of the situation: if a given quatntity is describing the population, then it must be a population parameter.\n\nIf, instead, it is describing a sample, then it must be a sample statistic.\n\nThe other way to think about this is through randomness: remember that different samples correspond to different observed values of our sample statistic.\n\nSo, imagine asking yourself: “if I took a different sample, would this quantity change?” If so, then it is a sample statistic (or, more carefully, an observed instance of a sample statistic)."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#worked-out-example",
    "href": "Pages/Lectures/Lecture11/Lec11.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nA veterinarian wishes to determine the true proportion of all cats that suffer from FIV (Feline Immunodeficiency Virus). To that end, she takes a representative sample of 100 cats and finds that 3.2% of cats in this sample have FIV.\n\nIdentify the population.\nIdentify the sample.\nIdentify the population parameter of interest.\nIs the value of \\(3.2\\%\\) a population parameter, or an observed instance of a sample statistic?"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#solution",
    "href": "Pages/Lectures/Lecture11/Lec11.html#solution",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Solution",
    "text": "Solution\n\nthe population is the set of all cats in the world, since we the veterinarian seeks to describe the prevalence of FIV among all cats.\nIn this context, the sample is the set of 100 cats the veterinarian examined.\nThe population parameter of interest is \\(p =\\) “the true proportion of cats that suffer from FIV”.\nThe value of 3.2% is an observed instance of a sample statistic, because if the veterinarian had taken a different sample of 100 cats she likely would have observed a different proportion of FIV-positive cats.\n\nIf, instead, the problem stated that 3.2% of all cats have FIV, then this would be describing a population parameter as it is a statement about the population."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#your-turn",
    "href": "Pages/Lectures/Lecture11/Lec11.html#your-turn",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nA group of (slightly bored) college students would like to determine the true average amount of soda (in liters) in 1-liter soda bottles. To that effect, they purchase 12 different 1-liter soda bottles and find the average amount of soda in these 12 bottles is 0.98L.\n\nIdentify the population.\nIdentify the sample.\nIdentify the population parameter of interest.\nIs the value of 0.98 a population parameter, or an observed instance of a sample statistic?"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#different-population-parameters",
    "href": "Pages/Lectures/Lecture11/Lec11.html#different-population-parameters",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Different Population Parameters",
    "text": "Different Population Parameters\n\nSo far we’ve seen examples of two different population parameters:\n\nA population proportion\nA population mean\n\nOther population parameters exist! For instance, we could talk about the population median, the population variance, or even the population IQR.\nUp until now, I’ve been pretty vague about what “inferences” mean. This is because “making inferences” is a broad term!\nOne part of making inferences is trying to estimate the value of a population parameter.\n\nThis process is called parameter estimation."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#parameter-estimation",
    "href": "Pages/Lectures/Lecture11/Lec11.html#parameter-estimation",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Parameter Estimation",
    "text": "Parameter Estimation\n\nBoth Worked-Out Example 1 and Exercise 1 were (implicitly) problems about parameter estimation.\n\nIn Worked-Out Example 1, the veterinarian wanted to estimate the true proportion of FIV-positive cats\nIn Exercise 1, the college students wanted to estimate the true average amount of soda in 1L soda bottles.\n\nIn general, we use a sample statistic to estimate a population parameter. Some common estimators of some population parameters are:"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#a-wrench-in-the-spanner",
    "href": "Pages/Lectures/Lecture11/Lec11.html#a-wrench-in-the-spanner",
    "title": "PSTAT 5A: Lecture 11",
    "section": "A Wrench in the Spanner",
    "text": "A Wrench in the Spanner\n\nNow, can anyone see a potential problem in using a sample statistic to estimate a population parameter?\n\nThat’s right- the sample statistics are random!\nFor example, every time the veterinarian in Worked-Out Example 1 took a new sample of 100 cats she would obtain a different estimate for the true proportion of cats that have FIV.\nSo, we need some way to express the uncertainty that comes from the randomness of these sample statistics.\n\nTo explore this, let’s do a live demo using Python.\n\nIn the demo, I will simulate drawing several samples of 500 cats, recording the proportion of FIV-positive cats, and drawing the distribution of these sample proportions.\n\nBy the way, the distribution of a sample statistic is called the sampling distribution of that statistic."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#result",
    "href": "Pages/Lectures/Lecture11/Lec11.html#result",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Result",
    "text": "Result\n\nBefore we summarize these results, we should introduce a bit of notation.\n\nWe let \\(\\widehat{P}\\), read “p-hat” denote the random variable that is the proportion of a hypothetical sample.\nWe let \\(\\widehat{p}\\) denote an observed instance of \\(\\widehat{P}\\); i.e. the sample proportion of a particular sample.\n\nWith this notation in mind, we can see that the demo illustrated the following fact: \\(\\widehat{P}\\) is normally distributed!\n\nIt turns out that the expected value of \\(\\widehat{P}\\) is \\(p\\), the true population proportion, and the standard deviation (which, in the context of estimation, is sometimes called the standard error) is \\[ \\sqrt{\\frac{p(1 - p)}{n} } \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#central-limit-theorem-for-proportions",
    "href": "Pages/Lectures/Lecture11/Lec11.html#central-limit-theorem-for-proportions",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Central Limit Theorem for Proportions",
    "text": "Central Limit Theorem for Proportions\n\n\n\n\n\n\nImportant\n\n\n\nIf we have reasonably representative samples taken from a population with true proportion \\(p\\) and let \\(\\widehat{P}\\) denote the sample proportion, then \\[ \\widehat{P} \\sim \\mathcal{N}\\left( p, \\ \\sqrt{ \\frac{p(1 - p)}{n} } \\right) \\] provided that\n\n\n\\(np \\geq 10\\)\n\\(n(1 - p) \\geq 10\\)\n\n\n\n\n\n\n\nThe two conditions above are sometimes referred to as the success-failure conditions, and must be satisfied in order to invoke the Central Limit Theorem for Proportions (CLTP).\n\nWe’ll talk a bit more about “reasonably representative” samples later in the course (time-permitting)."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture11/Lec11.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSuppose a recent study has revealed that 87% of Americans are in favor of offering more healthy options at fast-food restaurants. A surveyor takes a representative sample of size 120 Americans, and records the proportion of these Americans that support offering more healthy options at fast-food restaurants.\n\nDefine the random variable of interest.\nWhat is the probability that fewer than 90% of people in the sample support offering more healthy options at fast-food restaurants?\nWhat is the probability that the proportion of people in the sample who support offering more healthy options at fast-food restaurants lies within 2% of the true proportion of 87%?"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#solutions",
    "href": "Pages/Lectures/Lecture11/Lec11.html#solutions",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Solutions",
    "text": "Solutions\n\nLet \\(\\widehat{P} =\\) the proportion of Americans in the sample of size \\(n = 120\\) that support offering more healthy options at fast-food restaurants."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#solutions-1",
    "href": "Pages/Lectures/Lecture11/Lec11.html#solutions-1",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Solutions",
    "text": "Solutions\n\nWe would like to utilize the Central Limit Theorem for Proportions. First, we check:\n\n\\(np = (120) \\cdot (0.87) = 104.4 \\geq 10 \\ \\checkmark\\)\n\\(n(1 - p) = (120) \\cdot (1 - 0.87) \\approx 15.6 \\geq 10 \\ \\checkmark\\)\n\n\nSince both of the success-failure conditions are satisfied, we know that \\[ \\widehat{P} \\sim \\mathcal{N}\\left(0.87, \\ \\sqrt{\\frac{0.87 \\cdot (1 - 0.87)}{120}} \\right) \\sim \\mathcal{N}\\left(0.87, \\ 0.031 \\right) \\] and so \\[ \\mathbb{P}(\\widehat{P} \\leq 0.9) = \\mathbb{P}\\left( \\frac{\\widehat{P} - 0.87}{0.031} \\leq \\frac{0.9 - 0.87}{0.031} \\right) = \\mathbb{P}\\left( \\frac{\\widehat{P} - 0.87}{0.031} \\leq 0.97 \\right) \\] which equates to around \\(\\boxed{0.8340 = 83.40\\%}\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#solutions-2",
    "href": "Pages/Lectures/Lecture11/Lec11.html#solutions-2",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Solutions",
    "text": "Solutions\n\nWe seek \\(\\mathbb{P}(0.85 \\leq \\widehat{P} \\leq 0.89)\\).\n\nFirst, we write this as \\(\\mathbb{P}(\\widehat{P} \\leq 0.89) - \\mathbb{P}(\\widehat{P} \\leq 0.85)\\)\nNext, we standardize: \\[\\begin{align*}\nz_1 & = \\frac{0.89 - 0.87}{0.031} \\approx 0.65   \\\\\nz_2 & = \\frac{0.85 - 0.87}{0.031} \\approx -0.65\n\\end{align*}\\]\nFinally, we consult a table to see that the desired probability is \\[ 0.7422 - 0.2578 = \\boxed{0.4844 = 48.44\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#your-turn-1",
    "href": "Pages/Lectures/Lecture11/Lec11.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nAt a certain company, it is known that 65% of employees are from underrepresented minorities (UMs). A representative sample of 80 employees is taken, and the proportion of people from UMs is recorded.\n\nDefine the random variable of interest, and use the notational convention introduced above.\nWhat is the probability that greater than than 50% of people in the sample are from UMs?\nWhat is the probability that the proportion of people in the sample who are from UMs lies within 5% of the true proportion of 65%?"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#recap",
    "href": "Pages/Lectures/Lecture17/Lec17.html#recap",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Recap",
    "text": "Recap\n\nBefore the second midterm, we discussed Hypothesis Testing on population proportions.\nThat is: given a population with true proportion \\(p\\), we took samples and tried to use the data collected in these samples to assess the validity of the claim \\(H_0: \\ p = p_0\\) (for some fixed value \\(p_0\\)) against some alternative hypothesis (two-sided, lower-tailed, or upper-tailed)\nIndeed, we can conduct hypothesis testing on any population parameter.\nOne parameter of interest to statisticians is the population mean \\(\\mu\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#hypothesis-testing-for-the-mean-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#hypothesis-testing-for-the-mean-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Hypothesis Testing for the Mean",
    "text": "Hypothesis Testing for the Mean\n\nThe classification of tests doesn’t change much: we still have two-sided, lower-tailed and upper-tailed tests for the mean.\nMore specifically, if our null is \\(H_0: \\ \\mu = \\mu_0\\) then:\n\nA two-sided alternative is \\(H_A: \\ \\mu \\neq \\mu_0\\)\nA lower-tailed alternative is \\(H_A: \\ \\mu &lt; \\mu_0\\)\nAn upper-tailed alternative is \\(H_A: \\ \\mu &gt; \\mu_0\\)\nA simple-vs-simple alternative is \\(H_A: \\ \\mu = \\mu_1\\) for some \\(\\mu_1 \\neq \\mu_0\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#hypothesis-testing-for-the-mean-2",
    "href": "Pages/Lectures/Lecture17/Lec17.html#hypothesis-testing-for-the-mean-2",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Hypothesis Testing for the Mean",
    "text": "Hypothesis Testing for the Mean\n\nOur testing procedure will be similar to that of hypothesis tests for a proportion: we compute a test statistic, and then compare this value of the test statistic to a critical value that is determined by both the distribution of the test statistic under the null as well as the significance level \\(\\alpha\\).\nLet’s focus on a two-sided test for now.\nOur test statistic will certainly involve \\(\\overline{X}\\), the sample mean.\n\nSpecifically, it will be some standardized version of \\(\\overline{X}\\).\nHowever, exactly how we standardize will be dependent upon the sampling distribution of \\(\\overline{X}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#sampling-distribution-of-overlinex",
    "href": "Pages/Lectures/Lecture17/Lec17.html#sampling-distribution-of-overlinex",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Sampling Distribution of \\(\\overline{X}\\)",
    "text": "Sampling Distribution of \\(\\overline{X}\\)\n\n\n\n\n\n\ngraph TB\n  A[Is the population Normal?  . ] --&gt; |Yes| B{{Use Normal .}}\n  A --&gt; |No| C[Is n &gt;= 30?  .]\n  C --&gt; |Yes| D[sigma or s?  .]\n  C --&gt; |No| E{{cannot proceed   .}}\n  D --&gt; |sigma| F{{Use Normal .}}\n  D --&gt; |s| G{{Use t }}"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#test-statistic",
    "href": "Pages/Lectures/Lecture17/Lec17.html#test-statistic",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nWhat this means is that our test statistic itself will take different forms depending on the information that is provided.\nSpecifically, we have \\[ \\mathrm{TS} = \\begin{cases}\n\\displaystyle \\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}}    & \\text{if } \\quad  \\begin{array}{rl} \\bullet & \\text{pop. is normal, OR} \\\\ \\bullet & \\text{$n \\geq 30$ AND $\\sigma$ is known} \\end{array} \\quad \\stackrel{H_0}{\\sim} \\mathcal{N}(0, \\ 1) \\\\[5mm]\n\\displaystyle \\frac{\\overline{X} - \\mu_0}{s / \\sqrt{n}}         & \\text{if } \\quad  \\begin{array}{rl} \\bullet & \\text{$n \\geq 30$ AND $\\sigma$ is not known} \\end{array} \\quad \\stackrel{H_0}{\\sim} t_{n - 1}\n\\end{cases} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#test-statistic-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#test-statistic-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nIn the two-sided case, our critical value will still be the value \\(c\\) that ensures \\[ \\mathbb{P}(|\\mathrm{TS}| &gt; c) = \\alpha \\] (i.e. we reject \\(H_0\\) whenever \\(\\overline{X}\\) is far away from \\(\\mu_0\\)) meaning \\(c\\) will the \\((\\alpha /2) \\times 100\\%\\) percentile, scaled by negative 1, of either the standard normal distribution or the \\(t_{n - 1}\\) distribution.\n\nAgain, don’t forget about how the symmetry of the normal and t-distributions translates to relationships between percentiles!\n\nOur test in the two-sided case will then take the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| &gt; c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]\nLet’s do an example."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#worked-out-example",
    "href": "Pages/Lectures/Lecture17/Lec17.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nAn article published by the US Census claims that in 2019 the average American took around 27.6 minutes to commute to work. To test this claim, Jenny took a representative sample of 101 Americans in 2019 and observed that these individuals had a combined average commute time of around 20.1 minutes and a standard deviation of around 12.4 minutes.\nUse the information from Jenny’s sample, along with an \\(\\alpha = 0.05\\) level of significance, to test the claim that the average commute time is 27.6 minutes against a two-sided alternative."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nIt’s a good idea to first determine which distribution we need to use.\nIs the population normally distributed?\n\nIn other words, are the commute times of all Americans normally distributed?\nNo; or, at least, there is not enough inforamtion to conclude that they are.\n\nIs our sample large enough?\n\nYes; \\(n = 101 \\geq 30\\)\n\nDo we have \\(\\sigma\\) or \\(s\\)?\n\nWe only have \\(s\\), the standard devaition of commute times of Jenny’s sample."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nTherefore, what distribution are we going to use?\n\nThe \\(t_{100}\\) distribution.\n\nIn other words, \\[ \\mathrm{TS} = \\frac{\\overline{X} - 27.6}{12.4 / \\sqrt{101}} \\stackrel{H_0}{\\sim} t_{100} \\]\nFrom the \\(t-\\)table, we see that the critical value will be \\(1.98\\).\nAdditionally, the observed value of our test statistic (based on Jenny’s sample) is \\[ \\frac{20.1 - 27.6}{12.4 / \\sqrt{101}} \\approx -6.08 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-2",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-2",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nOur test rejects \\(H_0\\) when \\(|\\mathrm{TS}| &gt; c\\).\nIn this case, \\(|\\mathrm{TS}| = |-6.07| = 6.07 &gt; 1.98\\)\nTherefore, we reject the null:\n\n\n\nAt an \\(\\alpha = 0.05\\) level of significance, there was sufficient evidence to reject the Census’ claim that the average commute time of Americans was 27.6 minutes in favor of the alternative that the true average commute time was not 27.6 minutes."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#one-sided-tests",
    "href": "Pages/Lectures/Lecture17/Lec17.html#one-sided-tests",
    "title": "PSTAT 5A: Lecture 17",
    "section": "One-Sided Tests",
    "text": "One-Sided Tests\n\nAnalogously as with the hypothesis testing for proportions, the lower-tailed test of a population mean takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\mathrm{TS} &lt; c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] and the upper-tailed test of a population mean takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\mathrm{TS} &gt; c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]\nI’ll keep stressing this: there is no need to memorize these forms if you understand what the null and alternative are saying and how that translates to comparing \\(\\overline{X}\\) to \\(\\mu_0\\)!"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture17/Lec17.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nMcGaucho’s claims that, on average, customers will only need to wait 10 minutes before getting their food. Dubious of these claims, Markus takes a representative sample of 40 customers and finds that these customers waited an combined average of 15 minutes before receiving their food, with a standard deviation of 10 minutes. Based on this, he decides to perform an upper-tailed test on McGaucho’s claims at an \\(\\alpha = 0.01\\) level of significance. Conduct the test, and state your conclusions."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-3",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-3",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nAgain, we should ask ourselves what distribution we are going to use.\nIs the population normally distributed?\n\nIn other words, are the wait times of all customers normally distributed?\nNo; or, at least, there is not enough inforamtion to conclude that they are.\n\nIs our sample large enough?\n\nYes; \\(n = 40 \\geq 30\\)\n\nDo we have \\(\\sigma\\) or \\(s\\)?\n\nWe only have \\(s\\), the standard devaition of commute times of Markus’ sample.\n\nHence, we will use the \\(t_{39}\\) distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-4",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-4",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nNow, we are using an upper-tailed alternative, meaning our critical value is the point such that \\((1 - 0.01) = 0.99\\) area lies to the left (or, equivalently, \\(0.01\\) lies to the right).\nThis means our critical value is 2.43, and we reject the null whenever \\(\\mathrm{TS} &gt; 2.43\\).\nThe observed value of our test statistic is \\[ \\frac{15 - 10}{10 / \\sqrt{40}} = 3.162 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-5",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-5",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nSince \\(\\mathrm{TS} = 3.162 &gt; 2.43\\), we reject the null:\n\n\n\nAt an \\(\\alpha = 0.01\\) level of significance, there was sufficient evidence to reject McGaucho’s’ claim that the average wait time of customers is 10 minutes in favor of the alternative that the true wait time is longer than 10 minutes."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#your-turn",
    "href": "Pages/Lectures/Lecture17/Lec17.html#your-turn",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nA university administrator at Gaucho University (GU) claims that GU students get, on average, 8 hours of sleep per night. A sample of 35 students had a combined average of 7.72 hours of sleep and a standard deviation of 3.8 hours. Using an \\(\\alpha = 0.05\\) level of significance, test the administrator’s claims against a…\n\n…two-sided alternative\n…lower-tailed alternative"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#recap-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#recap-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Recap",
    "text": "Recap\n\nLast time, we discussed Hypothesis Testing on population proportions.\nThat is: given a population with true proportion \\(p\\), we took samples and tried to use the data collected in these samples to assess the validity of the claim \\(H_0: \\ p = p_0\\) (for some fixed value \\(p_0\\)) against some alternative hypothesis (two-sided, lower-tailed, or upper-tailed)\nThe test we constructed last time compared the value of the test statistic to some critical value, and made a decision about rejection based on the comparison between these two values."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#lower-tailed-example",
    "href": "Pages/Lectures/Lecture17/Lec17.html#lower-tailed-example",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Lower-Tailed Example",
    "text": "Lower-Tailed Example\n\nFor illustrative purposes, let’s consider a lower-tailed test: \\[ \\left[ \\begin{array}{rr}\nH_0:    & p = p_0   \\\\\nH_A:    & p &lt; p_0\n\\end{array} \\right.\\]\nUnder appropriate conditions, our test statistic \\[ \\mathrm{TS} = \\frac{\\widehat{P} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}} \\] follows the standard normal distribution under the null."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#lower-tailed-example-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#lower-tailed-example-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Lower-Tailed Example",
    "text": "Lower-Tailed Example\n\n\nRecall that \\(\\alpha\\), the level of significance, is constructed to be the \\(\\alpha \\times 100\\)th percentile of the standard normal distribution:"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#reject-or-not",
    "href": "Pages/Lectures/Lecture17/Lec17.html#reject-or-not",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Reject or Not?",
    "text": "Reject or Not?"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#reject-or-not-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#reject-or-not-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Reject or Not?",
    "text": "Reject or Not?"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#leadup",
    "href": "Pages/Lectures/Lecture17/Lec17.html#leadup",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Leadup",
    "text": "Leadup\n\nSo, up until now, our decision has been based on comparing the raw value of the test statistic to the critical value.\n\nIf the test statistic is smaller than the critical value, we reject.\nOtherwise, we fail to reject.\n\nAgain, remember the intuition behind why this is: if we observe a value of \\(\\widehat{P}\\) that is much smaller than \\(p_0\\), this leads credence to the claim that \\(p &lt; p_0\\) (i.e. this leads credence to our alternative and away from our null).\n\nValues of \\(\\widehat{P}\\) less than \\(p_0\\) lead to very negative values of \\(\\mathrm{TS}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#leadup-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#leadup-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Leadup",
    "text": "Leadup\n\nNow, we like to think in terms of areas.\nSo, here’s an idea: let’s translate our reject/fail to reject scheme to be in terms of areas.\nSpecifically, here is what I mean.\nConsider again a situation in which we reject the null:"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#leadup-2",
    "href": "Pages/Lectures/Lecture17/Lec17.html#leadup-2",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Leadup",
    "text": "Leadup\n\nSo, we can rephrase our decision as follows:\n\nReject \\(H_0\\) if the area to the left of the test statistic is smaller than \\(\\alpha\\).\nOtherwise, fail to reject \\(H_0\\).\n\nNotice that the area to the left of the test statistic, since we are dealing with a lower-tailed test, is equivalent to “the probability of observing something as or more extreme, under the null, as the value we currently observe”.\n\nHere, “extreme” can be thought of as in the direction of the alternative; in other words, we consider the area in the direction of the sign in the alternative hypothesis.\n\nThis is an example of what we call a p-value."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#p-values",
    "href": "Pages/Lectures/Lecture17/Lec17.html#p-values",
    "title": "PSTAT 5A: Lecture 17",
    "section": "p-Values",
    "text": "p-Values\n\nThe textbook defines \\(p-\\)values as:\n\n\n\n[…] the probability of observing data at least as favorable to the alternative hypothesis as our current data set, if the null hypothesis were true.\n\n\n\nAnother way to think about it is this: if the null were true, what is the probability that we would observe something even more extreme (i.e. even “farther away”, in the direction of the alternative) as our current observations?\nNote that exactly how we compute \\(p-\\)values depends heavily on our alternative."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#the-test-statistic",
    "href": "Pages/Lectures/Lecture17/Lec17.html#the-test-statistic",
    "title": "PSTAT 5A: Lecture 17",
    "section": "The Test Statistic",
    "text": "The Test Statistic"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#p-value-lower-tailed-test",
    "href": "Pages/Lectures/Lecture17/Lec17.html#p-value-lower-tailed-test",
    "title": "PSTAT 5A: Lecture 17",
    "section": "p-value; Lower-Tailed Test",
    "text": "p-value; Lower-Tailed Test"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#p-value-upper-tailed-test",
    "href": "Pages/Lectures/Lecture17/Lec17.html#p-value-upper-tailed-test",
    "title": "PSTAT 5A: Lecture 17",
    "section": "p-value; Upper-Tailed Test",
    "text": "p-value; Upper-Tailed Test"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#p-value-two-sided-test",
    "href": "Pages/Lectures/Lecture17/Lec17.html#p-value-two-sided-test",
    "title": "PSTAT 5A: Lecture 17",
    "section": "p-value; Two-Sided Test",
    "text": "p-value; Two-Sided Test"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture17/Lec17.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle. To test that claim, we take a representative sample of 500 US households and observe that 89.4% of these households own a vehicle.\n\nCompute the \\(p-\\)value of our observed value of the statistic, assuming we are using a lower-tailed alternative.\nCompute the \\(p-\\)value of our observed value of the statistic, assuming we are using a two-sided alternative."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-6",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-6",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nLet’s first compute the observed value of our test statistic: \\[ \\mathrm{TS} = \\frac{0.894 - 0.917}{\\sqrt{\\frac{(0.917) \\cdot (1 - 0.917)}{500}}} = -1.86 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-7",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-7",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nIn the lower-tailed case, our p-value is\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can compute this in one of two ways."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-8",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-8",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nWe could simply use Python:\n\n\n\nimport scipy.stats as sps\nsps.norm.cdf(-1.86)\n\n0.03144276298075271\n\n\n\n\n\n\nOr, we could use our z-table: \\(0.0314\\)\n\n\n\n\nEither way, looks like our p-value in the lower-tailed case is \\(0.0314\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-9",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-9",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nIn the two-sided case, our p-value is\n\n\n\n\n\n\n\n\n\n\n\n\n\nSame deal as before: we can compute this using either Python or our table."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-10",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-10",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nAlternatively, note that we can utilize the symmetry of the normal distribution in our favor- the area we’re interested in is simply twice the area we found in the lower-tailed case!\nSo, our \\(p-\\)value in the two-sided case is simply \\(2 \\times (0.0314) = 0.0628\\).\nIf you don’t believe me:\n\n\n\nsps.norm.cdf(-1.86) + (1 - sps.norm.cdf(1.86))\n\n0.06288552596150537"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#p-values-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#p-values-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "p-Values",
    "text": "p-Values\n\nOf course, p-values are not restricted to the case of hypothesis testing a proportion; we can absolutely compute \\(p-\\)values in a setting where we are testing a population mean!\n\n\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nConsider again the setup of Worked-Out Example 2: McGaucho’s claims that, on average, they are able to serve customers their food in 10 minutes. Dubious of these claims, Markus takes a representative sample of 40 customers and finds that these customers waited an combined average of 15 minutes before receiving their food, with a standard deviation of 10 minutes.\n\nCompute the p-value if we were to use a lower-tailed alternative.\nCompute the p-value if we were to use a two-sided alternative."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-11",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-11",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nRecall that we ended up needing to use the \\(t_{39}\\) distribution as opposed to the standard normal distribution.\nThis doesn’t affect our computations of the p-value much, except for the fact that now we will need to use Python (as our \\(t-\\)table doesn’t really give us probabilities).\nThe observed value of the test statistic is \\[ \\frac{15 - 10}{10 / \\sqrt{40}} = 3.162 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-12",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-12",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nOn a graph, this looks like:"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-13",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-13",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nUsing a lower-tailed alternative:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsps.t.cdf(3.162, 39)\n\n0.998485142798897"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-14",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-14",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nUsing a two-sided alternative:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2 * sps.t.cdf(-3.162, 39)\n\n0.0030297144022061392\n\n\nor\n\nsps.t.cdf(-3.162, 39) + (1 - sps.t.cdf(3.162, 39))\n\n0.0030297144022060733"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#what-do-we-do-with-a-p-value",
    "href": "Pages/Lectures/Lecture17/Lec17.html#what-do-we-do-with-a-p-value",
    "title": "PSTAT 5A: Lecture 17",
    "section": "What Do We Do with a p-Value?",
    "text": "What Do We Do with a p-Value?\n\nOkay, so after we’ve computed our p-value… what do we do?\nWe reject when p is small; specifically, we reject when p is less than the level of significance.\nSo, in other words, we now have two ways to conduct hypothesis tests under our belt: using critical values, and using p-values.\nI’d like to stress- these two frameworks are completely equivalent!\n\nYour decision using critical values will always be the same as your decision using p-values, by construction."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#your-turn-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nUCSB claims that 40% of its undergraduate student body are first-generation students (as in, the first in their families to earn a Bachelor’s degree). To test this claim, a representative sample of 120 undergraduates is taken and it is found that 43% of these students are first-generation. Suppose we use an \\(\\alpha = 0.05\\) level of significance, and we test the University’s claims against a two-sided alternative.\n\nConduct the test using critical values.\nConduct the test using p-values."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#your-turn-2",
    "href": "Pages/Lectures/Lecture17/Lec17.html#your-turn-2",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nA botanist claims that the average weight of a chestnut is around 5 grams. To test this claim, a representative sample of 35 chestnuts is taken; these chestnuts have a combined average weight of 4.87 grams and a standard deviation of 1.82 grams. Suppose we use an \\(\\alpha = 0.05\\) level of significance, and a lower-tailed alternative.\n\nConduct the test using critical values.\nConduct the test using p-values."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#criticisms-of-p-values",
    "href": "Pages/Lectures/Lecture17/Lec17.html#criticisms-of-p-values",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Criticisms of p-Values",
    "text": "Criticisms of p-Values\n\nI should note that there are some growing criticism within the statistical and data scientific communities regarding p-values.\nFor one thing, they can be easily misinterpreted.\n\nTake a look at this Wikipedia page for a list of common misinterpretations of p-values to make sure you don’t make any of these (and yes, I might test you on some of these on a quiz/exam!)\n\nTo fully understand and appreciate the more nuanced criticisms, however, we would need a bit of extra background/theory pertaining to the framework of Hypothesis Testing - material that we don’t cover in this class.\n\nI encourage you to take PSTAT 120B and PSTAT 120C if you’d like to learn more about this!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#leadup",
    "href": "Pages/Lectures/Lecture19/Lec19.html#leadup",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Leadup",
    "text": "Leadup\n\nConsider the following situation: a new drug claims to significantly lower systolic blood pressure.\nTo ensure these claims are validated, a clinical trial collects several volunteers and groups them into four groups: a control group, and three groups which each are administered a different dosage of the drug.\nIf the drug is truly ineffective, we would imagine the average systolic blood pressure of each group to be fairly similar to the average systolic blood pressures of the other groups.\nIn other words, given \\(k\\) groups, each with some population mean \\(\\mu_i\\) (for \\(i = 1, 2, \\cdots, k\\)), we wish to determine whether or not all of the \\(\\mu_i\\)’s are the same."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#anova",
    "href": "Pages/Lectures/Lecture19/Lec19.html#anova",
    "title": "PSTAT 5A: Lecture 19",
    "section": "ANOVA",
    "text": "ANOVA\n\nThis is the basic setup of Analysis of Variance (often abbreviated as ANOVA).\nGiven \\(k\\) groups, each with mean \\(\\mu_i\\), we wish to test the null hypothesis that all group means are equal (i.e. \\(H_0: \\ \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\)) against the alternative that at least one of the group means differs significantly from the others.\n\n\n\n\n\n\n\n\nCAUTION\n\n\nNote the alternative hypothesis!\n\n\n\n\n\nIt is NOT correct to write the alternative as \\(H_A: \\ \\mu_1 \\neq \\mu_2 \\neq \\cdots \\neq \\mu_k\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#anova-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#anova-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "ANOVA",
    "text": "ANOVA\n\nHere is the general idea.\nObservations within each group will have some amount of variability (by virtue of being random observations).\nHowever, the sample means (of the groups) themselves will also have some variability (again, due to the fact that sample means are random).\nThe question ANOVA seeks to answer is: is the variability between sample means greater than what we would expect due to chance alone?\n\nIf so, we may have reason to believe that at least one of the group means differs significantly from the others; i.e. we would have evidence to reject the null."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#anova-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#anova-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "ANOVA",
    "text": "ANOVA\n\nAlright, let’s see if we can make things a little more concrete.\nSuppose we have some measure of variability between sample means; perhaps we could call this \\(\\mathrm{MS}_{\\mathrm{G}}\\) (for mean-squared between groups).\nSuppose we also have some measure of variability within each group (i.e. variability due to chance); perhaps we could call this \\(\\mathrm{MS}_{\\mathrm{E}}\\) can be thought of as a measure of variability within groups; i.e. as a sort of variance due to error/randomness.\nThen, the ratio \\[ F = \\frac{\\mathrm{MS}_\\mathrm{G}}{\\mathrm{MS}_{\\mathrm{E}}} \\] could be used as a test statistic."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#anova-3",
    "href": "Pages/Lectures/Lecture19/Lec19.html#anova-3",
    "title": "PSTAT 5A: Lecture 19",
    "section": "ANOVA",
    "text": "ANOVA\n\nWhy would this be a good test statistic?\nWell, we previously stated that we would reject the null (that all group means are the same) in favor of the alternative when the variability across groups is much larger than the variability due to chance.\n\nAnother way to say “the variability across groups is much larger than the variability due to chance” is to say that \\(\\mathrm{MS}_{\\mathrm{G}}\\) is much larger than \\(\\mathrm{MS}_{\\mathrm{E}}\\).\nWhen this is true, \\(F\\) will be much larger than 1.\nHence, we would reject \\(H_0\\) for large values of \\(F\\).\n\nSo, this quantity (which we call the F-statistic) is a good test statistic for our test!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#anova-4",
    "href": "Pages/Lectures/Lecture19/Lec19.html#anova-4",
    "title": "PSTAT 5A: Lecture 19",
    "section": "ANOVA",
    "text": "ANOVA\n\nAlright, so how would we compute this statistic mathematically?\n\nIn other words, what are good formulas for \\(\\mathrm{MS}_{\\mathrm{G}}\\) and \\(\\mathrm{MS}_{\\mathrm{E}}\\)?\n\nTo answer this, let’s establish some notation.\nRecall that we have \\(k\\) groups; our assumption is that we have some set of observations from each group.\nLet’s use the notation \\(X_{ij}\\) to mean the ith observation from the jth group.\n\nFor instance, \\(X_{2, 4}\\) would denote the second observation from the fourth group.\nSince we have \\(k\\) groups, the index \\(j\\) is allowed to take values from \\(\\{1, 2, \\cdots, k\\}\\).\nLet’s also assume that we have \\(n_k\\) observations from group \\(k\\), so that \\(i\\) takes values from \\(\\{1, 2, \\cdots, n_k\\}\\) for the different values of \\(k\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#the-data",
    "href": "Pages/Lectures/Lecture19/Lec19.html#the-data",
    "title": "PSTAT 5A: Lecture 19",
    "section": "The Data",
    "text": "The Data"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#the-data-example",
    "href": "Pages/Lectures/Lecture19/Lec19.html#the-data-example",
    "title": "PSTAT 5A: Lecture 19",
    "section": "The Data: Example",
    "text": "The Data: Example\n\n\nHere we have \\(k = 3\\) groups, with \\(n_1 = 5\\) observations from group 1, \\(n_2 = 3\\) observations from group 2, and \\(n_3 = 4\\) observations from group 3."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#msg",
    "href": "Pages/Lectures/Lecture19/Lec19.html#msg",
    "title": "PSTAT 5A: Lecture 19",
    "section": "MSG",
    "text": "MSG\n\nLet’s start with MSG, our quantity that is supposed to measure variability between group means.\nWe start by computing the means of our \\(k\\) sets of observations. In other words, we compute \\[ \\overline{X}_j = \\frac{1}{n_j} \\sum_{i=1}^{n} X_{ij} \\] for each \\(j = 1, 2, \\cdots, k\\).\nNext, we compute the grand mean, which is simply the mean of all observations: \\[ \\overline{X} = \\frac{1}{n} \\sum_{j = 1}^{k} \\sum_{i=1}^{n_j} X_{ij}; \\quad n = \\sum_{j=1}^{k} n_j \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#msg-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#msg-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "MSG",
    "text": "MSG\n\nThen, we compute the sum of squares between groups \\[ \\mathrm{SS}_{\\mathrm{G}} = \\sum_{j = 1}^{k} n_j (\\overline{X}_j - \\overline{X})^2 \\] and obtain our quantity for MSG by dividing this by \\(k - 1\\): \\[ \\mathrm{MS}_{\\mathrm{G}} = \\frac{1}{k - 1} \\cdot \\mathrm{SS}_{\\mathrm{G}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#mse",
    "href": "Pages/Lectures/Lecture19/Lec19.html#mse",
    "title": "PSTAT 5A: Lecture 19",
    "section": "MSE",
    "text": "MSE\n\nNow, let’s discuss MSE, our quantity that is supposed to measure variability due to chance/within each group.\nThere are two main ways statisticians go about finding this quantity.\nFirst, we compute the sum of squares total \\[ \\mathrm{SS}_{\\mathrm{T}} = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (X_{ij} - \\overline{X})^2 \\]\nThen we compute the sum of squared errors \\[ \\mathrm{SS}_{\\mathrm{E}} = \\mathrm{SS}_{\\mathrm{T}} - \\mathrm{SS}_{\\mathrm{G}} \\]\nAnd finally we divide by \\(n - k\\) to obtain MSE: \\[ \\mathrm{MS}_{\\mathrm{E}} = \\frac{1}{n - k} (\\mathrm{SS}_{\\mathrm{E}}) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#what-we-have-so-far",
    "href": "Pages/Lectures/Lecture19/Lec19.html#what-we-have-so-far",
    "title": "PSTAT 5A: Lecture 19",
    "section": "What We Have So Far",
    "text": "What We Have So Far\n\nLet’s take stock of what we have so far.\nOur hypotheses are \\[ \\left[ \\begin{array}{rl}\nH_0:    & \\mu_1 = \\mu_2 = \\cdots = \\mu_k    \\\\\nH_A:    & \\text{at least one of the group means is different}\n\\end{array} \\right. \\]\nOur test statistic is \\[ F = \\frac{\\mathrm{MS}_\\mathrm{G}}{\\mathrm{MS}_{\\mathrm{E}}} \\]\nOur test takes the form \\[ \\texttt{decision}(\\mathrm{F}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } F &gt; c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#whats-left",
    "href": "Pages/Lectures/Lecture19/Lec19.html#whats-left",
    "title": "PSTAT 5A: Lecture 19",
    "section": "What’s Left?",
    "text": "What’s Left?\n\nSo, what’s left?\n\nThat’s right; the critical value \\(c\\).\n\nJust as we did before, we construct our test to control for Type I errors.\nThat is, given a significance level \\(\\alpha\\), we select the value of \\(c\\) that ensures \\[ \\mathbb{P}_{H_0}(F &gt; c) = \\alpha \\]\nThis requires knowledge about the sampling distribution of \\(F\\)!\nIt turns out that, if we assume observations within each group are normally distributed (which ends up being a very crucial assumption), the statistic \\(F\\) follows what is known as the F-distribution.\nAs such, the critical value of our test is the appropriate percentile of the F-distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#the-f-distribution",
    "href": "Pages/Lectures/Lecture19/Lec19.html#the-f-distribution",
    "title": "PSTAT 5A: Lecture 19",
    "section": "The F-Distribution",
    "text": "The F-Distribution\n\nThe F-distribution is quite different from the distributions we have encountered thus far.\nFor one thing, it admits only nonnegative values in its state space (i.e. it has state space \\([0, \\infty)\\)).\nAdditionally, it takes two parameters, referred to as the numerator degrees of freedom and the denominator degrees of freedom (sometimes abbreviated as just “degree of freedom 1” and “degree of freedom 2”.)\nTo notate the fact that a random variable \\(X\\) follows the F-distribution with degrees of freedom d1 and d2, respectively, we write \\[ X \\sim F_{\\texttt{d1}, \\ \\texttt{d2}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#the-f-distribution-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#the-f-distribution-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "The F-Distribution",
    "text": "The F-Distribution"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#the-test-statistic",
    "href": "Pages/Lectures/Lecture19/Lec19.html#the-test-statistic",
    "title": "PSTAT 5A: Lecture 19",
    "section": "The Test Statistic",
    "text": "The Test Statistic\n\nRecall that our test statistic in ANOVA is \\[ F = \\frac{\\mathrm{MS}_\\mathrm{G}}{\\mathrm{MS}_{\\mathrm{E}}} \\]\nAs mentioned previously, if we assume normality within groups, then, under the null, \\(F\\) follows the F-distribution with \\(k - 1\\) and \\(n - k\\) degrees of freedom, respectively, where \\(k\\) is the number of groups and \\(n\\) is the combined number of observations: \\[ F = \\frac{\\mathrm{MS}_\\mathrm{G}}{\\mathrm{MS}_{\\mathrm{E}}}  \\stackrel{H_0}{\\sim} F_{k - 1, \\ n - k} \\]\nSince we reject only for large values of \\(F\\), our p-values are always computed as upper-tail probabilities:"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#p-values-in-anova",
    "href": "Pages/Lectures/Lecture19/Lec19.html#p-values-in-anova",
    "title": "PSTAT 5A: Lecture 19",
    "section": "p-Values in ANOVA",
    "text": "p-Values in ANOVA"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#anova-tables",
    "href": "Pages/Lectures/Lecture19/Lec19.html#anova-tables",
    "title": "PSTAT 5A: Lecture 19",
    "section": "ANOVA Tables",
    "text": "ANOVA Tables\n\nAs mentioned previously, computing \\(\\mathrm{MS}_{\\mathrm{G}}\\) and \\(\\mathrm{MS}_{\\mathrm{E}}\\) is not particularly challenging, but it can be quite tedious.\n\nThe process can be somewhat illuminating, though, so I have put a question on this week’s homework walking you through how to perform an ANOVA by hand.\n\nAs such, computer software is usually utilized to carry out an ANOVA.\nOften times, the results of such a computer-generated ANOVA are displayed in what is known as an ANOVA Table.\nI find ANOVA tables to be best described by way of an example."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#example",
    "href": "Pages/Lectures/Lecture19/Lec19.html#example",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\nReference Example 1\n\n\n\n\nA state official would like to determine whether or not the average fluoride levels in the water supplies across three cities, called A, B, and C, are the same.\nTo that end, they took a sample of 100 fluoride measurements from city A, 110 from city B, and 100 from city C.\n\n\n\n\n\n\n\nAfter running an ANOVA in a computer software, the following output was produced:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDF\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nBetween Groups\n2\n0.541799\n0.2709\n1.30682497808\n0.272179497817\n\n\nResiduals\n307\n63.6399\n0.207296\n\n\n\n\n\n\n\nLet’s go through this table in more detail."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#interpreting-an-anova-table",
    "href": "Pages/Lectures/Lecture19/Lec19.html#interpreting-an-anova-table",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Interpreting an ANOVA Table",
    "text": "Interpreting an ANOVA Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nDF\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nBetween Groups\n2\n0.343981\n0.171991\n0.927001041587\n0.396843557892\n\n\nResiduals\n307\n56.9591\n0.185534\n\n\n\n\n\n\n\n\n\nThe DF column gives the degrees of freedom of the resulting F-statistic.\n\nRecall that these are meant to be \\(k - 1\\) and \\(n - k\\) respectively.\n\\(k\\) is the number of groups (i.e. 3, in this example), hence the numerator d.f. of 2.\n\\(n\\) is the total number of observations (i.e. 100 + 110 + 100 = 310, in this example), hence the denominator d.f. of 307 (310 - 3 = 307).\n\nThe rownames (“Between Groups” and “Residuals”) refer to whether the specified entry is in relation to a between group calculation or a within group calculation.\n\nThe reason for calling the second row “Residuals” instead of “Within Group” will become clear next week, after we talk about Regression."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#interpreting-an-anova-table-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#interpreting-an-anova-table-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Interpreting an ANOVA Table",
    "text": "Interpreting an ANOVA Table\n\n\n\n\n\n\n\n\n\n\n\n\nDF\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nBetween Groups\n2\n0.343981\n0.171991\n0.927001041587\n0.396843557892\n\n\nResiduals\n307\n56.9591\n0.185534\n\n\n\n\n\n\n\n\nThe Sum Sq column is precisely the sum of squares between groups and the sum of squared errors, SSG and SSE respectively.\nThe Mean Sq entries are found by dividing the corresponding Sum Sq entry by the corresponding degree of freedom.\n\nThat is: 0.171991 = 0.343981 / 2\nAnd: 0.185534 = 56.9591 / 307\n\nFinally, the F value is simply the ration of the two Mean Sq values, and represents the value of our test statistic.\n\nThe Pr(&gt;F) is just our p-value."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#analyzing-the-data",
    "href": "Pages/Lectures/Lecture19/Lec19.html#analyzing-the-data",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Analyzing the Data",
    "text": "Analyzing the Data\n\nMaybe that was a little too opaque.\nIf you’d like, there is the actual data:\n\n\n\n\n                A          B               C\n1     1.765793252  1.0969903    0.8893372313\n2     1.457639487  0.2870172     1.673980678\n3    0.3272998539  0.6808363     1.251111092\n4     1.095808075  1.2379006    0.9022573626\n5     1.410667987  0.9922533    0.7675894536\n6    0.7399572196  0.8899447     1.117615437\n7     1.232755793  0.9316065    0.3737125741\n8     1.154274263  1.3892834      1.67516293\n9     1.102145715  1.3327322     1.296435132\n10    1.012699895  0.7350924     1.607457771\n11    1.316513095  1.1671763     0.816422448\n12    1.701518687  0.8616771      1.45467294\n13   0.6174671647  2.0117302     1.379557127\n14    1.614137039  0.7542573    0.3801190768\n15    1.214315065  0.9248685     1.137032315\n16    1.322568806  0.9104472     1.211800026\n17   0.5316988941  0.8743895    0.8112870512\n18    1.167568439  1.1064417    0.9079229895\n19    1.842268501  1.1592197     1.266283383\n20   0.5600975571  0.4025717     1.052936953\n21   0.1111358597  0.8207547    0.6178776414\n22   0.2283138521  0.5218210     1.336153889\n23    1.233035082  1.5444821     1.172517814\n24    2.292483665  2.0361300     1.182130396\n25    1.661845627  1.6032929     1.113093457\n26    1.936311107  1.1884932     1.272059779\n27    1.149574608  1.4590064    0.9412990966\n28    1.798998189  1.1167241    0.6817587653\n29    0.964376006  1.7126866    0.7361788226\n30    1.406602092  1.3171558     1.567508811\n31   0.9663414057  0.9580213    0.1620266063\n32   0.8253454929  0.5981795     1.313243321\n33    1.166354148  0.7940150    0.4209655028\n34   0.8619289925  1.1755322    0.3460173367\n35     1.75423654  0.5596656    0.6170441472\n36     1.19750664  0.9612827    0.9541098688\n37    1.300104994  1.5859705     1.398728351\n38   0.9311838315  1.0617239     0.305719329\n39    1.728236132  1.8441543     1.823333996\n40    0.734015249  1.6288662     1.593675665\n41    1.430115776  0.9349482     1.010341883\n42   0.9245640543  0.5062758    0.9393893861\n43   0.6302833201  1.0307835    0.6409104121\n44   0.8553313916  0.8576851     1.334618267\n45   0.6977044289  1.2011012     1.100960662\n46   0.9936511806  1.6957632     1.705428937\n47    0.930429877  0.6007584    0.7612029733\n48    1.256084968  0.8945088     1.316574767\n49    1.382576335  1.1829400     1.053627858\n50    1.026289871  1.5039893    0.6888426861\n51    1.087047332  1.2542858     1.926036819\n52    1.244547102  1.2157191     1.195822426\n53   0.8300604643  1.5878198     1.349114526\n54     1.45408001  1.2474061    0.5891445312\n55    1.521112369  0.9551983    0.7448206699\n56    1.201790399  1.0729141       1.1078628\n57    2.297351832  0.7671714    0.8069951795\n58    1.558729469  1.2686659     1.340910022\n59    1.043863764  1.4270002     1.267264358\n60   0.9189097764 -0.1418387    0.7179455626\n61    0.983908872  1.5372595     1.156821193\n62   0.8491355502  1.0273917      1.07181121\n63    1.664392577  1.2343554    0.9961661192\n64   0.7510949848  1.3149496     0.553034481\n65    1.059438908  0.1319587    0.6940211732\n66   0.8353519595  0.5716283     1.164702828\n67    1.623091428  0.9167595     1.526293084\n68   0.3907219854  0.7334437    0.6334109797\n69   0.9187504084  1.1596691     1.617731702\n70    1.039047154  0.8440360      1.15219132\n71    1.259678211  0.3655715    0.4985811837\n72    1.330451451  1.2949189     1.215525468\n73   0.9921050538  1.4900517     1.468598686\n74    1.594536229  1.2757410     1.188977611\n75    1.257376889  0.7648802     2.113607293\n76    2.333825528  0.9456562     1.208900453\n77   0.3458392564  1.2693712     1.471653814\n78    1.410300332  1.2912796     1.074960214\n79   0.5774337312  1.4527445     1.511033358\n80   0.7009955912  0.9322296 -0.008007406026\n81    2.092542296  0.4816478    0.6841148682\n82    1.972407074  0.7784213    0.9060748612\n83   0.1719072597  1.2635539     1.296767106\n84   0.9886131504  1.3475898     1.154749047\n85    1.067076076  1.9400199     1.558352183\n86  0.03414394576  1.0968726      1.23062258\n87    1.075584744  1.1478458     1.163298305\n88    1.296670609  0.9790805     1.004720958\n89    1.208632573  0.8877082     1.143756914\n90   0.1028031151  1.3627101     1.443280618\n91    1.653854117  1.1913910     1.409884981\n92    1.222271988  0.9670882     1.465768561\n93    1.069043985  0.6181210    0.9620196655\n94   0.7230535196  1.0007721    0.7572245096\n95    1.455979508  0.8936071    0.8578699627\n96    1.559134576  1.5520695     1.227264384\n97   0.8589534306  1.4288504     1.795155782\n98    1.144793806  0.6410757      1.17770267\n99    1.513499311  1.5168673     1.288251183\n100    0.12274394  0.8212093     0.323627882\n101            NA  1.1032550              NA\n102            NA  1.5310975              NA\n103            NA  0.6400988              NA\n104            NA  1.1141597              NA\n105            NA  0.5978099              NA\n106            NA  0.2734801              NA\n107            NA  1.4446391              NA\n108            NA  1.0612909              NA\n109            NA  0.7210530              NA\n110            NA  0.4153793              NA"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#analyzing-the-data-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#analyzing-the-data-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Analyzing the Data",
    "text": "Analyzing the Data\n\nWhoops- maybe that’s too detailed.\nAny ideas on how we might be able to get a better sense of the data that doesn’t involve looking at all those numbers?\nMaybe… something we learned in Week 1?"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#checking-assumptions",
    "href": "Pages/Lectures/Lecture19/Lec19.html#checking-assumptions",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Checking Assumptions",
    "text": "Checking Assumptions\n\nFinally, I should mention: every good statistician and data scientists starts by checking assumptions.\nOne of the key assumptions in ANOVA is that observations within each group are normally distributed.\nHow can we check that?\n\nThat’s right: QQ-plots!\n\nIn the upcoming lab, you’ll begin to start talking about how to start statistical analyses.\n\nSpecifically, you will learn about something called Exploratory Data Analysis (EDA), part of which entails producing any diagnostic tools you may need to produce in order to ensure assumptions are being satisfied!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#qq-plots-for-the-fluoride-dataset",
    "href": "Pages/Lectures/Lecture19/Lec19.html#qq-plots-for-the-fluoride-dataset",
    "title": "PSTAT 5A: Lecture 19",
    "section": "QQ-Plots for the Fluoride Dataset",
    "text": "QQ-Plots for the Fluoride Dataset"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#recap-of-probability",
    "href": "Pages/Lectures/Lecture09/Lec09.html#recap-of-probability",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Recap of Probability",
    "text": "Recap of Probability\n\nRecall the basic ingredients of probability we have discussed so far:\n\nExperiment: any procedure we can repeat an infinite number of times, where on each repetition there is a fixed set of things (called outcomes) that can happen.\nOutcome space (\\(\\boldsymbol{\\Omega}\\)): the set of all outcomes associated with a particular experiment\nEvent: a subset of the outcome space\nProbability: a function that maps events to a number; specifically, one that quantifies our beliefs about a particular event"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#an-experiment",
    "href": "Pages/Lectures/Lecture09/Lec09.html#an-experiment",
    "title": "PSTAT 5A: Lecture 09",
    "section": "An Experiment",
    "text": "An Experiment\n\nLet’s actually conduct an experiment together!\nSpecifically, suppose we toss a coin 3 times and record the outcomes.\nFirst question: what is the outcome space?"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-1",
    "href": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-1",
    "title": "PSTAT 5A: Lecture 09",
    "section": "An Experiment",
    "text": "An Experiment\n\n\n\n\n\n\n\ntree_diagram\n\n\n\nbase\no\n\n\n\nH1\nH\n\n\n\nbase-&gt;H1\n\n\n\n\n\nT1\nT\n\n\n\nbase-&gt;T1\n\n\n\n\n\nH21\nH\n\n\n\nH1-&gt;H21\n\n\n\n\n\nT21\nT\n\n\n\nH1-&gt;T21\n\n\n\n\n\nH22\nH\n\n\n\nT1-&gt;H22\n\n\n\n\n\nT22\nT\n\n\n\nT1-&gt;T22\n\n\n\n\n\nH311\nH\n\n\n\nH21-&gt;H311\n\n\n\n\n\nT311\nT\n\n\n\nH21-&gt;T311\n\n\n\n\n\nH321\nH\n\n\n\nT21-&gt;H321\n\n\n\n\n\nT321\nT\n\n\n\nT21-&gt;T321\n\n\n\n\n\nH312\nH\n\n\n\nH22-&gt;H312\n\n\n\n\n\nT312\nT\n\n\n\nH22-&gt;T312\n\n\n\n\n\nH322\nH\n\n\n\nT22-&gt;H322\n\n\n\n\n\nT322\nT\n\n\n\nT22-&gt;T322"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-2",
    "href": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-2",
    "title": "PSTAT 5A: Lecture 09",
    "section": "An Experiment",
    "text": "An Experiment\n\nOkay, let’s conduct this experiment!\n\n\n\n\nviewof toss = Inputs.button(\"Toss\")\n\n\n\n\n\n\n\ndummy = toss + 1\ncoin = [\"H\", \"T\"]\ns1 = coin[Math.floor(Math.random()*coin.length*dummy/dummy)];\ns2 = coin[Math.floor(Math.random()*coin.length*dummy/dummy)];\ns3 = coin[Math.floor(Math.random()*coin.length*dummy/dummy)];\ns4 = coin[Math.floor(Math.random()*coin.length*dummy/dummy)];\n[s1, s2, s3];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditionally, let’s keep track of the number of heads we observe each time we run this experiment.\n\nIn fact, let’s do this on the whiteboard."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-3",
    "href": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-3",
    "title": "PSTAT 5A: Lecture 09",
    "section": "An Experiment",
    "text": "An Experiment\n\n\nAlright, let’s make note of a few things.\n\n\n\nNote that each time we run this experiment, we (sure enough) get an element of the outcome space.\nBut, also note that each time we run the experiment, we get a (potentially) different number of heads.\nIn fact, each outcome in the outcome space corresponds to a different number of heads:"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-4",
    "href": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-4",
    "title": "PSTAT 5A: Lecture 09",
    "section": "An Experiment",
    "text": "An Experiment\n\n\n\nOutcome\nNumber of Heads\n\n\n\n\n(H,  H,   H)\n3\n\n\n(H,   H,   T)\n2\n\n\n(H,   T,   H)\n2\n\n\n(T,   H,   H)\n2\n\n\n(H,   T,   T)\n1\n\n\n(T,   H,   T)\n1\n\n\n(T,   T,   H)\n1\n\n\n(T,   T,   T)\n0"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#random-variables",
    "href": "Pages/Lectures/Lecture09/Lec09.html#random-variables",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Random Variables",
    "text": "Random Variables\n\nThis leads us to the notion of random variables.\nLoosely speaking, a random variable is a variable or process with a random numerical outcome.\nWe denote random variables using capital letters; e.g. \\(X\\), \\(Y\\), \\(Z\\), \\(W\\), etc.\nSo, for example, \\(X =\\) “the number of heads in 3 tosses of a coin” is a random variable because (a) it is a numerical outcome of an experiment and (b) it is random (i.e. its value changes depending on the outcome of the experiment).\n\nBy the way, note that we also use capital letters to denote events. So, how will we know whether something is an event or a random variable?\nThat’s right; based on how it is defined! So, again, make sure you are defining everything clearly and explicitly."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#state-space",
    "href": "Pages/Lectures/Lecture09/Lec09.html#state-space",
    "title": "PSTAT 5A: Lecture 09",
    "section": "State Space",
    "text": "State Space\n\nA key part of the definition of random variables is that they must be numerical.\nWhat this means is we can always look at the set of values a random variable could take: this is what we call the state space of a random variable.\nFor example: if \\(X =\\) “number of heads in 3 tosses of a coin”, we see that \\(X\\) will only ever be \\(0\\), \\(1\\), \\(2\\), or \\(3\\).\n\nThis is because it is not possible to toss 3 coins and get, say, 5 heads, or a negative number of heads!\n\nWe often denote the state space of a random variable using the notation \\(S_{\\verb|&lt;variable&gt;|}\\); e.g. \\(S_X\\) to mean the state space of \\(X\\), \\(S_Y\\) to mean the state space of \\(Y\\), etc."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#classifying-random-variables",
    "href": "Pages/Lectures/Lecture09/Lec09.html#classifying-random-variables",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Classifying Random Variables",
    "text": "Classifying Random Variables\n\nBecause random variables are numerical, their state spaces will always be numerical sets of values.\nThis means we can classify state spaces using our Variable Classification scheme from Week 1!\n\nSpecifically: the state space \\(S_X\\) of a random variable will either have “jumps”, or not.\n\nWe extend the same classification language to random variables:\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nGiven a random variable \\(X\\), we say that:\n\n\\(X\\) is a discrete random variable (or just “\\(X\\) is discrete) if \\(S_X\\) is has jumps\n\\(X\\) is a continuous random variable (or just “\\(X\\) is continuous) if \\(S_X\\) has no jumps"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#leadup",
    "href": "Pages/Lectures/Lecture09/Lec09.html#leadup",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s return to our coin tossing example.\nWhat is the probability that we observe zero heads?"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#leadup-1",
    "href": "Pages/Lectures/Lecture09/Lec09.html#leadup-1",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Leadup",
    "text": "Leadup\n\nWell, in the language of our random variable \\(X\\) (which counts the number of heads in these three tosses of our fair coin), we can translate “zero heads” to the event “\\(\\{X = 0\\}\\)’’, meaning we want to find \\(\\mathbb{P}(X = 0)\\).\nObserving zero heads is equivalent to observing all tails, meaning the event \\(\\{X = 0\\}\\) is equivalent to the event { (T,  T,  T) }.\nNow, up to this point I have been careful to avoid explicitly mentioning whether our coin is fair or not.\n\nFor the time being, let’s assume that the probability our coin lands ‘heads’ on any given toss is some fixed value \\(p\\). (If the coin were fair, then \\(p = 0.5\\) but let’s not make that assumption yet.)"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#probability-mass-function",
    "href": "Pages/Lectures/Lecture09/Lec09.html#probability-mass-function",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\n\nThe table on the previous slide is called a probability mass function, and is often abbreviated as p.m.f..\nIn general, the p.m.f. of an arbitrary random variable \\(X\\) is a table or formula that specifies all the possible values a random variable can take (i.e. the state space), along with the probability with which the random variable attains those values.\nWe use the term “function” to describe this because, in abstraction, we can notate the p.m.f. as \\[ p_X(k) := \\mathbb{P}(X = k) \\] where \\(k\\) can be any value in the state space of \\(X\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#example",
    "href": "Pages/Lectures/Lecture09/Lec09.html#example",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSuppose we toss three fair coins independently, and let \\(X\\) denote the number of heads observed. Construct the p.m.f. (probability mass function) of \\(X\\).\n\n\n\n\n\n\nBy our work from above, the p.m.f. of \\(X\\) is given by \\[\\begin{array}{r|cccc}\n\\boldsymbol{k}    &     0   & 1   & 2   & 3   \\\\\n\\hline\n\\boldsymbol{\\mathbb{P}(X = k)}   & 1/8   & 3/8  & 3/8 &  1/8\n\\end{array}\\]\nBy the way, notice that the probabilities in the p.m.f. sum up to 1.\n\nThis is not a coincidence! Because the probabilities represent the probabilities of all values \\(X\\) can take, they must sum up to 1."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#properties-of-pmfs",
    "href": "Pages/Lectures/Lecture09/Lec09.html#properties-of-pmfs",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Properties of PMF’s",
    "text": "Properties of PMF’s\n\nThis leads us to posit the following two properties of probability mass functions:\n\n\n\n\n\n\n\n\n\nProperties of a PMF\n\n\n\n\nThe values in a PMF must sum to 1\nThe values in a PMF must always be nonnegative\n\n\n\n\n\n\n\n\nAlso: we implicitly set probabilities not contained in the p.m.f. to be zero.\n\nFor instance: in our coin tossing example, \\(\\mathbb{P}(X = 1.5) = 0\\).\nThis makes sense! If \\(k \\notin S_X\\), then by definition of the state space it is impossible for \\(X\\) to attain the value \\(k\\), and so \\(\\mathbb{P}(X = k) = 0\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example",
    "href": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nA random variable \\(X\\) has the following p.m.f.: \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1.4   & 0   & 3   & 4.15   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1 & 0.2 & \\boldsymbol{a} & 0.6\n\\end{array}\\] What must be the value of \\(\\boldsymbol{a}\\)?\n\n\n\n\n\n\nBecause the values in a p.m.f. must sum to 1, we must have \\[ 0.1 + 0.2 + a + 0.6 = 1 \\] which means \\[ a = 1 - (0.1 + 0.2 + 0.6) = \\boxed{0.1} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nA random variable \\(X\\) has the following p.m.f.: \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1.4   & 0   & 3   & 4.15   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1 & 0.2 & 0.1 & 0.6\n\\end{array}\\] Compute both \\(\\mathbb{P}(X = 0)\\) and \\(\\mathbb{P}(X \\leq 0)\\).\n\n\n\n\n\n\nFor \\(\\mathbb{P}(X = 0)\\), we can simply read off the corresponding element from the p.m.f.:"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#expected-value",
    "href": "Pages/Lectures/Lecture09/Lec09.html#expected-value",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Expected Value",
    "text": "Expected Value\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe expected value (or just expectation) of a discrete random variable \\(X\\) is \\[ \\mathbb{E}[X] = \\sum_{\\text{all $k$}} k \\cdot \\mathbb{P}(X = k) \\] where the sum ranges over all values of \\(k\\) in the state space.\n\n\n\n\n\n\nIn words: multiply each value in the state space by the corresponding probability, and then sum.\nThe expected value is a sort of ‘center’ of a random variable."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-4",
    "href": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-4",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nA random variable \\(X\\) has the following p.m.f.: \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1.4   & 0   & 3   & 4.15   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1 & 0.2 & 0.1 & 0.6\n\\end{array}\\] Compute \\(\\mathbb{E}[X]\\).\n\n\n\n\n\n\nWe compute \\[\\begin{align*}\n\\mathbb{E}[X]   & = \\sum_{\\text{all $k$}} k \\cdot \\mathbb{P}(X = k)   \\\\[5mm]\n  & = (-1.4) \\cdot \\mathbb{P}(X = -1.4) + (0) \\cdot \\mathbb{P}(X = 0) + (3) \\cdot \\mathbb{P}(X = 3)   \\\\\n    & \\hspace{10mm} + (4.15) \\cdot \\mathbb{P}(X = 4.15)    \\\\[5mm]\n    & = (-1.4) \\cdot (0.1) + (0) \\cdot (0.2) + (3) \\cdot (0.1) + (4.15) \\cdot (0.6) = \\boxed{2.65}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#variance-and-sd",
    "href": "Pages/Lectures/Lecture09/Lec09.html#variance-and-sd",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Variance and SD",
    "text": "Variance and SD\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe variance of a discrete random variable \\(X\\) is \\[ \\mathrm{Var}(X) = \\sum_{\\text{all $k$}} (k - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = k) \\] where the sum ranges over all values of \\(k\\) in the state space. The standard deviation is the square root of the variance: \\[ \\mathrm{SD}(X) = \\sqrt{\\mathrm{Var}(X)} \\]\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Formula for Variance\n\n\n\n\\[ \\mathrm{Var}(X) = \\left( \\sum_{\\text{all $k$}} k^2 \\cdot \\mathbb{P}(X = k) \\right) - \\left( \\mathbb{E}[X] \\right)^2 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-5",
    "href": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-5",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 5\n\n\n\n\nA random variable \\(X\\) has the following p.m.f.: \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1.4   & 0   & 3   & 4.15   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1 & 0.2 & 0.1 & 0.6\n\\end{array}\\] Compute \\(\\mathrm{Var}(X)\\) and \\(\\mathrm{SD}(X)\\).\n\n\n\n\n\n\nWe previously found that \\(\\mathbb{E}[X] = 2.65\\).\nHence, we need only to find \\(\\sum_{k} k^2 \\cdot \\mathbb{P}(X = x)\\): \\[\\begin{align*}\n\\sum_{\\text{all $k$}} k^2 \\cdot \\mathbb{P}(X = k) & = (-1.4)^2 \\cdot \\mathbb{P}(X = -1.4) + (0)^2 \\cdot \\mathbb{P}(X = 0) + (3)^2 \\cdot \\mathbb{P}(X = 3)   \\\\\n    & \\hspace{10mm} + (4.15)^2 \\cdot \\mathbb{P}(X = 4.15)    \\\\[5mm]\n    & = (-1.4)^2 \\cdot (0.1) + (0)^2 \\cdot (0.2) + (3)^2 \\cdot (0.1) + (4.15)^2 \\cdot (0.6) = 11.4295\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#your-turn",
    "href": "Pages/Lectures/Lecture09/Lec09.html#your-turn",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\nExercise 1\n\n\n\n\nSuppose \\(X\\) is a random variable with p.m.f. (probability mass function) given by \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1 & 0 & 1 & 2   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.3 & 0.2 & 0.1 & \\boldsymbol{a}\n\\end{array}\\]\n\nFind the state space \\(S_X\\) of \\(X\\).\nFind the value of \\(\\boldsymbol{a}\\)\nFind \\(\\mathbb{P}(X = 0.5)\\)\nFind \\(\\mathbb{P}(X \\leq 1)\\)\nFind \\(\\mathbb{P}(X &gt; 1)\\)\nFind \\(\\mathbb{E}[X]\\)\nFind \\(\\mathrm{Var}(X)\\) and \\(\\mathrm{SD}(X)\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#your-turn-1",
    "href": "Pages/Lectures/Lecture09/Lec09.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\nExercise 2\n\n\n\n\nConsider the following game: a fair six-sided die is rolled. If the number showing is 1 or 2, you win a dollar; if the number showing is 3, 4, or 5 you win 2 dollars; if the number showing is 6, you lose 1 dollar. Let \\(W\\) denote your net winnings after playing this game once.\n\nWrite down the state space \\(S_W\\) of \\(W\\).\nFind the p.m.f. of \\(W\\).\nWhat are your expected winnings after one round of the game?"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#back-to-coins",
    "href": "Pages/Lectures/Lecture09/Lec09.html#back-to-coins",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Back to Coins",
    "text": "Back to Coins\n\nAlright, let’s close out this lecture by returning to our coin tossing example.\nAs a reminder: if we let \\(X\\) denote the number of heads in 3 tosses of a \\(p-\\)coin (i.e. a coin that lands ‘heads’ with probability \\(p\\)), the p.m.f. of \\(X\\) is given by\n\n\n\\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     0   & 1   & 2   & 3   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & (1 - p)^3   & 3  p (1 - p)^2  & 3  p^2 (1 - p) &  p^3\n\\end{array}\\]\n\n\nWhat if instead of tossing 3 coins, we had tossed 4? Or 5? Or 10?\nWe could go through the same steps we did before, when deriving the p.m.f. for three tosses, but let’s be a little smarter about this; let’s answer the following more general question:"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#binomial-distribution",
    "href": "Pages/Lectures/Lecture09/Lec09.html#binomial-distribution",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\n\n\n\n\nThe Binomial Distribution\n\n\n\n\nSuppose the probability of a single trial resulting in a ‘success’ is \\(p\\). Letting \\(X\\) denote the number of successes in \\(n\\) independent trials, then we say that \\(X\\) follows the Binomial Distribution with parameters \\(n\\) and \\(p\\). We use the notation \\(X \\sim \\mathrm{Bin}(n, p)\\) to denote this.\n\n\n\n\n\n\n\n\n\n\n\n\nFacts about the Binomial Distribution\n\n\n\n\nIf \\(X \\sim \\mathrm{Bin}(n, p)\\), then\n\n\\(\\displaystyle \\mathbb{P}(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1 - p)^{n - k}\\)\n\\(\\mathbb{E}[X] = np\\) and \\(\\mathrm{Var}(X) = np(1 - p)\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#binomial-conditions",
    "href": "Pages/Lectures/Lecture09/Lec09.html#binomial-conditions",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Binomial Conditions",
    "text": "Binomial Conditions\n\n\n\n\n\n\nFour Conditions to Check\n\n\n\n\nIf \\(X\\) counts the number of successes in \\(n\\) trials, there are four conditions that need to be satisfied in order for \\(X\\) to follow the Binomial Distribution:\n\nThe trials must be independent\nThe number of trials, \\(n\\), must be fixed\nThere should be a well-defined notion of “success” and “failure” on each trial\nThe probability of “success” must remain constant across trials.\n\n\n\n\n\n\n\nSo, remember: \\(X \\sim \\mathrm{Bin}(n, p)\\) just means “\\(X\\) counts the number of successes in \\(n\\) trials, where success occurs with probability \\(p\\) on any given trial, subject to the four conditions above being satisfied."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-6",
    "href": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-6",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 6\n\n\n\n\n\nIf we roll a fair \\(6-\\)sided die \\(13\\) times (assume rolls are independent of each other) and let \\(X\\) denote the number of times we observe an even number, is \\(X\\) binomially distributed?\nIn a large population of \\(100\\) students, of which \\(70\\) own Android phones, we draw a random sample of 10 without replacement and let \\(Y\\) denote the number of students in this sample that have Android phones. Is \\(Y\\) binomially distributed?\nConsider the same setup as in part (b) above, except this time suppose students are selected with replacement. Is \\(Y\\) binomially distributed?"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#part-a",
    "href": "Pages/Lectures/Lecture09/Lec09.html#part-a",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Part (a)",
    "text": "Part (a)\n\nWe check the Binomial Conditions.\n\nIndependent trials? Yup!\nFixed number of trials? Yup! (\\(n = 13\\))\nWell-defined notion of success? Yup! (“success” = “rolling an even number” and “failure” = “rolling an odd number”)\nFixed probability of success? Yup! (\\(p = 1/2\\)).\n\nSince all 4 conditions are satisfied, \\(X\\) binomially distributed: specifically, \\[ X \\sim \\mathrm{Bin}(13, \\ 1/2) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#part-b",
    "href": "Pages/Lectures/Lecture09/Lec09.html#part-b",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Part (b)",
    "text": "Part (b)\n\nWe check the Binomial Conditions.\n\nIndependent trials? ; because sampling is done without replacement, trials are no longer independent (i.e. the result of our second trial is very much dependent on the result of our first).\n\nSince at least one condition is violated, \\(Y\\) does follow the binomial distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#part-c",
    "href": "Pages/Lectures/Lecture09/Lec09.html#part-c",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Part (c)",
    "text": "Part (c)\n\nWe check the Binomial Conditions (a.k.a. the Binomial Criteria).\n\nIndependent trials? Yup!\nFixed number of trials? Yup! (\\(n = 10\\))\nWell-defined notion of success? Yup! (“success” = “owning an Android phone” and “failure” = “not owning an Android phone”)\nFixed probability of success? Yup! (\\(p = 7/10\\)).\n\nSince all 4 conditions are satisfied, \\(Y\\) binomially distributed: specifically, \\[ Y \\sim \\mathrm{Bin}(10, \\ 7/10) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#your-turn-2",
    "href": "Pages/Lectures/Lecture09/Lec09.html#your-turn-2",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\nExercise 3\n\n\n\n\nSuppose Jana tosses \\(65\\) different \\(12-\\)sided dice, independently of each other; let \\(Z\\) denote the number of times a multiple of three results.\n\nVerify that \\(Z\\) follows the Binomial Distribution, and identify its parameters.\nWhat is the probability that Jana observes exactly 23 multiples of three?\nWhat is the expected number of multiples of three Jana will observe?\nWhat is the standard deviation of the number of multiples of three Jana will observe?"
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#smoking",
    "href": "Pages/Lectures/Lecture23/Lec23.html#smoking",
    "title": "PSTAT 5A: Lecture 23",
    "section": "Smoking",
    "text": "Smoking\n\nI previously emphasized that observational studies cannot be used to draw causal conclusions, even if they reveal associations.\nI mentioned this is because of the potential presence of so-called confounding variables.\nLet’s think through a concrete example together.\n\n\n\n\n\n\n\n\nSetup\n\n\nA researcher would like to determine whether or not there is an association between smoking and increased rates of lung cancer. To that end, they perform an observational study in which 50 people who regularly smoke were observed along with 50 people who do not smoke. Lung cancer rates within each group were recorded at the end of the study, and the data clearly (and statistically) displays that the average lung cancer rates among smokers is higher than that among non-smokers."
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#smoking-1",
    "href": "Pages/Lectures/Lecture23/Lec23.html#smoking-1",
    "title": "PSTAT 5A: Lecture 23",
    "section": "Smoking",
    "text": "Smoking\n\nAgain, we cannot then simply conclude that smoking causes lung cancer.\nAll we can conclude is precisely what was stated above: there is statistical evidence to suggest that smoking is associated with higher lung cancer rates.\nWhy? It really boils down to asking: was the control (i.e. non-smoking) group truly similar to the treatment (i.e. smoking) group?\nFor example, what if it turns out that the group of smokers that were selected were also heavy drinkers? In that case, whether or not someone regularly drinks could be a confounding variable as it is one the researcher did not explicitly control for, but that could potentially skew results.\nAdditionally, some studies have shown that smokers tend to be predominantly male. As such, gender could also be a confounding variable."
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#confounding-variables",
    "href": "Pages/Lectures/Lecture23/Lec23.html#confounding-variables",
    "title": "PSTAT 5A: Lecture 23",
    "section": "Confounding Variables",
    "text": "Confounding Variables\n\nThe main point is: there are lots of variables that were not controlled for in this study, but that could be also contributing to the increased rates of lung cancer that was observed.\nHence, the study (as it was conducted above) cannot be used to say that smoking definitively causes lung cancer.\nNow, I’d also like to stress- even if the researcher were to re-do the study as an experiment, we still wouldn’t be able to simply declare that smoking causes lung cancer.\nTo truly establish causal relationships, one must use results from causal inference (which is outside the scope of this course)."
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#case-study-admissions-at-uc-berkeley",
    "href": "Pages/Lectures/Lecture23/Lec23.html#case-study-admissions-at-uc-berkeley",
    "title": "PSTAT 5A: Lecture 23",
    "section": "Case Study: Admissions at UC Berkeley",
    "text": "Case Study: Admissions at UC Berkeley\n\nIn the 1970’s, UC Berkeley conducted an observational study to determine whether or not there was gender bias in the graduate student admittance practices at the university.\n\nA disclaimer: at the time, “gender” was treated as a binary variable with values male and female. I would like to also acknowledge that we now recognize that there are a great deal many more genders than simply “male” and “female”.\n\nOverall, the survey included 8,422 men and 4,321 women.\nOf the men 44% were admitted; of the women only 35% were admitted.\n\nThis difference was also deemed statistically significant.\n\nSo, on the surface, it does appear as though women are being disproportionately denied entry."
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#case-study-admissions-at-uc-berkeley-1",
    "href": "Pages/Lectures/Lecture23/Lec23.html#case-study-admissions-at-uc-berkeley-1",
    "title": "PSTAT 5A: Lecture 23",
    "section": "Case Study: Admissions at UC Berkeley",
    "text": "Case Study: Admissions at UC Berkeley\n\nSomething puzzling happens, however, when we take a look at the data after grouping by major:\n\n\n\n\n\n\n\n\nMen\n\n\nWomen\n\n\n\n\nMajor\n\n\nNum. Applicants\n\n\n% Admitted\n\n\nNum. Applicants\n\n\n% Admitted\n\n\n\n\n\n\nA\n\n\n825\n\n\n62\n\n\n108\n\n\n82\n\n\n\n\nB\n\n\n560\n\n\n63\n\n\n25\n\n\n68\n\n\n\n\nC\n\n\n325\n\n\n37\n\n\n593\n\n\n34\n\n\n\n\nD\n\n\n417\n\n\n33\n\n\n375\n\n\n35\n\n\n\n\nE\n\n\n191\n\n\n28\n\n\n393\n\n\n24\n\n\n\n\nF\n\n\n373\n\n\n6\n\n\n341\n\n\n7"
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#case-study-admissions-at-uc-berkeley-2",
    "href": "Pages/Lectures/Lecture23/Lec23.html#case-study-admissions-at-uc-berkeley-2",
    "title": "PSTAT 5A: Lecture 23",
    "section": "Case Study: Admissions at UC Berkeley",
    "text": "Case Study: Admissions at UC Berkeley\n\nNearly none of the majors on their own display this bias against women.\n\nIndeed, in Major A there almost appears to be a bias against men\n\nSo, what’s going on? How can it be that none of the majors individually display a discrimination against women, but overall they display discrimination against women?\nThe answer actually lies in how difficult each major was to get into.\nFor instance, Major A appears to have an overall 64% acceptance rate, whereas Major E appears to have an overall 53.62% acceptance rate.\n\nMajor A seems to be harder to get into than, say, Major E.\nIndeed, Majors A and B are easier to get into than majors C through F."
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#case-study-admissions-at-uc-berkeley-3",
    "href": "Pages/Lectures/Lecture23/Lec23.html#case-study-admissions-at-uc-berkeley-3",
    "title": "PSTAT 5A: Lecture 23",
    "section": "Case Study: Admissions at UC Berkeley",
    "text": "Case Study: Admissions at UC Berkeley\n\nIndeed, if we look at the Num. Applicants column within each gender, we see that, on the aggregate, men were applying to easier majors!\n\nOver half (i.e. \\((825 + 560) / (825 + 560 + 325 + 417 + 191 + 373) \\approx 51.5\\%\\)) of men applied to Majors A and B, whereas nearly 90% (i.e. $(593 + 375 + 393 + 341) / (108 + 25 + 593 + 375 + 393 + 341) $) of women applied to Majors C through F.\n\nIn other words, difficulty of major was a confounding variable that influenced the acceptance rates.\n\nAfter controlling for this variable, it was actually found that there was no significant difference in addmittance rates between men and women."
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#simpsons-paradox",
    "href": "Pages/Lectures/Lecture23/Lec23.html#simpsons-paradox",
    "title": "PSTAT 5A: Lecture 23",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\n\nWhat this shows us is that relationships between percentages in subgroups can sometimes be reversed after the subgroups are aggregated.\nThis is what we call Simpson’s Paradox."
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#on-the-subject-of-misleading",
    "href": "Pages/Lectures/Lecture23/Lec23.html#on-the-subject-of-misleading",
    "title": "PSTAT 5A: Lecture 23",
    "section": "On the Subject of Misleading",
    "text": "On the Subject of Misleading"
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#what-now-1",
    "href": "Pages/Lectures/Lecture23/Lec23.html#what-now-1",
    "title": "PSTAT 5A: Lecture 23",
    "section": "What Now?",
    "text": "What Now?\n\nAlright, so that was the last bit of new material I wanted to cover in this class.\nBut…. why did we do all of this?\nWhat was the point of this course?\n\nIf you say “the point was for me to finish my major”, well, then, fair enough!\n\nBut, more fundamentally, this course is designed to try and provide an introduction to Data Science."
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#what-now-2",
    "href": "Pages/Lectures/Lecture23/Lec23.html#what-now-2",
    "title": "PSTAT 5A: Lecture 23",
    "section": "What Now?",
    "text": "What Now?\n\nThe key operating word in this is “introduction-” we only just scratched the surface of the topics we discussed!\n\nI sometimes (affectionately) refer to PSTAT 5A as a “table of contents” of statistics and data science.\n\nEven within our own PSTAT department, there are lots of different courses that dive deeper into the subjects we discussed."
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#what-now-3",
    "href": "Pages/Lectures/Lecture23/Lec23.html#what-now-3",
    "title": "PSTAT 5A: Lecture 23",
    "section": "What Now?",
    "text": "What Now?\n\nIf you’d like to learn some more programming (and get a recap of some probability), take PSTAT 10 (where you will learn a very popular programming language among statisticians, called R).\nPSTAT 120A provides a deeper look at Probability, and some more sophisticated probability tools.\nPSTAT 120B and 120C provide a deeper look at inferential statistics, and how to answer much more interesting and complex problems than those we looked at in this course.\n\nYou’ll even learn a third, strange yet useful appraoach to probability called the Bayesian approach.\n\nInterested in Experimental Design? PSTAT 122 is devoted entirely to that!\nWant to learn more about regression (including logistic regression)? Take PSTAT 126 and 131!"
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#story",
    "href": "Pages/Lectures/Lecture23/Lec23.html#story",
    "title": "PSTAT 5A: Lecture 23",
    "section": "Story",
    "text": "Story\n\nNow, I know many of you are graduating this quarter (or have recently graduated).\n\nCongratulations, by the way!!!\n\nFor those of you who are not, and especially for those of you who aren’t quite sure what you want to do, I’d like to leave you with a story.\nI encountered a student who had come into undergrad not knowing what they wanted to do at all.\nThey had a vague inkling that they might want to do math, but after a quarter switched to Econ, then Physics, and finally wound their way into the equivalent of PSTAT 5A at their undergraduate institution.\nBy the end of the quarter, they were so intrigued to learn more the decided to take the analog of 120A, and then 120B, and then, before they knew it, they had completed a degree in statistics!"
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#story-1",
    "href": "Pages/Lectures/Lecture23/Lec23.html#story-1",
    "title": "PSTAT 5A: Lecture 23",
    "section": "Story",
    "text": "Story\n\nThat student…. was me!\nYears ago, I stumbled into the analog of PSTAT 5A and was so enamored by the field that here I am now, pursuing a PhD in it!\nI truly believe statistics and data science are some of the most useful and applicable fields around.\nWherever there is data, there is the need for a data scientist.\nWhenever there is uncertainty, there is the need for a statistician.\nStatistics and Data Science have far reaching applications in so many fields!"
  },
  {
    "objectID": "Pages/Lectures/Lecture23/Lec23.html#story-2",
    "href": "Pages/Lectures/Lecture23/Lec23.html#story-2",
    "title": "PSTAT 5A: Lecture 23",
    "section": "Story",
    "text": "Story\n\nThere is a famous quote from an extremely influential statistician named John Tukey:\n\n\n\nThe best thing about being a statistician is that you get to play in everyone’s backyard.\n\n\n\nI couldn’t agree more!\n\nThough, I would perhaps update this quote to say “statistician and/or data scientist”\n\nSo, now that you’ve learned the basics…\n\n\n…go out and play!"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#last-time",
    "href": "Pages/Lectures/Lecture12/Lec12.html#last-time",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Last Time",
    "text": "Last Time\n\nLast time we began discussing inference on a proportion.\nWe had a population with proportion \\(p\\), drew representative samples from this population, and used the sample proportion \\(\\widehat{P}\\) (i.e. the proportion observed in the sample) as a proxy for \\(p\\).\nOur main result was the Central Limit Theorem for Proportions which states \\[ \\widehat{P} \\sim \\mathcal{N}\\left( p, \\ \\sqrt{ \\frac{p(1 - p)}{n} } \\right) \\] assuming the success-failure conditions are met:\n\n\\(np \\geq 10\\)\n\\(n(1 - p) \\geq 10\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#substitution-approximation",
    "href": "Pages/Lectures/Lecture12/Lec12.html#substitution-approximation",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Substitution Approximation",
    "text": "Substitution Approximation\n\nCan anyone point out a potential difficulty with verifying the success-failure conditions?\nThat’s right; they involve the parameter \\(p\\), which is in many cases unknown!\n\nRemember - in the beginning of last lecture, I mentioned that the whole point of performing statistical inference is to try and make claims about a population parameter that is unknowable, or too difficult to determine exactly.\n\nTo remedy this, we often use the so-called substitution approximation to the success-failure conditions:\n\n\\(n \\widehat{p} \\geq 10\\)\n\\(n(1 - \\widehat{p}) \\geq 10\\)\n\nSometimes, we substitute \\(\\widehat{p}\\) into the formula for the standard deviation of \\(\\widehat{P}\\), as the next example illustrates."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example",
    "href": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nA veterinarian wishes to determine the true proportion of cats that suffer from FIV (Feline Immunodeficiency Virus). To that end, she takes a representative sample of 500 cats and finds that 3.2% of cats in this sample have FIV. What is the probability that the proportion of cats that are FIV-positive in her sample of 500 cats lies within 1 percent of the true proportion of FIV-positive cats?"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solution",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solution",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solution",
    "text": "Solution\n\nLet \\(p\\) denote the true proportion of FIV-positive cats. Let \\(\\widehat{P}\\) denote the proportion of FIV-positive cats in a representative sample of size 500.\n\nDo we know the value of \\(p\\)?\nNo we do not.\n\nWhat we do have is \\(\\widehat{p} = 0.032\\).\n\nTherefore, we use the substitution approximation to the success-failure conditions:\n\\(n \\widehat{p} = (500)(0.032) = 16 \\geq 10 \\ \\checkmark\\)\n\\(n (1 - \\widehat{p}) = (500)(1 - 0.032) = 484 \\geq 10 \\ \\checkmark\\)\n\nSince both conditions are met, the CLT tells us \\[ \\widehat{P} \\sim \\mathcal{N}\\left(p, \\ \\sqrt{\\frac{p(1 - p)}{500}} \\right)\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solution-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solution-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solution",
    "text": "Solution\n\nWe seek \\(\\mathbb{P}(p - 0.01 \\leq \\widehat{P} \\leq p + 0.01)\\).\nOur first step is to write this as \\[ \\mathbb{P}(\\widehat{P} \\leq p + 0.01) - \\mathbb{P}(\\widehat{P} \\leq p - 0.01 ) \\]\nNext, we find the associated \\(z-\\)scores: \\[\\begin{align*}\nz_1   & = \\frac{(p + 0.01) - p}{\\sqrt{\\frac{p(1 - p)}{500}}}  = \\frac{0.01}{\\sqrt{\\frac{p(1 - p)}{500}}}    \\\\\nz_2   & = \\frac{(p - 0.01) - p}{\\sqrt{\\frac{p(1 - p)}{500}}} = - \\frac{0.01}{\\sqrt{\\frac{p(1 - p)}{500}}}  \n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solution-2",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solution-2",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solution",
    "text": "Solution\n\nNow, we can apply the substitution approximation to plug in \\(\\widehat{p}\\) in place of \\(p\\) in the denominator of our \\(z-\\)scores to compute \\[\\begin{align*}\nz_{1, \\ \\text{sub}}   & = \\frac{0.01}{\\sqrt{\\frac{(0.032)(1 - (0.032))}{500}}} = \\frac{0.01}{0.00787}  = 1.27  \\\\\nz_{2, \\ \\text{sub}}   & = - \\frac{0.01}{\\sqrt{\\frac{(0.032)(1 - (0.032))}{500}}} = - \\frac{0.01}{0.00787}  = -1.27\n\\end{align*}\\]\nFinally, consulting our standard normal table, we find the answer to be \\[ 0.8980 - 0.1020 = \\boxed{0.796 = 79.6\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#a-note",
    "href": "Pages/Lectures/Lecture12/Lec12.html#a-note",
    "title": "PSTAT 5A: Lecture 12",
    "section": "A Note",
    "text": "A Note\n\nI’d like to stress that the substitution approximation is just that- an approximation.\nIt is not, for instance, true that \\(\\mathbb{E}[\\widehat{P}] = \\widehat{p}\\); the center of the distribution of \\(\\widehat{P}\\) will always (provided the success-failure conditions are met) be \\(p\\), the true value of the proportion."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#leadup",
    "href": "Pages/Lectures/Lecture12/Lec12.html#leadup",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s quickly take stock of what we’ve learned.\nIf we have a population with some unknown population parameter \\(p\\), we can repeatedly take representative samples, compute the sample proportion in each sample, and construct the sampling distribution of \\(\\widehat{P}\\).\nAssuming the success-failure conditions are met, this sampling distribution will be centered at \\(p\\), the true proportion value, and hence a decent estimator for \\(p\\) would be \\(\\mathbb{E}[\\widehat{P}]\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#leadup-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#leadup-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Leadup",
    "text": "Leadup\n\nHowever, the key assumption in this procedure is our ability to take multiple samples from the population.\nIn many practical situations, this is not feasible.\nSo, here is a new question to consider: given just a single sample from the population, what can we say about \\(p\\)?\nWell, we’ve already seen that it’s risky to simply take \\(\\widehat{p}\\) (i.e. the value of \\(\\widehat{P}\\) that was observed in the sample we took) to be an estimate of \\(p\\), due to the randomness associated with \\(\\widehat{P}\\).\nInstead of looking for point estimates of \\(p\\), what happens if we instead provide intervals we believe may contain \\(p\\)?"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#leadup-2",
    "href": "Pages/Lectures/Lecture12/Lec12.html#leadup-2",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s make things a bit more concrete. Since I like cats, let’s go back to our veterinarian example:\n\n\n\nA veterinarian wishes to determine the true proportion of cats that suffer from FIV (Feline Immunodeficiency Virus). To that end, she takes a representative sample of 100 cats and finds that 3.2% of cats in this sample have FIV.\n\n\n\nAgain, it’s risky to say that “the true proportion of FIV-positive cats is 3.2%” based solely on this sample.\nInstead, we are going to start proposing intervals of values that we believe contain \\(p\\).\nNow, clearly the strengths of our beliefs will depend on the interval we provide.\nFor example, I am 100% confident that the true proportion of FIV-positive cats is somewhere in the interval \\((-\\infty, \\infty)\\).\nBut, suppose we instead consider the interval \\((0.030, \\ 0.034)\\); now we can’t really say that we’re 100% certain this interval covers the true value of \\(p\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nThis is the basic idea of what are known as confidence intervals.\nI particularly like the analogy our textbook (OpenIntro Statistics) uses:\n\n\n\n[…] Using only a point estimate is like fishing in a murky lake with a spear. We can throw a spear where we saw a fish, but we will probably miss. On the other hand, if we toss a net in that area, we have a good chance of catching the fish. (page 181)\n\n\n\nFor the purposes of this class, we will construct confidence intervals for an arbitrary parameter \\(\\theta\\) (e.g. a population proportion \\(p\\), a population mean \\(\\mu\\), etc.) of the form \\(\\widehat{\\theta} \\pm \\mathrm{m.e.}\\) where \\(\\widehat{\\theta}\\) represents some point estimate of \\(\\theta\\) and \\(\\mathrm{m.e.}\\) represents a margin of error.\nSo, for the veterinarian example, our confidence interval will be of the form \\(\\widehat{p} \\pm \\mathrm{m.e.}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nBefore constructing a confidence interval, however, we need to specify our confidence level. In other words, we need first have an idea of how confident we want to be that our interval contains the true parameter value.\nFor example, a 95% confidence interval is an interval \\(\\widehat{\\theta} \\pm \\mathrm{m.e.}\\) that we are 95% confident covers the true value of \\(\\theta\\).\nHere’s a question: based on everything we’ve talked about thus far, do you think higher confidence levels correspond to wider or narrower intervals?\n\nThat’s right: the higher our confidence level, the wider our interval will be.\nAs an extreme example, consider again the slightly absurd confidence interval \\((-\\infty, \\ \\infty)\\); this is a 100% confidence interval because we are 100% confident that it covers the true value of the parameter!\n\nSo, therein lies the tradeoff: the more confidence we want, the wider we need to make our intervals and the less informative they become in pinning down the true value of the parameter."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nAlright, let’s return to our considerations on population proportions.\nAgain, our confidence interval will take the general form \\(\\widehat{p} \\pm \\mathrm{m.e.}\\).\nIt makes sense that the margin of error should include some information about the variability of \\(\\widehat{P}\\). As such, we take our confidence intervals to be of the form \\[ \\widehat{p} \\pm z^{\\ast} \\cdot \\sqrt{ \\frac{p(1 - p)}{n} } \\] where \\(z^{\\ast}\\) is a constant that depends on our confidence level.\n\n\\(z^{\\ast}\\) is sometimes called the confidence coefficient, though that name is not fully standard."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nTo see exactly how this dependency manifests itself, let’s make things a bit more concrete and consider a 95% confidence level. It turns out that this implies \\[ \\mathbb{P}(-z^{\\ast} \\leq Z \\leq z^{\\ast}) = 0.95 \\] where \\(Z \\sim \\mathcal{N}(0, \\ 1)\\).\n\nI’ll try to post some supplementary material for those of you curious as to why this is- for now, I ask you to just take this fact at face value.\n\nAs always, we draw a picture."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-2",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-2",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nThe region we will sketch is the area under the standard normal curve between \\(-z^{\\ast}\\) and \\(z^{\\ast}\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, here’s the slightly peculiar thing- in this case, we know that the area itself must be 0.95. What we don’t know is exactly where those endpoints are."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-3",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-3",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nSome of you may have an inkling that a normal table may be helpful…. and it will be!\nTo make clear how a normal table will help, let’s convert our picture to be in terms of tail areas:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat must be the area of the shaded bit above?\n\nThat’s right: 5% (since the area in the middle is, by construction, 95%)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-4",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-4",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nBecause the standard normal density curve is symmetric, the area of any one of the two tails must be (5% / 2) = 2.5%:\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, what we have shown, is that \\(z^{\\ast}\\) must satisfy the condition \\[ \\mathbb{P}(Z \\leq -z^{\\ast}) = 0.025 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-5",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-5",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nAgain, \\(z^{\\ast}\\) must satisfy \\[ \\mathbb{P}(Z \\leq -z^{\\ast}) = 0.025 \\]\nFrom a normal table, we see that \\[ \\mathbb{P}(Z \\leq -1.96) = 0.025 \\]\nTherefore, we must have \\[ \\mathbb{P}(Z \\leq -z^{\\ast}) = \\mathbb{P}(Z \\leq -1.96) \\] that is, \\(-z^{\\ast} = -1.96\\) or \\(\\boxed{z^{\\ast} = 1.96}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-6",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-6",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nSo, in conclusion, a 95% confidence interval for a population proportion will take the form \\[ \\widehat{p} \\pm 1.96 \\cdot \\sqrt{ \\frac{p(1 - p)}{n} } \\]\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nUse a similar set of reasoning to show that a 90% confidence interval for a population proportion \\(p\\) takes the form \\[ \\widehat{p} \\pm 1.645 \\cdot \\sqrt{\\frac{p(1 - p)}{n}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#quick-aside-percentiles-of-the-standard-normal-distribution",
    "href": "Pages/Lectures/Lecture12/Lec12.html#quick-aside-percentiles-of-the-standard-normal-distribution",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Quick Aside: Percentiles of the Standard Normal Distribution",
    "text": "Quick Aside: Percentiles of the Standard Normal Distribution\n\nAs a quick aside: notice that what we’ve done is actually found various percentiles of the standard normal distribution!\nPercentiles of a distribution are defined much in the same way we defined the percentiles of a list of numbers: the pth percentile of a random variable \\(X\\) is the value \\(\\pi_p\\) such that \\(\\mathbb{P}(X \\leq \\pi_p) = p\\).\nTo find the pth percentile of the standard normal table, here are the steps we use:\n\nFind \\(p\\) in the body of the table\nWhatever \\(z-\\)score that corresponds to the value of \\(p\\) in the table will be the pth percentile\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nFind the 4.55th, 83.4th, and 96.41th percentiles of the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution",
    "href": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Symmetry of the Normal Distribution",
    "text": "Symmetry of the Normal Distribution\n\nLet’s also quickly discuss one more property of the standard normal distribution: its density curve is symmetric about the \\(y-\\)axis.\nThis actually leads to an interesting (and very useful) result about percentiles of the standard normal distribution:\n\n\n\n\n\n\n\n\nResult\n\n\n\n\nThe \\(p\\)th percentile of the standard normal distribution is equal to negative one times the \\((1 - p)\\)th percentile of the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Symmetry of the Normal Distribution",
    "text": "Symmetry of the Normal Distribution\n\nTo see why this is, we sketch a picture. Suppose \\(\\pi_{p}\\) is the \\(p\\)th percentile for the standard normal distribution. Then, the area below must be equal to \\(p\\):"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution-2",
    "href": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution-2",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Symmetry of the Normal Distribution",
    "text": "Symmetry of the Normal Distribution\n\n\nBy the complement rule, the red area below must therefore be \\(1 - p\\):"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution-3",
    "href": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution-3",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Symmetry of the Normal Distribution",
    "text": "Symmetry of the Normal Distribution\n\n\nFinally, it is precisely the symmetry of the standard normal density curve that guarantees the red area below will also be \\(1 - p\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore, if the area to the left of \\(\\pi_p\\) is \\(p\\) (which was our initial assumption), the area to the left of \\(-\\pi_p\\) is \\((1 - p)\\).\n\nIn other words, the \\(p\\)th percentile (\\(\\pi_p\\)) is negative one times the \\((1 - p)\\)th percentile (\\(-\\pi_p\\))."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-7",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-7",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nHere are some common confidence levels, and their corresponding values of \\(z^{\\ast}\\).\n\n\n\n\n\nConfidence Level\nValue of \\(\\boldsymbol{z^{\\ast}}\\)\n\n\n\n\n90%\n1.645\n\n\n95%\n1.96\n\n\n99%\n2.575\n\n\n\n\n\nRecall that these \\(z^{\\ast}\\)’s are simply corresponding percentiles (scaled by \\(-1\\)) of the standard normal distribution.\nTo find \\(z^{\\ast}\\) corresponding to an arbitrary \\(100 \\times (1 - \\alpha)\\) interval we either:\n\nfind the \\((\\alpha / 2) \\times 100\\)th percentile of the standard normal distribution and multiply by \\((-1)\\)\nfind the \\([1 - (\\alpha / 2)] \\times 100\\)th percentile of the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-8",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-8",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nFor example, suppose we want to construct a 99% confidence interval.\nThis is equivalent to constructing a \\((1 - 0.01) \\times 100\\%\\) confidence interval, meaning to find the confidence coefficient we can:\n\nfind the \\((0.01 / 2) \\times 100 = 0.5\\)th percentile of the standard normal distribution and scale by \\(-1\\), which yields a value of around \\(2.575\\)\nfind the \\([1 - (0.01 / 2)] \\times 100 = 99.5\\)th percentile of the standard normal distribution, which (again) yields a value of around \\(2.575\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-9",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-9",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nIn practice: since the value of \\(p\\) is unknown, we typically replace \\(p\\) with \\(\\widehat{p}\\) to obtain an approximate confidence interval: \\[ \\widehat{p} \\pm z^{\\ast} \\cdot \\sqrt{ \\frac{\\widehat{p}(1 - \\widehat{p})}{n}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nA veterinarian wishes to determine the true proportion of cats that suffer from FIV (Feline Immunodeficiency Virus). To that end, she takes a representative sample of 500 cats and finds that 3.2% of cats in this sample have FIV. Construct a 95% confidence interval for the true poportion of FIV-positive cats.\n\n\n\n\n\n\n\nWe simply plug into our formula from above: \\[ (0.032) \\pm 1.96 \\cdot \\sqrt{ \\frac{(0.032) \\cdot (1 - 0.032)}{500}} = \\boxed{0.032 \\pm 0.0155}\\] or, written out more explicitly, \\[ \\boxed{ [0.0165 \\ , \\ 0.0475] } \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#interpreting-confidence-intervals",
    "href": "Pages/Lectures/Lecture12/Lec12.html#interpreting-confidence-intervals",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\n\nOkay, now that we have an example of a confidence interval under our belt, let’s talk about the correct interpretation of confidence intervals.\nThe following are all correct interpretations of our confidence interval:\n\nWe are 95% confident that the true proportion of FIV-positive cats is between 0.0165 and 0.0475.\nWe are 95% confident that the interval \\([0.0165 \\ , \\ 0.0475]\\) covers the true proportion of FIV-positive cats.\n\nHere is a technically incorrect way of interpreting the confidence interval: there is a 95% probability that the true proportion of FIV-positive cats lies between 0.0165 and 0.0475."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#interpreting-confidence-intervals-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#interpreting-confidence-intervals-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\n\nWhy is this typically rejected as an interpretation of a confidence interval?\nBecause this phrasing makes it sound as though the true proportion of FIV-positive cats is a random variable!\n\nThe true proportion of FIV positive cats is a fixed, deterministic value \\(p\\).\nWhat is random are the endpoints of our confidence interval!\nThis is why we phrase our interpretation in terms of “coverage”; it is to highlight the fact that the endpoints of our interval are where our uncertainty (i.e. randomness) comes into play.\n\nI grant that the above is a very subtle point. However, Statisticians are quite particular about wording when it comes to interpreting confidence intervals. As such, we will be particular in this class as well!"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#your-turn",
    "href": "Pages/Lectures/Lecture12/Lec12.html#your-turn",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nAs a film critic, you are interested in determining the true proportion of people that have watched The Mandalorian. You take a representative sample of 100 people, and note that 47% of these people have watched The Mandalorian.\n\nConstruct a 95% confidence interval for the proportion of people that have watched The Mandalorian, and interpret your interval in the context of the problem.\nWhen constructing an 85% confidence interval for the proportion of people that have watched The Mandalorian, would you expect this interval to be wider or shorter than the interval you found in part (a)?\nNow, actually construct an 85% confidence interval for the proportion of people that have watched The Mandalorian and see if this agrees with your answer to part (b)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#your-turn-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nAs a political scientist, Morgan would like to know the true proportion of people in a city that support Candidate A in an upcoming election. To that effect, they take a representative sample of 120 people and determine that 51% of these sampled individuals support Candidate A.\nConstruct an 87% confidence interval for the true proportion of people that support Candidate A."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#previously",
    "href": "Pages/Lectures/Lecture13/Lec13.html#previously",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Previously",
    "text": "Previously\n\nOver the course of the past few lectures, we’ve been dealing primarily with population proportions.\nA natural point estimate of \\(p\\) is \\(\\widehat{P}\\), the sample proportion.\n\nThe Central Limit Theorem for Proportions helped us even further by providing the sampling distribution of \\(\\widehat{P}\\), under certain conditions (the success-failure conditions).\n\nWe then used the sampling distribution of \\(\\widehat{P}\\) to construct confidence intervals for the true proportion \\(p\\).\nNow we will turn our attention to a different population parameter."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#leadup",
    "href": "Pages/Lectures/Lecture13/Lec13.html#leadup",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Leadup",
    "text": "Leadup\n\nRecall from last lecture that any of the descriptive statistics we discussed in Week 1 can be viewed as population parameters, when they apply to the population.\n\nE.g. population proportion (\\(p\\)), population variance (\\(\\sigma^2\\)), etc.\n\nOf particular interest to statisticians is often the population mean, \\(\\mu\\).\nLet’s try and draw some analogies from our work with population proportions.\nWhen trying to make inferences on a population proportion \\(p\\), we used the sample proportion \\(\\widehat{P}\\) as a proxy (specifically, a point estimator).\n\nAny guesses on what we might use as a point estimator of \\(\\mu\\)?\nThat’s right- the sample mean \\(\\overline{X}\\)!"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#notation",
    "href": "Pages/Lectures/Lecture13/Lec13.html#notation",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Notation",
    "text": "Notation\n\nAgain, it will be useful to establish some notation:\n\n\\(\\mu\\) represents the population mean, and is deterministic (i.e. fixed) but unknown.\n\\(\\overline{X}\\) represents the mean of some hypothetical sample, and is therefore random (as different samples result in different sample means)\n\\(\\overline{x}\\) represents the mean of a specific sample, and is therefore deterministic (i.e. “we’ve taken this particular sample right here and computed its mean”).\n\nJust as \\(\\widehat{P}\\) has a sampling distribution, so too does \\(\\overline{X}\\).\nThe sampling distribution of \\(\\overline{X}\\), however, will end up requiring a few more considerations than the sampling distribution of \\(\\widehat{P}\\).\n\nWe will need it, however, in order to construct confidence intervals for \\(\\mu\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#general-confidence-intervals",
    "href": "Pages/Lectures/Lecture13/Lec13.html#general-confidence-intervals",
    "title": "PSTAT 5A: Lecture 13",
    "section": "General Confidence Intervals",
    "text": "General Confidence Intervals\n\nWe will follow the general idea we used before of constructing confidence intervals as \\(\\widehat{\\theta} \\pm \\mathrm{m.e.}\\).\nIn this case, we use \\(\\overline{X}\\) as our point estimator.\nHere is a useful result from Probability Theory:\n\n\n\n\n\n\n\n\nResult\n\n\n\n\nConsider a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). If \\(\\overline{X}\\) denotes the mean of a sample of size \\(n\\) taken from this population, then \\[ \\mathrm{SD}(\\overline{X}) = \\frac{\\sigma}{\\sqrt{n}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#general-confidence-intervals-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#general-confidence-intervals-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "General Confidence Intervals",
    "text": "General Confidence Intervals\n\nTherefore, our confidence intervals will take the form \\[ \\overline{X} \\pm c \\cdot \\frac{\\sigma}{\\sqrt{n}} \\] where the constant \\(c\\) depends on both the sampling distribution of \\(\\overline{X}\\) as well as the confidence level."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#normal-population",
    "href": "Pages/Lectures/Lecture13/Lec13.html#normal-population",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Normal Population",
    "text": "Normal Population\n\nLet’s work on finding the sampling distribution of \\(\\overline{X}\\).\nIt turns out that the first thing we need to ask is whether the underlying population is normally distributed or not.\nIf the underlying population is normally distributed [again with population mean \\(\\mu\\) and population standard deviation \\(\\sigma\\)], we have that \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0, \\ 1) \\] meaning the constant \\(c\\) should be selected as the appropriate percentile of the standard normal distribution: \\[ \\overline{x} \\pm z^{\\ast} \\cdot \\frac{\\sigma}{\\sqrt{n}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example",
    "href": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nThe heights of adult males are assumed to follow a normal distribution with mean 70 in and standard deviation 15 in. A representative sample of 120 adult males is taken, and the average height of males in this sample is recorded.\n\nWhat is the random variable of interest?\nIs the value of 70 in a population parameter or a sample statistic?\nWhat is the probability that the average height of males in the sample is between 69.5 in and 71.5 in?"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\n\\(\\overline{X} =\\) the average height of a sample of 120 adult males.\nThe value of 70 in is a population parameter, as it is the true average height of all adult males.\nThe quantity we seek is \\(\\mathbb{P}(69.5 \\leq \\overline{X} \\leq 71.5)\\). Because the population is normally distributed, we can use our result above to conclude \\[ \\overline{X} \\sim \\mathcal{N}\\left( 70, \\ \\frac{15}{\\sqrt{120}} \\right) \\sim \\mathcal{N}\\left( 70, \\ 1.369 \\right) \\]\n\n\nTo find \\(\\mathbb{P}(69.5 \\leq \\overline{X} \\leq 71.5)\\), we therefore utilize the same techniques we used previously, when dealing with normal distribution problems: \\[\\mathbb{P}(69.5 \\leq \\overline{X} \\leq 71.5) = \\mathbb{P}(\\overline{X} \\leq 71.5) - \\mathbb{P}(\\overline{X} \\leq 69.5) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\n\nThe associated \\(z-\\)scores are \\[\\begin{align*}\nz_1     & = \\frac{71.5 - 70}{\\left( \\frac{15}{\\sqrt{120}} \\right)} \\approx 1.10    \\\\\nz_2     & = \\frac{69.5 - 70}{\\left( \\frac{15}{\\sqrt{120}} \\right)} \\approx -0.37\n\\end{align*}\\]\n\n\n\nThe associated probabilities (from a \\(z-\\)table) are \\(0.8643\\) and \\(0.3557\\), respectively, meaning the desired probability is \\[ 0.8643 - 0.3557 = \\boxed{ 0.5086 = 50.86\\% } \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#non-normal-population",
    "href": "Pages/Lectures/Lecture13/Lec13.html#non-normal-population",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Non-Normal Population",
    "text": "Non-Normal Population\n\nAlright, so that explains what to do if the population values follow a normal distribution.\nBut what if they don’t? In real-world settings, we don’t typically get to know exactly what the population distribution is.\nIf our population is not normally distributed, we need to ask ourselves whether we have a “large enough sample”.\nAdmittedly, there isn’t a single agreed-upon cutoff for “large enough”- for the purposes of this class, we will use \\(n \\geq 30\\) to mean “large enough” and \\(n &lt; 30\\) to therefore be “not large enough.”"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#non-normal-population-n-30",
    "href": "Pages/Lectures/Lecture13/Lec13.html#non-normal-population-n-30",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Non-Normal Population, \\(n < 30\\)",
    "text": "Non-Normal Population, \\(n &lt; 30\\)\n\nIf the population is non-normal, and the sample size is not large enough…\n… we can’t do anything.\nMore specifically, there aren’t any results we can use to confidently make inferences about the population mean- there is just too much uncertainty, between the uncertainty regarding the population’s distribution and the small sample size."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#non-normal-population-n-geq-30",
    "href": "Pages/Lectures/Lecture13/Lec13.html#non-normal-population-n-geq-30",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Non-Normal Population, \\(n \\geq 30\\)",
    "text": "Non-Normal Population, \\(n \\geq 30\\)\n\nIf the population is non-normal, and the sample size is large enough…\n… we’re still (perhaps surprisingly) in business!\nIt turns out that if \\(n\\) is large enough, \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0, \\ 1) \\] that is, the sample mean once again has a normal sampling distribution!\nIn fact, this is such an important result, we give it a name:"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#central-limit-theorem-for-the-sample-mean",
    "href": "Pages/Lectures/Lecture13/Lec13.html#central-limit-theorem-for-the-sample-mean",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Central Limit Theorem for the Sample Mean",
    "text": "Central Limit Theorem for the Sample Mean\n\n\n\n\n\n\n\nCentral Limit Theorem for the Sample Mean\n\n\n\nIf we have reasonably representative samples of large enough size \\(n\\), taken from a population with true mean \\(\\mu\\) and true standard deviation \\(\\sigma\\), then \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}\\left(0, \\ 1 \\right) \\] or, equivalently, \\[ \\overline{X} \\sim \\mathcal{N}\\left( \\mu, \\ \\frac{\\sigma}{\\sqrt{n}} \\right) \\] where \\(\\overline{X}\\) denotes the sample mean."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nThe temperatures collected at all weather stations in Antarctica follow some unknown distribution with unknown mean and known standard deviation 8oF. A researcher records the temperature measurements from a representative sample of 81 different weather stations, and finds the average temperature to be 26oF.\n\nWhat is the population?\nWhat is the sample?\nDefine the random variable of interest.\nWhat is the probability that this observed average of 26oF lies within 1oF of the true average temperature across all weather stations in Antarctica?\nConstruct a 90% confidence interval for the true average temperature across all weather stations in Antarctica."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-2",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-2",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nThe population is the set of all weather stations in Antarctica.\nThe sample is the 81 weather stations selected by the researcher.\nThe random variable of interest is \\(\\overline{X}\\), the average temperature across 81 randomly-selected weather stations in Antarctica.\n\n\nPart (d): This is where things get interesting!\n\n\nIs the population normally distributed?\n\nNo. Or, at least, we don’t know for certain, so it’s safer not to assume it is.\n\nIs our sample size large enough to invoke the CLT?\n\nYes, since \\(n = 81 \\geq 30\\).\n\nTherefore, the CLT applies and tells us that \\[ \\overline{X} \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{8}{\\sqrt{81}} \\right) \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{8}{9} \\right) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-3",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-3",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nAgain, what we have found is \\[ \\overline{X} \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{8}{\\sqrt{81}} \\right) \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{8}{9} \\right) \\]\nWe seek \\(\\mathbb{P}(\\mu - 1 \\leq \\overline{X} \\leq \\mu + 1)\\), which we first write as \\[ \\mathbb{P}(\\overline{X} \\leq \\mu + 1) - \\mathbb{P}(\\overline{X} \\leq \\mu - 1) \\]\nComputing the necessary \\(z-\\)scores yields \\[\\begin{align*}\nz_1   &  = \\frac{(\\mu + 1) - \\mu}{8/9} = \\frac{9}{8} \\approx 1.13    \\\\\nz_2   &  = \\frac{(\\mu - 1) - \\mu}{8/9} = -\\frac{9}{8} \\approx -1.13\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-4",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-4",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nThe corresponding values from the normal table are \\(0.8708\\) and \\(0.1292\\), respectively, meaning the desired probability is \\[ 0.8708 - 0.1292 = \\boxed{74.16\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#unknown-sigma",
    "href": "Pages/Lectures/Lecture13/Lec13.html#unknown-sigma",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Unknown \\(\\sigma\\)?",
    "text": "Unknown \\(\\sigma\\)?\n\nNotice that in the previous worked-out example (and, indeed, in the CLT for sample means), we need information on the true population standard deviation \\(\\sigma\\).\nWhat happens if we don’t have access to \\(\\sigma\\)?\nWell, we encountered a somewhat similar situation in our discussion on proportions; the standard error of \\(\\widehat{P}\\) depended on \\(p\\), which proves to be a problem in practice (as, again, the true value of \\(p\\) is often unknown).\nDoes anyone remember how we solved this issue in the context of population proportions?\n\nThat’s right- we used the substitution approximation!\nSpecifically, we replaced the unknown parameter (\\(p\\)) with a natural point estimator of it."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#unknown-sigma-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#unknown-sigma-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Unknown \\(\\sigma\\)?",
    "text": "Unknown \\(\\sigma\\)?\n\nCan anyone propose a point estimator for \\(\\sigma\\)?\nThat’s right; \\(s\\), the sample standard deviation! \\[ s = \\sqrt{ \\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\overline{X})^2} \\]\nIn other words, our proposition is to use confidence intervals of the form \\[ \\overline{x} \\pm c \\cdot \\frac{s}{\\sqrt{n}} \\]\nNotice, however, that this introduces additional uncertainty into the problem as \\(s\\) itself is a random variable (different samples result in different sample standard deviations).\nIt turns out that the additional uncertainty introduced is so large that we become no longer able to use the normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#using-s-in-place-of-sigma",
    "href": "Pages/Lectures/Lecture13/Lec13.html#using-s-in-place-of-sigma",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Using \\(s\\) in place of \\(\\sigma\\)",
    "text": "Using \\(s\\) in place of \\(\\sigma\\)\n\nFirstly, recall that we used percentiles of the standard normal distribution because \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0, \\ 1) \\]\nMathematically, what the above discussion is saying is that the distribution of \\[ \\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} \\] is no longer normal.\nIt turns out that, still assuming a large enough sample size, the quantity above follows what is known as a t-distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#the-t-distribution",
    "href": "Pages/Lectures/Lecture13/Lec13.html#the-t-distribution",
    "title": "PSTAT 5A: Lecture 13",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nThe \\(t-\\)distribution looks very similar to the standard normal distribution in that it is centered at 1, and has a bell-like density curve.\nHowever, one key difference is that the \\(t-\\)distribution is parameterized by a single parameter, called the degrees of freedom, which we abbreviate \\(\\mathrm{df}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#the-t-distribution-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#the-t-distribution-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nAnother key property is that, for all finite degrees of freedom, the tails of the t-distribution density curve are “wider” (i.e. higher) than the tails of the standard normal density curve.\n\nWhat this means is that the t-distribution allows for higher probabilities of tail events, thereby incorporating the additional uncertainty injected into our confidence intervals by using \\(s\\) in place of \\(\\sigma\\)\n\nAn interesting fact is that the t-distribution with \\(\\infty\\) degrees of freedom is equivalent to the standard normal distribution.\n\nAs such, with greater degrees of freedom, the t-distribution - and its percentiles - more and more closely resembles the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#back-to-confidence-intervals",
    "href": "Pages/Lectures/Lecture13/Lec13.html#back-to-confidence-intervals",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Back to Confidence Intervals",
    "text": "Back to Confidence Intervals\n\nHere is the result we’ve been working toward: with samples of reasonably large size \\(n\\) from a distribution with mean \\(mu\\) and standard deviation \\(\\sigma\\), \\[ \\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} \\sim t_{n - 1} \\] where \\(t_{n - 1}\\) denotes the \\(t-\\)distribution with \\(n - 1\\) degrees of freedom.\nAs such, our confidence intervals become \\[ \\overline{x} \\pm t_{n - 1, \\ \\alpha} \\cdot \\frac{s}{\\sqrt{n}} \\] where \\(t_{n - 1, \\ \\alpha}\\) denotes the appropriate quantile (corresponding to our desired confidence level) of the \\(t_{n - 1}\\) distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nA sociologist is interested in performing inference on the true average monthly income (in thousands of dollars) of all citizens of the nation of Gauchonia. As such, she takes a representative sample of 49 people, and finds that these 49 people have an average monthly income of 2.25 and a standard deviation of 1.66.\n\nWhat is the population?\nWhat is the sample?\nDefine the random variable of interest.\nConstruct a 95% confidence interval for the true average monthly income (in thousands of dollars) of Gauchonian citizens."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-5",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-5",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nThe population is the set of all Gauchonian residents.\nThe sample is the set of 49 Gauchonian residents included in the sociologist’s sample.\nThe random variable of interest is \\(\\overline{X}\\), the sample average monthly income (in thousands of dollars) of a representative sample of 49 Gauchonian* residents."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-contd",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-contd",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions (cont’d)",
    "text": "Solutions (cont’d)\nPart (d)\n\nIs the population normally distributed?\n\nNo.\n\nIs the sample size large enough?\n\nYes; \\(n = 49 \\geq 30\\).\n\nDo we know the population standard deviation?\n\nNo, we only know \\(s\\).\n\nTherefore, we need to use the t-distribution with \\(n - 1 = 49 - 1 = 48\\) degrees of freedom.\nSpecifically, we need to find the 2.5th percentile of the \\(t_{48}\\) distribution.\nLet’s go over how to read a t-table."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#reading-a-t-table",
    "href": "Pages/Lectures/Lecture13/Lec13.html#reading-a-t-table",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Reading a t-table",
    "text": "Reading a t-table\n(In-Class Exercise)"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-contd-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-contd-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions (cont’d)",
    "text": "Solutions (cont’d)\n\nI’d also like to mention that we can use Python to help us:\n\n\n\nimport scipy.stats as sps\nsps.t.ppf(0.025, 48)\n\n-2.010634754696446\n\n\n\n\nYou’ll learn more about this code during Lab today!\nTherefore, our 95% confidence interval takes the form \\[ \\overline{x} \\pm 2.01 \\cdot \\frac{s}{\\sqrt{49}} \\] or, equivalently, \\[ (2.25) \\pm (2.01) \\cdot \\frac{1.66}{7} = 2.25 \\pm 0.477 = \\boxed{[1.773 \\ , \\ 2.727]}\\]\nThe interpretation of this interval is much the same as our intervals for proportions:\n\n\n\nWe are 95% confident that the true average monthly income (in thousands of dollars) of Gauchonian residents is between 1.773 and 2.727."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#general-flowchart",
    "href": "Pages/Lectures/Lecture13/Lec13.html#general-flowchart",
    "title": "PSTAT 5A: Lecture 13",
    "section": "General Flowchart",
    "text": "General Flowchart\n\n\n\n\n\ngraph TB\n  A[Is the population Normal?  . ] --&gt; |Yes| B{{Use Normal .}}\n  A --&gt; |No| C[Is n &gt;= 30?  .]\n  C --&gt; |Yes| D[sigma or s?  .]\n  C --&gt; |No| E{{cannot proceed   .}}\n  D --&gt; |sigma| F{{Use Normal .}}\n  D --&gt; |s| G{{Use t }}"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#testing-distributional-fits",
    "href": "Pages/Lectures/Lecture13/Lec13.html#testing-distributional-fits",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Testing Distributional Fits",
    "text": "Testing Distributional Fits\n\nNotice that the first question we need to ask ourselves when trying to perform inference on a sample mean is whether or not we believe our population to be normally distributed.\nThis begs the question: given a set of numbers, how can we tell if these numbers were drawn from a normal distribution or not?\nFor example, consider the following set of numbers (which have been assigned to a variable called x):\n\n\n\n\n[ 1.3315865   0.71527897 -1.54540029 -0.00838385  0.62133597 -0.72008556\n  0.26551159  0.10854853  0.00429143 -0.17460021  0.43302619  1.20303737\n -0.96506567  1.02827408  0.22863013  0.44513761 -1.13660221  0.13513688\n  1.484537   -1.07980489 -1.97772828 -1.7433723   0.26607016  2.38496733\n  1.12369125  1.67262221  0.09914922  1.39799638 -0.27124799  0.61320418\n -0.26731719 -0.54930901  0.1327083  -0.47614201  1.30847308  0.19501328\n  0.40020999 -0.33763234  1.25647226 -0.7319695   0.66023155 -0.35087189\n -0.93943336 -0.48933722 -0.80459114 -0.21269764 -0.33914025  0.31216994\n  0.56515267 -0.14742026 -0.02590534  0.2890942  -0.53987907  0.70816002\n  0.84222474  0.2035808   2.39470366  0.91745894 -0.11227247 -0.36218045\n -0.23218226 -0.5017289   1.12878515 -0.69781003 -0.08112218 -0.52929608\n  1.04618286 -1.41855603 -0.36249918 -0.12190569  0.31935642  0.4609029\n -0.21578989  0.98907246  0.31475378  2.46765106 -1.50832149  0.62060066\n -1.04513254 -0.79800882  1.98508459  1.74481415 -1.85618548 -0.2227737\n -0.06584785 -2.13171211 -0.04883051  0.39334122  0.21726515 -1.99439377\n  1.10770823  0.24454398 -0.06191203 -0.75389296  0.71195902  0.91826915\n -0.48209314  0.08958761  0.82699862 -1.95451212]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#idea-1-histogram",
    "href": "Pages/Lectures/Lecture13/Lec13.html#idea-1-histogram",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Idea 1: Histogram",
    "text": "Idea 1: Histogram\n\nOne idea would be to generate the histogram of x:"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#downsides",
    "href": "Pages/Lectures/Lecture13/Lec13.html#downsides",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Downsides",
    "text": "Downsides\n\nHowever, we know that histograms are visually very dependent on the binwidth that was selected!\n\nFurthermore, there are some distributions that look bell-shaped, but are not normal; e.g. the t-distribution!\n\nAs such, we would like a slightly more rigorous way to check for normality.\nThe tool statisticians most often use is called a quantile-quantile plot, or QQ-Plot for short.\nYou don’t have to worry too much (for now) about the details of how they are constructed; for now, we’ll just use Python to generate them and then interpret them ourselves."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#qq-plot",
    "href": "Pages/Lectures/Lecture13/Lec13.html#qq-plot",
    "title": "PSTAT 5A: Lecture 13",
    "section": "QQ-Plot",
    "text": "QQ-Plot\n\n\nscipy.stats.probplot(x, plot = plt);"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#interpreting-qq-plots",
    "href": "Pages/Lectures/Lecture13/Lec13.html#interpreting-qq-plots",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Interpreting QQ-Plots",
    "text": "Interpreting QQ-Plots\n\nThe more linear the QQ-plot, the more likely it is that the data came from a normal distribution.\nWhen checking for deviations from linearity, however, make sure to check the tails as that is most often where non-normality manifests itself the clearest."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#normal-or-not",
    "href": "Pages/Lectures/Lecture13/Lec13.html#normal-or-not",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Normal or Not?",
    "text": "Normal or Not?"
  },
  {
    "objectID": "Pages/schedule.html",
    "href": "Pages/schedule.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis schedule is tentative, and is subject to change. Please check back regularly for updates!\n\n\nLast Updated: Thursday, June 13, 2024\n\nAs a reminder: Monday sections are Lab Sections and Wednesday sections are Discussion Sections.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nLectures\nDiscussion Worksheet\nLab\n\n\n\n\n1\n(6/26 - 7/02)\n\n00: Introduction\n01: Descriptive Statistics I\n02: Descriptive Statistics II\n\nRemember to read the section titled “Transforming Data” on your own\n\n03: Intro to Probability\n04: Counting\nQz01 Solns\n\nWorksheet 01\nSolns\nLab 01  Introduction to Python\nSolns\n\n\n2\n(7/03 - 7/09)\n\n05: Conditional Probabilities\n06: NO LECTURE (Fourth of July)\n07: Review\n08: Midterm 1\n\nMultiple Choice: Blank Solns\nFree Response: Blank Solns\n\n\nWorksheet 02: Work on the MT1 Practice Problems posted under the “Exam Prep” page\nLab 02  Data Classes, Comparisons, Conditionals, and Functions\nSolns\n\n\n3\n(7/10 - 7/16)\n\n09: Discrete Random Variables\n10: Continuous Random Variables\n\nZ-Table\n\n11: Introduction to Inference\n12: Confidence Intervals for Proportions\n\nExercise 3\n\nQz02 Solns\n\nWorksheet 03\nSolns\nLab 03  Descriptive Statistics and Plotting\nSolns\n\n\n4\n(7/17 - 7/23)\n\n13: Confidence Intervals for Means\n\nT-Table\n\n14: Hypothesis Testing for Proportions\n15: Review\n\nFinal answer for Chalkboard Exercise 2 is approximately 0.2001. Be sure to check the Binomial Conditions!\n\n16: Midterm 2\n\nMultiple Choice (Version A): Blank Solns\nFree Response (Yellow Version): Blank Solns\n\nNote: It was clarified during the exam that, on the GauchoSteel problem, all units were meant to be in feet (not meters); conversion between feet and meters was not expected.\nNote: The solutions to the “households in Ethiopia” problem have been updated.\n\n\n\nWorksheet 04: Work on the MT2 Practice Problems posted under the “Exam Prep” page\nLab 04  Simulations, Sampling, and Loops\nSolns\n\n\n5\n(7/24 - 7/30)\n\n17: Hypothesis Testing, Part II\n18: Two-Sample t-Tests\n19: ANOVA (Analysis of Variance)\n20: Intro to Statistical Modeling, and Correlation\nQz03 Solns\n\nWorksheet 05\nSolns\n\nNOTE: There is a mistake in the solutions to Problem 2. As stated in lecture, the denominator degrees of freedom is n - k, not n - k - 1.\n\nLab 05  Markdown Syntax, Importing Data, and Manipulating Tables\nSolns (PDF)\nSolns (.ipynb)\n\n\n6\n(7/31 - 8/06)\n\n21: Regression, Part II\n22: Regression Diagnostics, and Sampling Procedures\n23: Ways Statistics Can Deceive\n24: Review\nFINAL EXAM: Friday, August 4 from 4 - 7pm\n\nMultiple Choice: [VA] [VB]\nSelected MC Solutions: [.pdf]\n\n\nNo Section\nLab 06  A Trip To the Movies",
    "crumbs": [
      "Course Info",
      "Schedule of Topics"
    ]
  }
]